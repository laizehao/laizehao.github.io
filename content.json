{"pages":[],"posts":[{"title":"关于我","text":"我卑微如尘 我弱小不堪 我天赋平庸 我志存高远 汗水是我唯一的武器 我没有时间浪费 只有掌握在手里的技术才是最可靠的 这里是我的技术博客.我会把自己学到的新知识记录在这里,也会把工作过程中遇到的问题写在这里,方便以后查阅.这个博客会见证我的成长 我要变得强大起来 我希望以后再次看到这篇文章的时候,我的初心没有改变.我依旧在努力前进","link":"/posts/a0000000.html"},{"title":"为什么要使用线程池以及线程池用法详解","text":"在java中,有两种创建线程任务的方式: 使用new Thread()创建新线程 通过将Runable() 和 Callable()对象提交到线程池执行任务 new Thread()的弊端在执行一个异步任务时,你还是像下面一样通过new Thread()吗? 12345678new Thread(new Runnable() { @Override public void run() { //task } }).start(); 这是一种不合适的做法.通过new Thread()创建新线程的弊端 线程的生命周期开销非常高.每次提交任务都创建一个新线程会消耗更多的系统资源. 缺乏统一管理线程的方式,,无法控制线程的并发数量.可能因为无限制的创建线程导致系统资源耗尽. 通过线程池提交任务的好处 使用线程池可以复用工作线程，减少创建/销毁线程的次数. 通过配置线程池的工作线程数量 从而控制并发数量. 将任务的提交和任务的执行操作解耦开来 Java 线程池Java通过Executors提供四种线程池，分别为： newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行 通过ThreadPoolExecutor 自定义线程池上面说的四种线程池是java自带的.但是有时候自带的线程池满足不了我们的要求或者需要对线程池做调优时,我们可以通过ThreadPoolExecutor()自定义线程池. 如何初始化ThreadPoolExecutor corePoolSize： 核心线程数量，即在没有任务时的线程池大小，并且只有在工作队列已满的情况下才会创建超出这个数量的线程 maximumPoolSize，最大线程数量，表示可以同时活动的线程的数量上限。 keepAliveTime： 线程池维护线程所允许的空闲时间。如果某个线程的空闲时间超过了存活时间，那么它将会被标记为可回收的。并且当线程池大小超过基本大小时，这个线程将会被终止。 corePoolSize，maximumPoolSize，keepAliveTime共同负责线程的创建和销毁。 unit： 线程池维护线程所允许的空闲时间的单位.(秒,分钟 等) workQueue： 工作队列。线程池所使用的缓冲队列 ,在核心线程都被占用的情况下，新增的任务会被保存到工作队列当中。 RejectedExecutionHandler： 当线程池中的所有线程都被占用,且工作队列被填满后,线程池对拒绝任务的处理策略 。JDK提供了几种不同的RejectedExecutionHandler实现，包括 ： AbortPolicy(中止策略).默认的处理策略,该策略会抛出未检查的RejectedExecutionException. CallerRunsPolicy(调用者运行策略).该策略会把任务回退到调用者,从而降低线程池的流量. DiscardPolicy 和 DiscardOldestPolicy (抛弃策略).DiscardPolicy策略会悄悄抛弃任务.DiscardOldestPolicy会抛弃下一个任务,然后尝试重新提交任务. 如何改写ThreadPoolExecutorThreadPoolExecutor是可以改写的.它提供了几个可以在子类中改写的方法. beforeExecute : 在执行线程的run()之前执行。如果beforeExecute抛出一个未受检异常,那么afterExecute和run()都不会被调用. afterExecute : 在执行线程的run()之后执行 Terminated : 在线程池完成关闭操作时执行,terminated可以用来释放Executor在其生命周期里分配的各种资源，此外还可以执行发送通知、记录日志或者手机finalize统计等操作 如何设置合适的线程池大小原则上，只需要避免”过大”和”过小”两种极端情况.如果线程池过大,那么大量的线程将在相对比较小的CPU和内存资源上发生竞争,这不仅会导致更高的内存占用量,还有可能耗尽资源.如果线程池过小,那么将导致许多空闲的处理器无法执行工作,从而降低吞吐率. 对于计算密集型的任务,在拥有N个处理器的系统上,当线程池的大小为N + 1时，通常可以实现最优的利用率。对于I/O密集型 或者包含其他阻塞操作的任务，由于线程并不会一直执行，因此线程池的规模应该更大。 要使处理器达到期望的使用率，线程池的最优大小 = CPU数量 * CPU使用率 * ( 1 + 等待时间/计算时间)","link":"/posts/863cd27f.html"},{"title":"ThreadLocal介绍以及原理","text":"先看 &lt;&lt;JAVA并发编程实战&gt;&gt;中对ThreadLocal的定义: ThreadLocal是一个关于创建线程局部变量的类。用于维持线程封闭的变量。 我的理解:​ 通常情况下,我们创建的变量是可以被任何一个线程访问并修改的. ​ 而使用ThreadLocal创建的变量只能被当前线程访问，其他线程则无法访问和修改. ​ 因为ThreadLocal只能被当前线程访问,所以不存在多线程之间的共享问题. 那么,什么情况下需要使用ThreadLocal呢?避免参数传递举个例子.对于java web应用而言,Session保存了很多的用户信息.很多时候需要通过Session获取和修改应用信息. 一方面，需要保证每个线程有自己单独的 Session 实例。另一方面，由于很多地方都需要操作 Session，存在多方法共享 Session 的需求。 如果不使用 ThreadLocal，可以在每个线程内构建一个 Session实例，并将该实例在多个方法间传递.但是每个需要使用 Session 的地方，都需要显式传递 Session 对象，方法间耦合度较高。 使用 ThreadLocal 的代码，不再需要在各个方法间传递 Session 对象，并且也非常轻松的保证了每个线程拥有自己独立的实例. 设置线程上下文最典型的,就是用于管理数据库的连接.每个线程对应一个连接,当前线程的所有数据库操作都是交给同一个连接管理.从而实现了事务管理.Hibernate对事务的管理就是基于ThreadLocal实现的. 做一下总结: ThreadLocal 适用于每个线程需要自己独立的实例且该实例需要在多个方法中被使用，也即变量在线程间隔离而在方法或类间共享的场景。 另外，该场景下，并非必须使用 ThreadLocal ，其它方式完全可以实现同样的效果，只是 ThreadLocal 使得实现更简洁。 ThreadLocal的实现原理想要更好的理解ThreadLocal,那么就有必要了解它的实现原理. 因为同一个ThreadLocal包裹的对象在不同的线程中有不同的副本,所以ThreadLocal内部其实是维护了一个map. 我们通过get(),set()方法设置threadLocal的值时,实际上是在调用这个map对应的get(),set()方法. 代码如下: set方法12345678public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } get方法123456789101112 public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) return (T)map.get(this); // Maps are constructed lazily. if the map for this thread // doesn't exist, create it, with this ThreadLocal and its // initial value as its only entry. T value = initialValue(); createMap(t, value); return value; } ThreadLocalMap是个静态的内部类：123static class ThreadLocalMap { ........ } 也就是说,通过ThreadLocal存取的变量其实是放在线程共享的ThreadLocalMap中.ThreadLocal只是对ThreadLocalMap做了一层封装. ThreadLocal在线程池中可能引起内存泄露问题感谢sonarlint弥补了我的知识盲区. 正常情况下使用ThreadLocal不会有问题,因为JVM利用设置ThreadLocalMap的Key为弱引用，来避免内存泄露. 为什么弱引用可以避免内存泄露? 弱引用的特点是，如果这个对象只存在弱引用，那么在下一次垃圾回收的时候必然会被清理掉. 假设 引用ThreadLocal变量的线程被回收了,这个变量在在ThreadLocalMap的对应的Key因为只剩下弱引用,所以会被垃圾回收掉.从而避免了泄露问题. 而线程池复用了线程资源,延长了线程的生命,那么就有可能增加内存泄露的风险. 如何避免问题?在线程池中 使用完 ThreadLocal变量以后.手动将其设置为null.即可避免线程泄露问题.","link":"/posts/2ed165e6.html"},{"title":"单例模式进化史","text":"最早我学习java的时候,了解到的是两种单例模式 饿汉式 饿汉模式单例实现简单,而且是线程安全的.缺点是在类加载时就创建了一个实例,但是这个实例可能不会被使用到,这就造成了资源浪费. 12345678910public class Singleton {private static final Singleton INSTANCE = new Singleton();private Singleton() {} public static Singleton getSingleton() { return INSTANCE;}} 懒汉式 只有在调用getInstance方法的时候才开始实例化。 但是不是线程安全的，所以要在方法上加synchronized关键字，但是加了同步后每次只能被一个线程调用，性能大大降低。所以出来一种基于双检锁的懒汉式. 12345678910111213141516171819202122public class DoubleCheckedLockingSingleton { private volatile DoubleCheckedLockingSingleton INSTANCE; private DoubleCheckedLockingSingleton() { } public DoubleCheckedLockingSingleton getInstance() { if (INSTANCE == null) { synchronized (DoubleCheckedLockingSingleton.class) { // double checking Singleton instance if (INSTANCE == null) { INSTANCE = new DoubleCheckedLockingSingleton(); } } } return INSTANCE;} // readResolve to prevent another instance of Singletonprivate Object readResolve() { return INSTANCE;}} 但是基于双检锁的懒汉式单例还是有问题.可能因为指令重排序导致其他线程观察到的INSTANCE为null.所以需要给INSTANCE加上volatile. 后来又学到两种单例模式 静态内部类单例模式. 静态内部类单例是单例设计模式中的一种，特点是线程安全、调用率高、也可以延时加载，多次创建的单例都是同一个对象，静态内部类是由饿汉式单例延伸而来，静态内部类是在类的内部再创建一个类，利用静态工厂方法调用内部类创建的对象，并返回. 123456789101112class SingletonExample { //静态工厂方法 public static SingletonExample getInstance() { return InteriorClass.single; } private static class InteriorClass { private static final SingletonExample single = new SingletonExample(); } } 静态内部类只会在getInstance被初始化一次,所以它是可以延迟加载的.另外还规避了多线程的并发问题.是很好的单例模式解决方式. 通过单元素枚举实现单例模式 我认为这是比较好的单例实现方式. 首先相比上面说的三种单例模式,它的实现是最简单的.使用枚举实现单例代码会精简很多. 另外,枚举可解决线程安全问题.在java底层为线程安全提供了保证. 需要注意,jvm初始化枚举类对象的时机是在启动时,也就是本质上和饿汉式一样.都是即时加载的. 如果需要使用变量时再加载实例,应该使用静态内部类单例模式. 123456public enum EnumSingleton { INSTANCE; public EnumSingleton getInstance(){ return INSTANCE; }}","link":"/posts/28d7539.html"},{"title":"为什么要使用IOC容器","text":"控制反转（Inversion of Control，缩写为IOC) 是一种设计原则.不再由依赖对象去创建,而是由IOC容器负责创建并且在软件运行过程中动态注入依赖. 一般情况下,当对象A依赖对象B时,需要在对象A中new一个对象实例B的实例. 123public class A{ private B b = new B();} 这是一个简单的例子.通过这个例子无法看出使用IOC的好处. 但是在实际情况里, 可不是简简单单new 一个对象就完事了. 被依赖的对象有可能依赖其他对象,这种情况下代码会变成这样 : 12345public class A{ private C depforB1 = new C(); private D depforB2 = new D(); private B b = new B(depforB1,depforB2);} 实际情况可能更加复杂.比如C和D还有各自的依赖.C和D的依赖还有各自的依赖.最后依赖层级越来越多,子子孙孙无穷尽也. 可以看出,在代码规模很大的情况下,每次编写创建依赖都将是一个沉重的负担. 那么使用了IOC容器的情况是怎么样呢? 以Spring 的依赖注入作为例子.如果我们需要在对象A里使用对象B的实例,只需要通过@Autowired注解告诉框架即可: 123456public class A{ @Autowired private B b;} 使用者不需要关心创建对象B的过程.因为这个过程由IOC容器代劳了. 有了IoC容器后，把创建和查找依赖对象的控制权交给了容器，由容器进行注入组合对象，所以对象与对象之间是 松散耦合，这样也方便测试，利于功能复用，更重要的是使得程序的整个体系结构变得非常灵活. 另外,AOP的两种代理实现方式.都需要修改原来的对象.如果没有获得对象的创建权,AOP也就无从谈起.","link":"/posts/b80bb92e.html"},{"title":"fegin传输字符串时处理空格的坑(feigin框架的bug)","text":"前言先说具体场景,我用A服务调用B服务的查询接口,查询参数里包含一个 Zonedatetime 字段,两个 String 字段. 因为B服务已经添加了A服务的依赖,所以可以通过实体类接收来自A服务的查询参数. 代码是这样,一个非常简单的feigin接口调用: 123@GetMapping(\"/api/plans\")@ApiOperation(\"查询接口\")ResponseEntity&lt;List&lt;PlanDTO&gt;&gt; getPlans(PlanQueryForm queryForm); 但是这样简单的一个接口调用却出现了异常,就是时间格式化异常. 排查了很久,最后分析出了结论.fegin在序列化Zonedatetime 类型的字段时,会把Zonedatetime 中表示时区的部分变成+8:00的字符串. 而反序列化的时候会把url中的 +号解码成半角空格 “ “ .这样必然会出现时间格式化异常问题.. 解决方法 修改接口,给参数加上@RequestBody可以规避这个问题.但是我认为这不是很好的解决方式.于是 通过google.我发现15年的时候已经有人在github-openFeigin 上提到过这个问题: According to rfc2396, space in url should be encoded as “%20” insteadof “+”. 那么真相大白了,原来url中的空格被编码成+号其实是一个BUG,+号应该被编码成%20才是正解. 看了我们公司使用的springboot版本是2.10.依赖的feigin版本是10.1.0 github中的feigin仓库里修复这个问题版本号是 8.13 照理说这个BUG早就应该被修复了才对. 尝试升级springCloud版本可能修复这个bug.但是风险太高.emm.","link":"/posts/f1f38b39.html"},{"title":"git常用命令速查","text":"记录一下常用的git命令 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165#初始化仓库：git init#添加文件到仓库2步 git add readme.txt git commit -m \"wrote a readme file\"#仓库当前的状态git status#查看修改的内容git diff readme.txt#最近到最远的提交日志git log#HEAD指向的版本就是当前版本上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100 #回退到上一个版本git reset --hard HEAD^#回退到指定的版本git reset --hard commit_id#查看每一个命令记录git reflog#撤销还没有add的文件git checkout -- readme.txt#可以把暂存区的修改撤销掉（unstage），重新放回工作区git reset HEAD readme.txt#删除一个文件git rm readme.txt #创建sshkey，可以在用户主目录里找到.ssh目录，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人并添加到github上面ssh-keygen -t rsa -C \"419400980@qq..com\" #本地仓库与Github关联git remote add origin git@github.com:ai419400980/learngit.git #提交代码到中央仓库git push -u origin master#(我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令) #从远程仓库获取代码git clone git@github.com:ai419400980/gitskills.git #查看分支：git branch#创建分支git branch &lt;name&gt;#切换分支git checkout &lt;name&gt;#创建+切换分支git checkout -b &lt;name&gt;#合并某分支到当前分支git merge &lt;name&gt;#删除分支git branch -d &lt;name&gt; #stash功能，可以把当前工作现场“储藏”起来，等以后恢复现场后继续工作git stash#一是用git stash apply恢复，但是恢复后，stash内容并不删除，你需要用git stash drop来删除；#另一种方式是用git stash pop，恢复的同时把stash内容也删了#你可以多次stash，恢复的时候，先用git stash list查看，然后恢复指定的stash，用命令：$ git stash apply stash@{0}#如果要丢弃一个没有被合并过的分支，可以通过git branch -D &lt;name&gt;强行删除。#查看远程库信息，使用git remote -v#从本地推送分支，使用git push origin branch-name#如果推送失败，先用git pull抓取远程的新提交；#在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name#本地和远程分支的名称最好一致；#建立本地分支和远程分支的关联，使用git branch --set-upstream branch-name origin/branch-name；#从远程抓取分支，使用git pull，如果有冲突，要先处理冲突 #标签#用于新建一个标签，默认为HEAD，也可以指定一个commit idgit tag &lt;tagname&gt;#可以指定标签信息git tag -a &lt;tagname&gt; -m \"blablabla...\"#可以查看所有标签。git tag命令git push origin &lt;tagname&gt;可以推送一个本地标签；命令git push origin --tags可以推送全部未推送过的本地标签；命令git tag -d &lt;tagname&gt;可以删除一个本地标签；命令git push origin :refs/tags/&lt;tagname&gt;可以删除一个远程标签。 忽略某些文件时，需要编写.gitignore；gitignore文件本身要放到版本库里，并且可以对.gitignore做版本管理！","link":"/posts/4d554704.html"},{"title":"jvm服务端优化配置速查","text":"总体思路 让服务端应用以服务的模式执行，以获得相应的性能提升支持 显式地设置堆容量分配，避免对内存资源不合理的应用 选择合适的GC算法，提高性能 保存GC日志，作为优化的依据 保存OOM时的内存信息，以提高OOM排查效率 开启JMX服务，以方便监测JVM状态(视情况定) 设置字符串常量池大小,减少因为哈希碰撞带来的性能损耗 注：对于堆容量分配和GC算法的选择需要有具体测试数据提供依据，否则可能无法提高性能，甚至降低性能。 让服务端应用以服务的模式执行-server开启 JVM 的 server 模式，以支持 JIT编译等相关特性 显示地设置堆容量分配-Xms -Xmx根据具体的可用资源规划，显式设置堆大小，可以减少 JVM 自动调整堆大小带来的开销 单位：g | m | k 参数内容示例： -Xms1g -Xmx4g -XX:MaxMetaspaceSizeJDK8中，元数据区容量默认可以自动增长。为了稳定起见，可根据实际情况设置一个明确的上限 单位：g | m | k 参数内容示例：-XX:MaxMetaspaceSize=512m -Xmn设置新生代容量 单位：g | m | k 参数内容示例：-Xmn512m 选择合适的GC算法注：GC算法的选择需要根据实际应用情况选择。此处仅针对（老年代）采用CMS GC时的一般情况。 在“吞吐量优先”的应用，也许应考虑 Parallel GC。 而 G1 GC 则是兼顾 吞吐量 和 停顿时间的 GC。 CMS GC 在大堆情况下会因为内存碎片导致严重停顿；所以堆大于16G时可能得考虑用G1，堆大于30G时则采用CMS得非常谨慎。 而且 JDK9 中 CMS 已被标记为废弃。 通常，G1 是非常值得考虑的GC。当然 JDK11 中的 ZGC 也是值得期待的。 《常见垃圾收集器》 -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled使用 CMS 作为GC方式，且开启并行标记，以减少服务端延迟时间 -XX:+UseParNewGC针对新生代的GC；是Serial GC的多线程版本 -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFractionCMS GC算法默认使用启发式的触发规则。这往往会导致老年代快慢的时候才触发GC。 显式地设置这个触发规则可以使得GC行为更能被预测，并减少Full GC带来的性能损耗（如，STW stop-the-world JVM暂停）。 +UseCMSInitiatingOccupancyOnly 可以取消默认的启发式触发规则； CMSInitiatingOccupancyFraction 则可以设置具体什么时候触发CMS GC 这个值需要根据具体部署环境下的老年代使用情况进行调优。如果数值较小，可能会导致CMS GC过于频繁；如果数值较大，可能会导致CMS GC触发时机太晚甚至“并行模式失败” 一般可以将该值设置为70，即老年代使用量达到70%时会触发CMS GC 参数内容示例：-XX:CMSInitiatingOccupancyFraction=70 -XX:+ScavengeBeforeFullGC在执行 Full GC 前执行一次 Minor GC可以较少老年代中对象“意外”存活的现象。 这些老年代对象被新生代中的对象所引用， 但这些新生代对象其实已经可以被回收了，所以这些老年代对象其实也应被回收 -XX:+CMSScavengeBeforeRemark在执行CMS Remark阶段前，执行一次 Minor GC，以降低STW的时间。 通过 Minor GC 可以减少新生代对老年代对象的引用，这样可以减少根对象（GC Roots）数量，从而降低 CMS Remark 的工作量 保存GC日志-XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xloggc:log_file_path将GC日志输出到指定的文件中 参数内容示例：-Xloggc:/opt/log/gc.log -XX:+PrintAdaptiveSizePolicy输出 GC 策略的详细信息，包括： Minor GC 中存活下来的对象的内存占用量 Minor GC 中进入老年代的对象的内存占用量 Minor GC 的起始时间 … GC 的某些行为是根据运行时情况适应性触发的，通过这些详细信息可以让我们更好地理解为什么会出现我们不希望发生的行为。 -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles -XX:GCLogFileSize与上一组设置配合，设置GC日志文件的大小上限与数量 参数内容示例： -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=10m 保存OOM时的内存信息-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath一般，OOM出现的频率不高，且OOM的复现成本也比较高，为了方便排查，需要在OOM出现时让JVM保存相关内存信息 参数内容示例：-XX:HeapDumpPath=/opt/log/heap.hprof 开启JMX服务-Djava.rmi.server.hostname一台计算机有两个IP，一个对外，一个对内，是很常见的网络部署设置。为了避免JVM自行选择IP带来的不确定性，需要显式地设置JVM上的服务所使用的IP。 注：如果应用本身已经有更明确的 IP选项，则应以具体应用为准，而不是使用该选项提供的值。 如，假设某个Java进程需要用IP1对外提供HTTP Web服务，同时需要用IP2对内其它系统提供dubbo服务，则可以显式地配置dubbo服务所使用的IP2。 当然，如果条件允许，拆分该Java进程，尝试利用“微服务”概念也许是个更好的做法。 参数内容示例：-Djava.rmi.server.hostname=10.1.100.123 -Dcom.sun.management.jmxremote.port设置该端口可方便在JVM运行时通过一些管理工具来检测运行状况（如，JMC） 参数内容示例：-Dcom.sun.management.jmxremote.port=6789 在确保所运行网络环境安全的情况下，也即只有授权用户才能连接到上述端口，可关闭该管理的权限认证，以方便连接： -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -设置字符串常量池哈希表的大小.-XX:StringTableSize : 10240 字符串常量池是一个大小固定的哈希表,数值越大,因为rehash造成的性能损失越小.","link":"/posts/714fb806.html"},{"title":"redis中的数据结构详解和使用场景","text":"1.String 字符串类型String 数据结构是简单的 key-value 类型，value 不仅可以是 String， 也可以是数字（当数字类型用 Long 可以表示的时候encoding 就是整型，其他都存储在 sdshdr 当做字符串) 常用命令get、set、incr、decr、mget等 使用场景 LEN STRING：O(1)获取字符串长度 APPEND STRING：往字符串 append 内容，而且采用智能分配内存（每次2倍） 设置和获取字符串的某一段内容 设置及获取字符串的某一位（bit） 批量设置一系列字符串的内容 原子计数器 GETSET 命令的妙用，请于清空旧值的同时设置一个新值，配合原子计数器使用 SETNX:分布式锁(不推荐) 内部实现:String在redis内部存储默认就是一个字符串，被redisObject所引用，当遇到incr,decr等操作时会转成数值型进行计算,转换失败则抛出异常. 2.Hash 哈希表Redis 哈希(Hash) 是一个string类型的field和value的映射表，Redis 中每个hash 可以存储232 - 1 键值对（40多亿） 常用命令hget,hset,hgetall(不推荐) 等. 使用场景 用来存储经常变更的数据.比如对象. 用来存储数据结构相同的映射.如 id =&gt; 数据. hash value支持incr.可以用做统计.比如 page =&gt;点击次数. 实现方式:上面已经说到Redis Hash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value redisObject的encoding为zipmap，当成员数量增大时会自动转成真正的HashMap，此时encoding为ht。 3.List 列表常用命令lpush,rpush,lpop,rpop,lrange等 就是一个双向链表. 一个list表最多可以包含232 - 1 个值.可以作为栈或者队列使用. 使用场景 消息队列(不推荐) lpush + ltrim 保持n个最新的记录.然后通过range获取.可用于取最近登录用户列表 4.Set 集合常用命令sadd,spop,smembers,sunion 等. Set 就是一个集合，集合的概念就是一堆不重复值的组合。利用 Redis 提供的 Set 数据结构，可以存储一些集合性的数据。 比如在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。 因为 Redis 非常人性化的为集合提供了求交集、并集、差集等操作， 那么就可以非常方便的实现如共同关注、共同喜好、二度好友等功能，对上面的所有集合操作， 你还可以使用不同的命令选择将结果返回给客户端还是存集到一个新的集合中。 使用场景 取交集共同好友、二度好友 利用唯一性，可以统计访问网站的所有独立 IP 好友推荐的时候，根据 tag 求交集，大于某个 threshold 就可以推荐 5.Zset (Sorted Sets) 有序集合常用命令zadd,zrange,zrem,zcard等 和Sets相比，Sorted Sets是将 Set 中的元素增加了一个权重参数 score，使得集合中的元素能够按 score 进行有序排列. 比如一个存储全班同学成绩的 Sorted Sets，其集合 value 可以是同学的学号，而 score 就可以是其考试得分. 这样在数据插入集合的时候，就已经进行了天然的排序。 另外还可以用 Sorted Sets 来做带权重的队列， 比如普通消息的 score 为1，重要消息的 score 为2，然后工作线程可以选择按 score 的倒序来获取工作任务。让重要的任务优先执行。 使用场景 带有权重的元素，比如一个游戏的用户得分排行榜 带时间戳的去重列表. 其他所有Set能做的事情. 6.HyperLogLog常用命令pfadd,pfcount,pfmerge HyperLogLog是Redis的高级数据结构，是基数统计的利器.HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。 但是,因为hpyerLogLog只会根据输入的数据做基数统计,不储存元素本身.所以hyperLoglog不能像集合那样返回输入的元素. 使用场景 统计访问网站的独立ip.","link":"/posts/1cafd345.html"},{"title":"如何选择合适的redis持久化策略","text":"先介绍一下redis的两种持久化方式 1.RDB模式每隔一段时间保存REDIS存储的数据快照,并且存储到磁盘等介质上. SAVE 和 BGSAVE 两个命令都会调用 rdbSave 函数，但它们调用的方式各有不同： SAVE 直接调用 rdbSave ，阻塞 Redis 主进程，直到保存完成为止。 BGSAVE 则 fork 出一个子进程，子进程负责调用 rdbSave ，并在保存完成之后向主进程发送信号，通知保存已完成。 BGSAVE时, rdbSave 在子进程被调用，所以 Redis 服务器在 BGSAVE 执行期间仍然可以继续处理客户端的请求。 而在SAVE时,服务器是阻塞的,所以当SAVE在执行时,新的命令不会产生任何作用.因此SAVE已经基本废弃,在生产环境要杜绝SAVE的使用. RDB模式的优点 一旦采用RDB模式,那么整个备份将只会包含一个文件.这对文件备份来说是有利的.比如你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复. 数据不太多情况下,效率会略高些。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了. 恢复数据效率更高.由于RDB模式是将内存数据的镜像写入到磁盘上,所以恢复数据的速度相对于AOF模式更快一些. RDB模式的缺点 如果需要保证数据的绝对一致性,也就是最大限度避免数据丢失.那么RDB将不是一个很好的选择. 因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失. 由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟. 2.AOF模式以日志的形式记录服务器所处理的每一个写、删除操作，查询操作不会记录，以文本的方式记录，可以打开文件看到详细的操作记录. 在下次redis重新启动时，只要把这些写指令从前到后再重复执行一遍，就可以实现数据恢复了. AOF模式的优点 相对于RDB模式可以带来更好的数据安全性.redis中提供了3种策略, 其中每秒同步也是异步完成的,效率是很高的.只是一旦出现宕机那么最近1秒写入的数据会丢失. 而每修改同步可以认为是同步持久化,每次发生的数据变化都会立刻被记录到磁盘里. 由于这种模式对日志文件的写入是追加模式,所以即使宕机也不会破坏日志中已经存在的内容. 如果写入到一半时出现宕机,也可以通过redis自带的工具对备份进行校验. AOF模式的缺点 对于相同数量的数据集而言，AOF文件通常要大于RDB文件.RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快 根据同步策略的不同，AOF在运行效率上往往会慢于RDB.总之，每秒同步策略的效率是比较高的. 二者选择的标准，就是数据一致性和性能之间的取舍. 看系统是愿意牺牲一些性能,换取更高的数据一致性（aof）,还是愿意写操作频繁的时候，不启用备份来换取更高的性能，待手动运行save的时候，再做备份（rdb）. 持久化配置 &amp; 比较rdb持久化配置: 配置命令 写入数据是否阻塞 保存数据是否阻塞 停机时丢失的数据量 说明 save 900 1 Y N 最后一次持久化操作之后的所有数据 表示900秒内如果有至少1个key变化,则进行持久化. save 300 10 Y N 最后一次持久化操作之后的所有数据 表示300秒内如果有至少10个key变化,则进行持久化. aof持久化配置: 配置命令 写入数据是否阻塞 保存数据是否阻塞 停机时丢失的数据量 说明 appendfsync no Y Y 操作系统最后一次对 AOF 文件触发 SAVE 操作之后的数据 从不同步。需要调用SAVE或者BGSAVE触发 appendfsync everysec Y N 不超过1秒钟的数据 每1秒钟同步一次，该策略为AOF的缺省策略 appendfsync always Y Y 最多只丢失一个命令的数据 每次有数据修改发生时都会写入AOF文件","link":"/posts/1xjwi143.html"},{"title":"spring bean 详解","text":"什么是spring beanSpring bean是由spring容器管理的对象.Spring 容器会自动完成bean对象的实例化. 这也是IOC的本质,由容器来管理对象的生命周期. bean的实例化过程&amp;&amp;生命周期bean的实例化过程如下图: Bean的完整生命周期经历了各种方法调用，这些方法可以划分为以下几类： Bean自身的方法 包括了Bean内部带有@PreDestroy和@PostConstruct 和通过配置文件中的init-method和destroy-method指定的方法 Bean级生命周期接口方法 包括了BeanNameAware、BeanFactoryAware、InitializingBean和DiposableBean这些接口的方法 容器级生命周期接口方法 包括了InstantiationAwareBeanPostProcessor 和BeanPostProcessor 这两个接口实现，一般称它们的实现类为“后处理器”。 工厂后处理器接口方法 包括了AspectJWeavingEnabler,ConfigurationClassPostProcessor, CustomAutowireConfigurer等等非常有用的工厂后处理器接口的方法。工厂后处理器也是容器级的。在应用上下文装配配置文件之后立即调用。 bean的作用域和实现方式 singleton 单例 默认的作用域 实现方式 在获取bean之前,会尝试从bean缓存中获取bean,如果存在,则直接返回对应的bean,否则会先通过beandefinition实例化bean并缓存.(该作用域适用于无状态的bean) 优点 : 一次创建成功以后可以重复使用.是spring bean默认的作用域. prototype 实现方式 每次获取bean时,都会创建一个新的实例. 适用于有状态的bean request &amp; session 顾名思义,这类bean 只在当前的request | session 内有效. 实现方式 通过RequestContextListener 监听 request的创建,通过Threadlocal保证每个线程都持有对应的request 实例化bean 把实例化的bean放入request 域中该作用域只支持在web容器中使用spring时创建 global session每个全局的HTTP Session，使用session定义的Bean都将产生一个新实例。典型情况下，仅在使用portlet context的时候有效。同样只有在Web应用中使用Spring时，该作用域才有效 常见的依赖注入方式常用的依赖注入方式有三种:: setter注入 在setter方法上使用@Autowired或者@Resource注解.Spring会自动查找并注入对应的bean. 构造器注入(spring官方推荐的注入方式) 在Bean的构造器上添加需要的依赖作为参数,Spring会自动查找并注入对应的bean 构造器注入的优点 : 保证依赖不可变（final关键字） 保证依赖不为空（省去了我们对其检查） 保证返回客户端（调用）的代码的时候是完全初始化的状态 避免了循环依赖(通过启动时抛出异常的方式进行) 提升了代码的可复用性 属性注入 在属性上使用@Autowired或者@Resource注解.Spring会自动查找并注入对应的bean. 虽然官方推荐使用构造器注入. 但是个人认为,面向service编程本质上是面向表结构的面向过程编程. 所以绝大多数情况下,使用setter注入和属性注入并无不良影响.还能省去处理循环依赖异常的麻烦.","link":"/posts/3ccfce38.html"},{"title":"spring事务传播行为介绍","text":"概念事务传播行为用来描述由某一个事务传播行为修饰的方法被嵌套进另一个方法的时事务如何传播。 用伪代码说明： 123456789public void methodA(){ methodB(); //doSomething}@Transaction(Propagation=XXX)public void methodB(){ //doSomething} 代码中methodA()方法嵌套调用了methodB()方法，methodB()的事务传播行为由@Transaction(Propagation=XXX)设置决定。这里需要注意的是methodA()并没有开启事务，某一个事务传播行为修饰的方法并不是必须要在开启事务的外围方法中调用。 事务的传播类型 事务传播行为类型 说明 PROPAGATION_REQUIRED 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。 PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY 使用当前的事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。该类型不能在事务中运行。 PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 默认传播类型为 PROPAGATION_REQUIRED 关于事务传播类型的详细解释PROPAGATION_REQUIRED假如当前正要执行的事务不在另外一个事务里，那么就起一个新的事务 比如说，ServiceB.methodB的事务级别定义为PROPAGATION_REQUIRED, 那么由于执行ServiceA.methodA的时候， ​ ServiceA.methodA已经起了事务，这时调用ServiceB.methodB，ServiceB.methodB看到自己已经运行在ServiceA.methodA 的事务内部，就不再起新的事务。而假如ServiceA.methodA运行的时候发现自己没有在事务中，他就会为自己分配一个事务。 这样，在ServiceA.methodA或者在ServiceB.methodB内的任何地方出现异常，事务都会被回滚。即使ServiceB.methodB的事务已经被 提交，但是ServiceA.methodA在接下来fail要回滚，ServiceB.methodB也要回滚 PROPAGATION_SUPPORTS如果当前在事务中，即以事务的形式运行，如果当前不再一个事务中，那么就以非事务的形式运行 这就跟平常用的普通非事务的代码只有一点点区别了。不理这个，因为我也没有觉得有什么区别 PROPAGATION_MANDATORY必须在一个事务中运行。也就是说，他只能被一个父事务调用。否则，他就要抛出异常。 PROPAGATION_REQUIRES_NEW这个就比较绕口了。 比如我们设计ServiceA.methodA的事务级别为PROPAGATION_REQUIRED，ServiceB.methodB的事务级别为PROPAGATION_REQUIRES_NEW， 那么当执行到ServiceB.methodB的时候，ServiceA.methodA所在的事务就会挂起，ServiceB.methodB会起一个新的事务，等待ServiceB.methodB的事务完成以后， 他才继续执行。他与PROPAGATION_REQUIRED 的事务区别在于事务的回滚程度了。因为ServiceB.methodB是新起一个事务，那么就是存在 两个不同的事务。如果ServiceB.methodB已经提交，那么ServiceA.methodA失败回滚，ServiceB.methodB是不会回滚的。如果ServiceB.methodB失败回滚， 如果他抛出的异常被ServiceA.methodA捕获，ServiceA.methodA事务仍然可能提交。 PROPAGATION_NOT_SUPPORTED当前不支持事务。比如ServiceA.methodA的事务级别是PROPAGATION_REQUIRED ，而ServiceB.methodB的事务级别是PROPAGATION_NOT_SUPPORTED ， 那么当执行到ServiceB.methodB时，ServiceA.methodA的事务挂起，而他以非事务的状态运行完，再继续ServiceA.methodA的事务。 PROPAGATION_NEVER不能在事务中运行。假设ServiceA.methodA的事务级别是PROPAGATION_REQUIRED， 而ServiceB.methodB的事务级别是PROPAGATION_NEVER ， 那么ServiceB.methodB就要抛出异常了。 PROPAGATION_NESTED理解Nested的关键是savepoint。他与PROPAGATION_REQUIRES_NEW的区别是，PROPAGATION_REQUIRES_NEW另起一个事务，将会与他的父事务相互独立， 而Nested的事务和他的父事务是相依的，他的提交是要等和他的父事务一块提交的。也就是说，如果父事务最后回滚，他也要回滚的。","link":"/posts/4e5f32a7.html"},{"title":"jvm垃圾回收算法介绍","text":"判断Java中对象是否存活的算法垃圾回收器在对堆内存回收之前,第一件事就是要确定哪些对象是存活的,哪些对象是可以 被回收的(也就是那些不会再被使用的对象).那么有哪些算法呢? 引用计数法引用计数器算法是给每个对象设置一个计数器，当有地方引用这个对象的时候，计数器+1，当引用失效的时候，计数器-1，当计数器为0的时候，JVM就认为对象不再被使用，可以被回收了。 引用计数器实现简单，效率高. 但是它有个致命缺陷,无法解决循环引用问题（A对象引用B对象，B对象又引用A对象，但是A,B对象已不被任何其他对象引用) 示例: 12345678910111213141516//在使用了引用计数法判断对象是否存活的jvm中运行public class ReferenceCountingGC { public Object instance = null; private static final int _1MB = 1024 * 1024; private byte[] bigSize = new byte[2 * _1MB]; public static void testGC() { ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; //假设在这行发生GC,objA和objB都不会被回收 System.gc(); }} 所以在JDK1.1之后，这个算法已经不再使用了. 可达性分析算法也就是通过被称为“GC Root”的对象作为起始点，从这些节点往下开始搜索，当一个对象没有任何引用链到GC Root时，则代表此对象是可以被回收的.目前主流的JVM实现都是通过此法来判断对象是否存活. 可以作为”GC Root”的对象包括以下几种: 本地变量,又叫局部变量,也就是方法内定义的变量. 静态类属性引用的对象. 常量 JNI引用的对象 垃圾回收算法标记-清除算法标记-清除（Mark-Sweep）算法是现代垃圾回收算法的思想基础。 标记清除算法分为两个阶段,首先通过可达性分析算法标记出所有需要回收的对象，在标记完成以后统一回收所有被标记的对象。 标记清除算法实现简单,但是有两个缺点: 首先标记和清除的过程效率都不高.而且,在清除完以后容易产生空间碎片，空间碎片太多的情况可能导致在分配大对象时可能会因为内存空间不足提前触发第二次GC. 标记-整理算法标记-整理算法是标记-清除算法的改进版本.标记过程和标记-整理算法一样，但是后续步骤不是直接对可回收对象进行清理，而是让所有存活对象都向一端移动，然后直接清理掉边界以外的内存(ArrayList 的扩容时的batchRemove() 方法类似这个算法，当然JVM的整理算法要比batchRemove()复杂得多) 它的缺点和标记-清除算法一样,标记的过程效率比较低.但是它的清除过程效率比较高,而且解决了内存碎片导致分配对象效率低下的问题. 标记-复制算法为了解决标记-清除算法的问题.标记-复制算法就出现了. 它将可用内存分为大小相等的2块，每次只使用其中的1块，当这一块的内存用完了，就将存活的对象复制到另一个块，再把使用过的内存清理掉. 这样使得每次垃圾回收都是对某个半区进行回收.内存分配时也不用考虑内存碎片等复杂情况.只需要按顺序分配内存即可. 标记-复制算法优点是实现简单运行高效,但是有两个问题. 因为把内存分为大小相等的两块,所以每次最多使用一半的内存空间,剩下一半内存空间就浪费掉了.代价太过高昂.另外复制算法在对象比较多的时候需要进行比较多的复制操作,效率低下. 分代收集算法当前商业虚拟机都是采用分代收集算法，它根据对象存活周期的不同将内存划分为几块，一般是把Java堆分为新生代和老年代，然后根据各个年代的特点采用最适当的收集算法，在新生代中，每次垃圾收集都发现有大批对象死去，只有少量存活，就选用复制算法，而老年代因为对象存活率高，没有额外空间对它进行分配担保，就必须使用“标记清理”或者“标记整理”算法来进行回收。 图的左半部分是未回收前的内存区域，右半部分是回收后的内存区域. 分配策略 对象优先在新生代(Eden)区域分配，如果对象过大直接分配到老年代(Old)区域 在多次GC中存活的对象,进入到老年代(Old)区域 分代收集算法是如何进行垃圾回收的 新生成的对象优先放到新生代(Eden)区,如果对象过大直接分配到老年代(Old)区域 当Eden空间满时，会触发Minor GC. 在Minor GC中存活下来的对象移动到幸存者(Survivor)区. 幸存者(Survivor)区分成两个部分,survivor0和survivor1. Survivor0区满后触发执行Minor GC.存活对象移动到Suvivor1区. Survivor0区存活对象移动到Suvivor1区，这样保证了一段时间内总有一个Survivor区为空.另一个不为空的Survivor区没有碎片. 经过多次Minor GC仍然存活的对象移动到老年代. 老年代存储长期存活的对象，占满时会触发Full GC，GC期间会停止所有线程等待GC完成，所以对响应要求高的应用尽量减少发生Major GC，避免响应超时。 如何改进复制算法 现在的商业虚拟机都采用这种收集算法来回收新生代，IBM公司的专门研究表明，新生代中的对象98%是“朝生暮死”的，所以并不需要按照1:1的比例来划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor 。当回收时，将Eden和Survivor中还存活着的对象一次性地复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。 HotSpot虚拟机默认Eden和Survivor的大小比例是8:1，也就是每次新生代中可用内存空间为整个新生代容量的90%（80%+10%），只有10%的内存会被“浪费”。当然，98%的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于10%的对象存活，当Survivor空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。 我的理解,对于web服务器来说,因为其”无状态”的特性,所以大部分对象都是朝生暮死的. 所以在对web服务器做jvm优化时,应该把大部分堆内存分配给Eden空间,这样才能做到最大化利用内存. 对于像游戏服务器这种强状态的服务来说,因为需要保存大量的玩家状态,所以大部分对象存活时间都很长,而存活时间长的对象经过多次GC 都在Old区中. 为了减少Full GC的频率, 应该把大部分堆内存分配给Old空间.","link":"/posts/6774949a.html"},{"title":"centos&redhat防火墙firewalld常用命令速查","text":"记录一下Centos&amp;RedHat中对firewalld的常用操作命令.以备不时之需. 虽然现在的弹性vps都自带出入站规则.但是偶尔还是会有点用的. 123456789101112131415161718192021222324252627282930313233343536# 1.查看防火墙状态：firewall-cmd --state # 2.启动防火墙systemctl start firewalld# 3.关闭防火墙systemctl stop firewalld# 4.检查防火墙开放的端口firewall-cmd --permanent --zone=public --list-ports# 5.开放一个新的端口firewall-cmd --zone=public --add-port=80/tcp --permanent# 6.重启防火墙firewall-cmd --reload# 7.验证新增加端口是否生效firewall-cmd --zone=public --query-port=443/tcp# 8.防火墙开机自启动systemctl enable firewalld.service# 9.防火墙取消某一开放端口firewall-cmd --zone=public --remove-port=443/tcp --permanent","link":"/posts/b44666ae.html"},{"title":"使用覆盖索引优化大偏移分页查询","text":"需求:查询100w+行数据的表A.需要支持 排序+ 动态条件 +分页. 做法A:123select b.id,b.book_name from book_store b order by b.title limit 0,20#然后分页查询,比如第二页就是select b.id,b.book_name from book_store b order by b.title limit 20,20 做法挺简单,但是有性能问题.偏移量较大的情况下SQL执行执行很慢,需要5-10秒.这显然不可接受. 分析:用explain分析sql执行过程 1select b.id,b.book_name from book_store b order by b.title limit 500000,20 发现使用了索引,但是扫描了500020行,丢弃了500000行.这是分页的执行过程.所以随着表的数据增加,sql扫描的数据会越来越多,导致很慢. 解决方法: 思路:使用覆盖索引进行优化 1select b.id,b.book_name from book_store join (select id from from book_store order by title limit 15000,50)as lim using(id)) 通过子查询返回需要的需要的主键,再通过这些主键关联原表获取需要的行. 在关联查询时,需要特别注意以下几点 : 确保on 或者using 子句列上有索引.因此,在创建索引时就要考虑关联的顺序.没有用到的索引只会带来额外的负担,一般来说,除非有特别的理由,否则只需要在关联顺序的第二张表上建立索引. 确保任何的group by或者 order by的表达式中只涉及到一张表的索引,这样mysql才能使用索引来优化这个过程.","link":"/posts/daa946f7.html"},{"title":"详细了解java并发的三大特性,指令重排序和先行发生原则","text":"在java的并发编程中,如果要保证线程安全,就要保证代码的原子性,可见性,和有序性. 原子性(Atomicity)定义如果某个变量 或者 一个或多个命令组成的一系列操作具有原子性,那么在同一个时间只能有一个线程对它进行操作.不会出现多个线程同时修改或者执行某个变量或者方法的情况. 基本类型变量的原子性基本类型的赋值和获取操作的原子性由java内存模型保证. 例子1234int x = 5;//原子操作,将5赋给xboolean y = true;//原子操作 将true赋给yint z = 1 + x; //不是原子操作;包含3个操作;获取x的值,将x + 1,将x + 1的计算结果赋给zx ++;//不是原子操作;包含3个操作,获取x的值,将x的值+ 1,将x + 1的结果赋给x 多个命令的原子性 通过加锁的机制实现独占,从而保证原子性. 例子 : synchronized关键字,显式锁ReadWriteLock,ReentrantLock.. 违反原子性的例子123456789101112131415161718192021222324252627282930313233343536public class AtomicityTest { private static int i = 1; public static void main(String[] args) { Thread thread1 = new Thread(() -&gt; mod()); Thread thread2 = new Thread(() -&gt; mod()); Thread thread3 = new Thread(() -&gt; mod()); thread1.start(); thread2.start(); thread3.start(); } @SneakyThrows public static void mod() { for (int n = 0; n &lt; 10; ++n) { System.out.println(i++); Thread.sleep(10); } } //测试n次 //第一次: 输出 1,2,3...19,20 //第二次: 输出 1,2,3...10,1,11,12 ...23 //第三次输出 1,2,3...9,1,12,12, ...25 //第三次输出 1,2,3...29 //可以发现,每次输出的结果都不一样 //因为i++不是原子操作,在修改i过程中可能i被其它线程修改, //所以通过多个线程对变量i进行循环递增时,无法保证i的结果一定能递增到30 //如何修改? //在\"i++\"操作上加上synchronized关键字: /** * synchronized(ThreadTest.class) { * System.out.println(i++); * } */} 可见性(Visibility)定义可见性就是指当一个线程修改了线程共享变量的值，其它线程能够立即得知这个修改. Java内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方法来实现可见性的. 普通变量的可见性根据上文可知,普通变量的值的同步需要通过主内存完成. 普通变量在修改完成以后,同步回主内存之前,其他线程读取到的是旧值,所以存在可见性问题. volatile变量的可见性volatile变量修改以后能立即同步到主内存,以及每次其他线程使用变量前立即从内存读取最新的值. 因此volatile修饰变量保证了线程操作时变量的可见性. 其它保证可见性的技术 synchronized关键字.在同步块结束之前，必须先把块内的变量同步回主内存中. final关键字.一旦变量初始化完成.那么它的值就不会发生变化,从而保证了变量的可见性. 违反可见性的例子123456789101112//线程a执行的代码int i = 0;i = 10;//线程b执行的代码j = i;int y = j += 1//线程a和线程b在不同的cpu上执行//线程a修改i的值以后没有立刻将它同步到堆内存中.//此时线程b执行 j = i，它会先去堆内存读取i的值注意此时内存当中i的值还是0，那么就会使得j的值为0，而不是10.//这就是可见性问题，线程a对变量i修改了之后，线程b没有立即看到线程1修改的值. 有序性(Ordering)在java内存模型中说过，为了性能优化，编译器和处理器会进行指令重排序. 也就是Java内存模型中的程序天然有序性可以总结为一句话： 如果在本线程内观察，所有操作都是有序的；如果在一个线程中观察另一个线程，所有操作都是无序的。 在单例模式的实现上有一种双重检验锁定的方式（Double-checked Locking）。代码如下： 12345678910111213141516171819202122public class Singleton { private Singleton() { } private volatile static Singleton instance; public Singleton getInstance(){ if(instance==null){ synchronized (Singleton.class){ if(instance==null){ instance = new Singleton(); } } } return instance; }}//这里为什么要加volatile了？我们先来分析一下不加volatile的情况，有问题的语句是这条：//instance = new Singleton();//这条语句实际上包含了三个操作：1.分配对象的内存空间；2.初始化对象；3.设置instance指向刚分配的内存地址。但由于存在重排序的问题，可能有以下的执行顺序://线程A :分配对象的内存空间 =&gt; 初始化对象 =&gt; 将变量指向刚刚分配的地址//线程B :判断对象是否为null =&gt; 初次访问对象//如果线程A中初始化对象和 将 变量指向分配的地址 进行了重排序.那么线程B进行判断if(instance==null)时就会为true，而实际上这个instance并没有初始化成功，显而易见对线程B来说之后的操作就会是错得//volatile关键字使用了内存屏障禁止了指令的重排序语义,因此,具有有序性. 关于指令重排序在程序能得出正确的结果的前提下,CPU可以按照任意顺序任意重排指令。重排序后的指令，对于优化执行以及成熟的全局寄存器分配算法的使用，都是大有裨益的，它使得程序在计算性能上有了很大的提升。 重排序类型包括： 编译器生成指令的次序，可以不同于源代码中的行号的顺序 处理器可以乱序或者并行的执行指令 缓存会改变写入提交到主内存的变量的次序 先行发生原则(happens-before)Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 先行发生原则. 注意,这里的先行指的不是实际运行时某个动作发生在另一个动作之前,而是指的操作结果的正确性. 如果按照源码顺序,操作A发生在操作B之前,在发生操作B之前,操作A产生的影响能被操作B观察到. 举个例子123i = i+1;j = j+1;i = i+2; 这里i = i + 1逻辑上先于j = j + 1发生，但是由于处理器的优化动作——指令重排序功能和编译器的优化动作——字节码重排序，实际运行的过程中，由于前两句之间没有数据相关，那么j = j + 1实际上可能会发生在i = i + 1之前，但是这并没有改变这段程序的运行结果，因此符合先行发生原则. 另外,先行发生原则只适用于单线程.如果操作A和操作B不在同一个线程,那么就不适用先行发生原则. 程序顺序规则: 源码顺序中写在前面的操作必须在写在后面的操作之前. 管理锁定规则：对同一个锁的解锁操作必须在锁操作之后. volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作 传递性原则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C 线程启动规则：线程的开始动作必须先于这个线程的所有动作. 线程中断规则：对线程中断动作必须发生于被中断线程的代码检测到中断事件的发生之前. 线程终结规则：线程中所有的操作都必须在线程中止之前执行. 对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始","link":"/posts/1f31e22c.html"},{"title":"volatile关键字作用和原理","text":"作用1 : 提供了变量可见性保证违反可见性的例子例子123456789101112//线程a执行的代码int i = 0;i = 10;//线程b执行的代码j = i;int y = j += 1//线程a和线程b在不同的cpu上执行//线程a修改i的值以后没有立刻将它同步到堆内存中.//此时线程b执行 j = i，它会先去堆内存读取i的值注意此时内存当中i的值还是0，那么就会使得j的值为0，而不是10.//这就是可见性问题，线程a对变量i修改了之后，线程b没有立即看到线程1修改的值. 为什么volatile变量能够保证变量的可见性当写入一个普通的变量时,会先将变量修改后的值写入cpu的高速缓存,然后再同步到堆内存. 在新值写入到堆内存之前,对其他线程是不可见的.此时其他线程读到的仍然是旧值. 而一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰后,当一个线程修改了这个变量的值时,会把修改以后的值立刻同步到堆内存中.从而保证了变量的可见性. 作用2.通过内存屏障保证有序性volatile是通过内存屏障保证有序性的. 观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码(字节码)发现，加入volatile关键字时，会多出一个lock前缀指令 lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能： 它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 它会强制将对缓存的修改操作立即写入主存； 如果是写操作，它会导致其他CPU中对应的缓存行无效 使用场景volatile关键字提供了对象的可见性和有序性保证.但是不能用于构建原子操作的复合操作.因此,当一个变量依赖其他变量,或者当变量的新值依赖变量的旧值时,使用volatile变量并不能保证对变量的修改操作不会丢失. 例如,i++看起来像一个原子操作,但是实际上它包含了3个独立的操作,获取变量的当前值,将当前值 +1,写入新值.为了保证更新操作不会丢失,必须保证整个读-写-改操作是原子的.","link":"/posts/1wx33990.html"},{"title":"在redis中应尽量用scan代替keys命令","text":"众所周知,redis是基于单线程的. 因此在使用一些时间复杂度 = O(n)的命令时要小心,可能一个不小心就会阻塞主进程,导致redis出现卡顿. 有时候,我们需要针对一部分符合条件的命令进行批量操作.比如获取以ARTICLE:开头的所有key. 那么要如何获得这些key呢? 在redis2.8之前,我们可以使用KEYS命令通过正则表达式获取我们想要的key. 但是KEYS命令有两个缺点: 没有分页功能.我们只能一次性获取我们想要的key.如果结果数量有上千万上亿条,那么等待你的将会是”内存溢出”. KEYS是遍历算法,时间复杂度 = O(n),如果key的数量特别多,可能阻塞主线程导致redis出现卡顿. 所以,redis在2.8版本推出了SCAN命令.可以用于增量的迭代redis内的元素. 关于Redis的SCAN命令SCAN cursor [MATCH pattern] [COUNT count] cusor : 迭代时的下标.SCAN命令每次被调用以后,都会返回一个新的下标.把这个新的用于下次迭代以此延续之前的迭代过程. MATCH pattern : 用于匹配数据库key的正则表达式. count : 每次迭代返回的元素数量的最大值,也就是本次要遍历多少个元素,然后返回其中符合条件的值. 相比KEYS,SCAN有两个优点: 提供了分页选项.可以控制每次迭代返回的元素数量. 虽然SCAN的时间复杂度还是O(n),但是它是分批次进行的,不会阻塞线程. 正常情况下,使用SCAN不会有问题,但是如果正在rehash,就有可能出现重复数据. 另外,在迭代过程中被添加或者删除的元素有可能不会被返回. 这是由于SCAN的rehash机制导致的. 所以,在使用scan时,需要客户端进行去重. 在spring中使用scan命令有几个坑要注意: cusor使用完以后需要关闭,否则会一直占用redis连接.这里使用了try-with-resource语法自动执行AutoCloseable的close方法. 在执行时使用了executeWithStickyConnection,因为scan需要在同一个连接上执行. scan返回的是byte[],需要自己写ConvertingCursor进行反序列化. 123456789101112131415161718192021 class RedisScanExample{ public void test() throws IOException { try (ConvertingCursor&lt;byte[], String&gt; cursor = getCursor()) { cursor.forEachRemaining(System.out::println); } } @SuppressWarnings(\"unchecked\") private ConvertingCursor&lt;byte[], String&gt; getCursor() { return (ConvertingCursor&lt;byte[], String&gt;) redisTemplate .executeWithStickyConnection((RedisCallback&lt;ConvertingCursor&lt;byte[], String&gt;&gt;) connection -&gt; { ScanOptions options = new ScanOptions .ScanOptionsBuilder() .match(\"test*\") .count(1000) .build(); return new ConvertingCursor&lt;&gt;(connection.scan(options), (b) -&gt; (String) redisTemplate.getStringSerializer().deserialize(b)); }); }} 额,虽然spring是事实标准但是我觉得这个api设计的有点过于底层了. spring完全可以封装一个类似scan(MATCHER,COUNT,TYPE,RESULT -&gt; {})的api让使用者直接调用.但是他没有这么干. 所以我建议在使用的时候封装一个类似RedisUtil的工具类供直接调用.","link":"/posts/2wx88364.html"},{"title":"通过原子变量保证程序的线程安全","text":"在java.util.concurrent包中的许多类中,都提供了比synchronized机制更高的性能和可伸缩性. 这种性能提升的主要来源是原子变量和非阻塞的同步机制. 在基于锁的算法中,如果一个线程在休眠或者自旋的同时持有一个锁,那么其他线程都无法继续执行下去. 而非阻塞算法不会受到单个线程失败的影响.从java5.0开始,可以使用原子变量构建高效的非阻塞算法. 锁的劣势通过使用独占锁,可以保证无论哪个线程持有锁,都能才用独占的方式访问这些变量,并且对变量所做的任何修改在释放锁以后对其他线程都是可见的. 在多个线程同时请求锁时,JVM就要借助操作系统的功能,如果出现这种情况,那么竞争锁失败的线程就会被挂起,并且在稍后恢复线程,在挂起和恢复线程时存在很大的开销. 如果在基于锁的类中包含粗粒度的操作,那么当在锁上存在着激烈的竞争时,调度开销的占比会非常高. 原子变量类原子变量类比锁的粒度更细,量级更轻.原子变量将竞争范围从块级缩小到单个变量上. 更新原子变量的速度(非竞争的情况下)不会比获取锁慢,并且通常会更快.而且不需要挂起或者重新调度线程.在使用基于原子变量而非锁的算法时,线程在执行时更不容易出现延迟,并且在遇到竞争时,也更容易恢复过来. 原子变量相当于一种泛化的volatile,能够支持原子和有条件的读-改-写操作.总共有12个原子变量类,可以分为四种 : 标量类 AtomicInteger,AtomicLong : 支持算术运算和CAS方法 AtomicBoolean : 用于多线程中安全处理业务逻辑的非锁 AtomicReference : 持有一个对象的引用,并且通过CAS方法操作它. 更新器类 AtomicIntegerFieldUpdater：基于反射的工具类，可以原子性的更新指定对象的指定int类型字段。 AtomicLongFieldUpdater：基于反射的工具类，可以原子性的更新指定对象的指定long类型字段。 AtomicReferenceFieldUpdater：基于反射的工具类，可以原子性的更新指定对象的指定应用类型字段。 关于原子类型字段更新器的使用需要注意一下几个方面 字段必须是volatile类型的，用于保证可见性。 字段和字段更新器的访问类型(public/protected/private)必须一致,也就是更新器必须可以直接操作对象字段. 字段只能是实例变量，不能是类变量(static)。 字段不能是final的变量，这样的字段不可修改。 对于AtomicIntegerFieldUpdater和AtomicLongFieldUpdater只能修改int/long类型的字段，不能修改其包装类型（Integer/Long）,**如果要处理Integer和Long类型，则需要使用AtomicReferenceFieldUpdater 数组类: 原子数组类(只支持Integer,Long和Reference版本)中的元素可以实现原子更新.原子数组类为数组提供了和volatile相等的访问语义,这是普通数组不具备的特性.","link":"/posts/16ac7bd0.html"},{"title":"java内存模型介绍","text":"关于java内存模型的定义先看&lt;&lt;深入了解java虚拟机&gt;&gt;里对java内存模型的解释: java内存模型是由jvm规范定义的用来屏蔽掉java程序在各种不同的硬件和操作系统对内存的访问的差异.这样就可以实现java程序在各种不同的平台上都能达到内存访问的一致性. 按照我的理解,就是java虚拟机在操作系统之上封装了一套自己的内存管理机制.在jvm中,所有对底层内存的操作都要通过这套机制间接的执行. 这样做有什么好处呢? 举个例子,windows内存底层是分页机制.32位操作系统下每个进程有属于自己的4G虚拟空间. 但是我们基于java在windows下编程的时候从来不用考虑如何分配释放内存之类的问题.因为这些事情全部由jvm代劳了. 这就是统一内存管理的好处:方便由虚拟机进行自动化的内存管理. java内存模型的结构根据 JVM 规范，JVM 内存共分为虚拟机栈、堆、方法区、程序计数器、本地方法栈五个部分. 程序计数器(Program Counter Register)可以看作是当前线程所执行的字节码文件（class）的行号指示器。 在虚拟机的世界中，字节码解释器就是通过改变计数器的值来选取下一条执行的字节码指令，实现分支、循环、跳转、异常处理、线程恢复. JAVA虚拟机栈(Virtual Machine Stack)与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程. 本地方法栈（Native Method Stacks）功能与Java虚拟机栈相同。区别在于，本地方法栈为虚拟机使用到的native方法服务. 堆内存(Heap) 堆是JVM内存占用最大，管理最复杂的一个区域,是线程共享的. 其唯一的用途就是存放对象实例：所有的对象实例及数组都在对上进行分配。 1.7后，字符串常量池从永久代中剥离出来，存放在堆中。 堆有自己进一步的内存分块划分，按照GC分代收集角度的划分请参见上图。 方法区(Method Area)是线程共享的,用来存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据.","link":"/posts/6a6ae4dd.html"},{"title":"docker常用命令速查","text":"记录一下docker的日常使用命令，本文主要针对linux + mac操作系统而言，window是否适用不太确定，谨慎使用 1. docker进程docker进程启动、停止、重启，常见的三种case 1234567# 启动dockerservice docker start# 关机dockerservice docker stop# 重启dockerservice docker restart复制代码 2. 镜像操作镜像作为容器执行的前提条件，一般需要掌握的几个命令无非是搜索，下载，删除，创建 123456789# 镜像列表docker images# 检索镜像, 从镜像仓库中检索docker search xxx# 下载镜像docker pull xxx# 删除镜像docker rmi xxx复制代码 关于创建镜像，有必要稍微详细一点点 12345# 通过容器创建镜像docker commit -m=&quot;首次提交&quot; -a=&quot;一灰灰Blog&quot; dd85eb055fe8 yh/centos:v0.1# 镜像历史查询docker history yh/centos复制代码 上面的几个参数进行说明 -m 和git的提交一样，后面跟上描述信息 -a 版权声明，这个东西是我创建的，有啥问题，找我 dd85eb055fe8 容器id yhh/quick-os:0.1 创建的镜像名 3. 容器操作接下来就是正菜了，容器的各种操作，启动，关闭，重启，日志查询以及各种进入容器内部搞事情 a. run万事开头第一步，加载镜像，创建容器 12docker run 镜像名:版本复制代码 run后面可以跟很多的参数，比如容器暴露端口指定，存储映射，权限等等，由于参数过多，下面只给出几个不同的例子，来具体的演示参数可以怎么加 case1: 创建并后台执行 12docker run -i -t -d centos:latest复制代码 其中关键参数为-d，指定容器运行与前台或者后台，不加上时前台 -i: 打开STDIN，用于控制台交互 -t: 支持终端登录 case2: 运行一个带命令在后台不断执行的容器 12docker run -d centos:latest ping www.baidu.com复制代码 case3: 运行一个在后台不断执行的容器，同时带有命令，程序被终止后还能重启继续跑 12docker run -d --restart=always centos:latest ping www.baidu.com复制代码 case4: 指定容器名 12docker run -d --name=yhh_centos centos:latest复制代码 case5: 暴露容器端口80，并与宿主机端口8080绑定 12docker run -d --name=yhh_centos -p 8080:80 centos:latest复制代码 case6: 指定容器与宿主机目录（/home/yihui/html/www）共享 12docker run -d --name=yhh_centos -v /home/yihui/html/www:/var/www centos:latest复制代码 b. 基操容器创建完毕之后，就是一些基本操作了，启动、停止、重启、删除 1234567891011# 查看容器列表， 列出所有的容器docker ps -a # 启动容器，start后面可以跟上容器名，或者容器iddocker start xxx # (这里的xxx可以是容器名：yhh_centos 也可以是容器id：f57398ab22c5)# 关闭容器docker stop xxx# 重启docker restart xxx# 删除docker rm xxx复制代码 在查看容器列表时，如果某个容器的启动参数特别长，直接使用docker ps -a会发现看不到完整的启动命令，这个时候可以带上参数--no-trunc来显示完整命令 12docker ps -a --no-trunc复制代码 c. 进阶接下来进入一些容器的高级操作技巧（实际上也并没有特别酷炫） 为了演示一些进阶的内容，这里创建一个容器作为测试 12docker run -it -d --name=yhhos centos复制代码 容器日志查询 日志，定位问题的神器 123# 查询xxx容器的日志docker logs yhhos复制代码 基本上不太会直接使用上面的命令，因为上面把所有的日志都打印出来了，可以直接晃瞎我们的钛合金x眼 一般日志可以加两个参数 -f, -t 12docker logs -f -t --since=&quot;2019-05-11&quot; --tail=10 yhhos复制代码 --since : 此参数指定了输出日志开始日期，即只输出指定日期之后的日志。 -f : 查看实时日志 -t : 查看日志产生的日期 --tail=10 : 查看最后的10条日志。 文件拷贝 将容器的某个文件捞出来；或者强塞，一个cp即可 123456# 将当前目录的test.md文件拷贝到容器的 /tmp 目录下docker cp test.md yhhos:/tmp# 将容器的/tmp/test.md目录拷贝到当前目录下docker cp yhhos:/tmp/test.md ./out.md复制代码 进入容器 进入容器内部，然后就可以为所欲为了… 12docker exec -it yhhos /bin/bash复制代码 获取容器所有信息 1docker inspect yhhos 转载自:https://juejin.im/post/5deb98d96fb9a01600533062","link":"/posts/6dfb3b6e.html"},{"title":"基于docker搭建redis和mongodb的过程和一些坑","text":"我是一个docker小白.这段时间正在学习如何使用docker. 因为测试环境需要,所以想通过docker搭建redis和mongodb.记录一下搭建过程留作以后备用. docker搭建redis1234#单机模式,不带密码docker run --name redis -p 127.0.0.1:6379:6379 -v /etc/localtime:/etc/localtime -d redis#单机模式,带密码docker run --name redis -p 127.0.0.1:6379:6379 -v /etc/localtime:/etc/localtime -d redis --requirepass \"密码\" docker搭建mongodb12345678910111213#单机,无认证docker run -p 127.0.0.1:27017:27017 -v &lt;LocalDirectoryPath&gt;:/data/db --name docker_mongodb -d mongo#单机,带身份验证的docker run -d --name mongodb -v /home/xxx/dbdir/mongodb:/data/db -e MONGO_INITDB_ROOT_USERNAME=xxx -e MONGO_INITDB_ROOT_PASSWORD=xxxxx -p 127.0.0.1:27017:27017 mongo --auth#-p 指定容器的端口映射，mongodb 默认端口为 27017#-v 为设置容器的挂载目录，这里是将&lt;LocalDirectoryPath&gt;即本机中的目录挂载到容器中的/data/db中作为 mongodb 的存储目录#--name 为设置该容器的名称#-d 设置容器以守护进程方式运行#注意 : 数据迁移的时候只需要把&lt;LocalDirectoryPath&gt;下的文件挪走就好了 搭建过程中的一些坑 报错 : SELinux is not supported with the overlay2 graph driver on this kernel 根据百度的结果, 出现这个问题是因为此linux的内核中的SELinux不支持 overlay2 graph driver ，解决方法有两个，要么启动一个新内核，要么就在docker里禁用selinux. 12345678910111213141516#编辑/etc/sysconfig/docker$ vi /etc/sysconfig/docker#找到这行配置OPTIONS='--selinux-enabled=false --log-driver=journald --signature-verification=false'#删除 '--selinux-enabled=false' #删除完以后大概是这样OPTIONS='--log-driver=journald --signature-verification=false'#重启docker$ systemctl restart docker 报错:RUN sudo mkdir -p … cannot allocate memory 查了一下文档,发现这个错误的意思就像它自己说的一样,无法分配内存了. 解决方案(暂时的): 1234#写入pid_maxecho \"kernel.pid_max=999999\" &gt;&gt; /etc/sysctl.conf#使配置重新生效sysctl -p 后来我发现这个问题是有原因的.具体表现是从某次更新以后服务的线程数就不断增加,从最初的几百,一千多,慢慢增长到一万,两万..初步判断是将系统最大线程占满导致的..导致系统最大线程占满的原因很多,这次是因为中了门罗币的挖矿病毒.当然实际情况多变,配置异常,中毒,代码中有创建大量线程的逻辑,都有可能. 报错:Error response from daemon: error creating overlay mount to … invalid argument. 出现这个错误是因为当前的内核不支持overlay2文件系统.解决方案: 123456789101112#step1：先停用docker服务$ service docker stop#step2：删除docker镜像文件夹$ rm -rf /var/lib/docker#step3：重新指定docker文件系统$ vi /etc/sysconfig/docker-storage # 找到下面的参数，做如下修改： DOCKER_STORAGE_OPTIONS=\"--storage-driver overlay \" # 保存 $ !wq#step4：重启docker服务$ service docker start","link":"/posts/ff0d6c01.html"},{"title":"基于docker快速搭建elk日志系统","text":"目的因为公司业务要求,目前的架构是采用了spring-cloud微服务架构. 无论是开发,测试,还是生产环境,都需要通过日志来排查出现问题的原因.ssh上去查询日志太麻烦了,特别是一个服务有多个实例的情况下,根本不知道报错位置在哪个实例上.只能一个个上去查找.非常浪费时间. 因此需要搭建基于elk的日志系统,另外,采用docker搭建elk更加方便,所以采用docker来搭建elk. 搭建过程 镜像版本 ​ kibana 6.8.2 ​ elasticsearch 6.8.2 ​ logstash 6.8.2 脚本 创建elsearch,kibana ,logstash 12345678910111213141516171819202122232425262728#创建子网用于服务内部通讯docker network create --subnet=192.119.0.0/16 multihost#设置文件句柄数量防止el-search启动失败sysctl -w vm.max_map_count=262144#安装elsearch# --net= 加入子网 multihost 并设定当前容器在子网内的ip地址为 192.119.2.51docker run -d --name elasticsearch \\ --net=multihost --ip=192.119.2.51 \\ elasticsearch:6.8.2#安装kibana#--net=multihost 加入子网,地址由任意分配#-e ELASTICSEARCH_URL 配置el-search地址#-p 5601:5601 映射端口到宿主机的5601端口docker run -d --name mykibana \\ --net=multihost \\ -e ELASTICSEARCH_URL=http://192.119.2.51:9200 \\ -p 5601:5601 \\ kibana:6.8.2#安装logstash#指定当前路径为basedirbasedir=`cd $(dirname $0); pwd -P`#-v /$basedir:/data/conf把当前路径映射到容器内的/data/confdocker run -d \\ -v \"$PWD\":$basedir \\ -v /$basedir:/data/conf \\ --net multihost \\ logstash:6.8.2 \\ logstash -f /data/conf/logstash.conf logstash.conf,需要跟脚本放在同一目录下: 12345678910111213141516171819202122232425#logstash.conf#接收数据的端口.input { tcp{ port =&gt; 29820 #端口 mode =&gt; &quot;server&quot; host =&gt; &quot;0.0.0.0&quot; }}#在将日志发送到logstash之前会将它转换成结构化的json格式.filter{ json{ source =&gt; &quot;message&quot; }}output { elasticsearch { hosts =&gt; [&quot;127.0.0.1&quot;] index =&gt; &quot;converge_log_%{+YYYY.MM.dd}&quot; document_type =&gt; &quot;verbose&quot; }} spring如何接入elk使用LoggerAppender. ElkLoggerConfiguration: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788@Configuration@Slf4j@EnableConfigurationProperties({ElkLoggerConfigureProperties.class, ElkLoggerLogstashProperties.class})public class ElkLoggerConfiguration { @Value(\"${spring.application.name}\") private String appName; @Value(\"${server.port}\") private String serverPort; @Value(\"${eureka.instance.instanceId}\") private String instanceId; @Value(\"${spring.profiles.active}\") private String profiles; @Autowired private ElkLoggerConfigureProperties configProperties; @Autowired private ElkLoggerLogstashProperties logstashProperties; @Autowired(required = false) @Qualifier(\"loggerContext\") private LoggerContext loggerContext; @Bean(\"loggerContext\") public LoggerContext loggerContext() { return (LoggerContext) LoggerFactory.getILoggerFactory(); } @Bean(\"logstashAppender\") @Conditional(NotInTestCondition.class) @Lazy public AsyncAppender addLogstashAppender() { log.info(\"Initializing logstash elk logging\"); LoggerContext context = loggerContext; if (loggerContext != null &amp;&amp; configProperties.getEnable()) { LogstashTcpSocketAppender logstashAppender = new LogstashTcpSocketAppender(); logstashAppender.setName(ElkConstant.ELK_APPENDER); logstashAppender.setContext(context); // 编码及message自定义字段(用于指定log来源) LogstashEncoder logstashEncoder = new LogstashEncoder(); String profile = Optional.ofNullable(this.profiles).orElse(ElkConstant.UNKNOWN); String customFields = \"{\\\"app_name\\\":\\\"\" + appName + \"\\\",\\\"app_port\\\":\\\"\" + serverPort + \"\\\",\" + \"\\\"instance_id\\\":\\\"\" + instanceId + \"\\\", \\\"profile\\\":\\\"\" + profile + \"\\\" }\"; logstashEncoder.setCustomFields(customFields); logstashAppender.setEncoder(logstashEncoder); // logstash地址 String destination = logstashProperties.getHost() + \":\" + logstashProperties.getPort(); logstashAppender.addDestination(destination); // 连接策略 DestinationConnectionStrategy destinationConnectionStrategy = new EnvDestinationConnectionStrategy(); logstashAppender.setConnectionStrategy(destinationConnectionStrategy); // 传输的日志级别 ThresholdFilter filter = new ThresholdFilter(); filter.setLevel(configProperties.getLogLevel()); filter.start(); logstashAppender.addFilter(filter); logstashAppender.start(); // Wrap the appender in an Async appender for performance AsyncAppender asyncLogstashAppender = new AsyncAppender(); asyncLogstashAppender.setContext(context); asyncLogstashAppender.setName(configProperties.getAppenderName()); asyncLogstashAppender.setQueueSize(logstashProperties.getQueueSize()); asyncLogstashAppender.addAppender(logstashAppender); asyncLogstashAppender.start(); context.getLogger(Logger.ROOT_LOGGER_NAME).addAppender(asyncLogstashAppender); log.info(\"初始化elk日志成功\"); return asyncLogstashAppender; } return null; }} EnvDestinationConnectionStrategy: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Slf4jpublic class EnvDestinationConnectionStrategy extends DestinationConnectionStrategyWithTtl { private Map&lt;Integer, Boolean&gt; successMap = new TreeMap&lt;&gt;(); /** * 顺序尝试连接，若爱上一个连接，即一往情深 * * @param previousDestinationIndex * @param numDestinations * @return */ @Override public int selectNextDestinationIndex(int previousDestinationIndex, int numDestinations) { int nextIndex = 0; if (!successMap.isEmpty()) { Set&lt;Map.Entry&lt;Integer, Boolean&gt;&gt; set = successMap.entrySet(); for (Map.Entry&lt;Integer, Boolean&gt; entry : set) { nextIndex = entry.getKey(); if (entry.getValue()) { nextIndex--; break; } } nextIndex++; } if (nextIndex &gt;= numDestinations) { nextIndex %= numDestinations; successMap.clear(); } log.warn(\"logstash节点选择：{}-&gt;{}\", previousDestinationIndex, nextIndex); return nextIndex; } @Override public void connectSuccess(long connectionStartTimeInMillis, int connectedDestinationIndex, int numDestinations) { log.info(\"logstash节点：{}，连接成功！\", connectedDestinationIndex); successMap.put(connectedDestinationIndex, Boolean.TRUE); } @Override public void connectFailed(long connectionStartTimeInMillis, int failedDestinationIndex, int numDestinations) { log.error(\"logstash节点：{}，连接失败！\", failedDestinationIndex); if (!successMap.containsKey(failedDestinationIndex)) { successMap.put(failedDestinationIndex, Boolean.FALSE); } }} 完整代码就不贴了这种东西看个大概思路就行.","link":"/posts/679009c3.html"},{"title":"Spring Cloud 选择合适的注册中心","text":"什么是注册中心在微服务架构中,注册中心是最核心的基础服务之一.在微服务架构流行之前,注册中心就已经开始出现在分布式架构的系统中. 注册中心可以说是微服务架构中的”通讯录“，它记录了服务和服务地址的映射关系.在分布式架构中,服务会注册到这里,当服务需要调用其它服务时,就到这里找到服务的地址,进行调用. 举个例子 没有注册中心的情况 假设我们知道微服务的IP和端口号,那么我们就可以调用它. 但是如果微服务换了地址或者端口,或者增加了新的节点,我们该如何得到通知? 有注册中心的情况 调用微服务时,通过服务名查找到对应的微服务的地址,然后就可以直接调用它. 如果微服务出现异动或者新增节点,那么微服务也会告知注册中心. 后续,我们调用微服务时,就可以获取到更新以后的地址. 上述两个场景就是注册中心的两个职责： 服务发现 服务注册 Nacos和Eureka简单介绍一下Nacos和Eureka,目前Spring Cloud体系中最常用的两个注册中心. Eureka是Netflix开发的一个基于rest的服务发现框架.SpringCloud将它集成在其子项目spring-cloud-netflix中，以实现SpringCloud的服务发现功能. Nacos 是阿里巴巴开源的一个更易于构建云原生应用的注册中心、配置管理和服务管理平台.同时具有服务发现 和 配置中心的功能. 首先,两者部署方式有差别.eureka注册中心需要启动一个Spring Boot工程.然后通过引入依赖 + 自动装配的方式启用 + 部署.nacos则是通过官网下载jar包然后直接启动. 其次,对于客户端来说,两者差别不大,都是通过注解开启服务发现功能. 另外,虽然nacos server也是基于spring boot实现的,但是其自带的后台管理界面无疑对使用者来说更加友好.而且nacos还提供另一个核心功能 - 动态配置中心. 相比eureka,nacos在功能性方面无疑是有优势的. 最关键的.eureka2.x闭源了.继续使用它需要承担风险.而nacos有阿里维护. 从Eureka无痛迁移到Nacos使用 Spring Cloud Alibaba 的开源组件 spring-cloud-starter-alibaba-nacos-discovery替换 Eureka，So Easy！ 123456&lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-alibaba-nacos-discovery --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;version&gt;0.2.1.RELEASE&lt;/version&gt;&lt;/dependency&gt; application配置修改application配置application.properties 123nacos.discovery.server-addr=127.0.0.1:8848#有人说不生效，就试下下面的spring.cloud.nacos.discovery.server-addr=127.0.0.1:8848 如果是application.yml，请试下下面的配置123456789nacos: discovery: server-addr: 127.0.0.1:8848#建议先用上面的spring: cloud: nacos: discovery: server-addr: 127.0.0.1:8848 @EnableDiscoveryClient注解更换EnableEurekaClient 注解。如果在你的应用启动程序启动类加了@EnableEurekaClient ，请修改为@EnableDiscoveryClient 其它的需要注意的点如何不停止服务的前提下更换注册中心? 有没有合适的方案呢?","link":"/posts/266848be.html"},{"title":"如何预防解决redis缓存雪崩,缓存穿透,缓存击穿问题","text":"在我们设计一个缓存系统时,不得不考虑缓存雪崩,缓存穿透,缓存击穿导致的问题 缓存雪崩缓存雪崩是指我们为key设置了相同的失效时间,导致大量缓存在某一时刻同时失效,查询请求全部落在数据库上,导致数据库因为瞬时压力过大而崩溃. 如何解决缓存雪崩导致的结果非常可怕,解决问题的思路就是把缓存的失效时间分散开来,从而避免大量缓存同时失效. 这里分享一个简单有效的处理方案,比如我们可以在设置缓存时在原有失效时间的基础上添加一个随机的值,比如60-120秒,这样每一个缓存同时失效的概率就会大大降低. 缓存穿透缓存穿透是指查询一个不存在的数据时,由于缓存只会在未命中且查询到结果时写入,并且出于容错性考虑,如果查到null时不写入缓存,就会导致这个不存在的数据请求每次都会落在数据库上,从而导致缓存失去意义. 如果有人利用不存在的key频繁攻击我们的应用,这就是一个漏洞. 如何解决解决缓存穿透的方法有好几种.最常见的方法是使用布隆过滤器.把所有存在数据的集合(可以是主键或者摘要)放入布隆过滤器中,每次查询数据库之前通过布隆过滤器判断数据是否不存在,从而把请求拦截掉. 布隆过滤器布隆过滤器（BloomFilter）是1970年由布隆提出的一种空间效率很高的随机数据结构，它利用位数组很简洁地表示一个集合，并判断一个元素是否属于这个集合. 布隆过滤器的特性是,能够准确判断一个元素不在某个集合内,但如果一个元素在集合内,布隆过滤器也有可能返回不存在. 也就是说,虽然布隆过滤器不适用于对错误”零容忍”的场合.但能够完美适配判断数据是否不存在的缓存穿透场景. 缓存击穿对于一些设置了过期时间的key,这些key可能在某一时间被超高并发的访问. 那么当这个key过期时,恰好在这个时间点有大量的缓存进来, 这些请求发现缓存过期时,一般会从数据库查询数据并且设置缓存.这时候大量的并发请求可能会瞬间把数据库冲垮. 如何解决就我目前知道的,有两种比较合适的解决方案: 对于热点数据设置永不过期,定时去更新缓存 使用这种方式的前提是,缓存数据对时效性要求不敏感,能够容忍一段时间内的数据更新对外部不可见.另外,需要提前知道哪些数据是热点数据. 这种方式比较适用于官网首页展示和仪表盘这类场景. 为查询操作添加互斥锁 这里锁的粒度最好是key值,这样就能保证在某一时间对某个key的多次查询,只会有一个落在数据库上.","link":"/posts/4dcxf120.html"},{"title":"说说redis的数据过期策略和内存淘汰机制","text":"redis的过期策略在我们设置redis key 的时候,一般会设置一个过期时间.不设置过期时间的情况下,默认是永不过期. 所谓的过期策略是指,当redis中的key过期时,redis如何处理. redis中使用的过期策略有两种 : 惰性删除 当key过期时,不做处理.每次从数据库获取key时,检查key是否过期.若过期,则删除,返回null 惰性删除对cpu是友好的,因为删除操作只在获取key时发生. 它的缺点是,对内存够不友好.如果有大量的key长时间没有被访问,那么这些key就永远不会被删除.可能因此导致内存泄露 为了弥补惰性删除的缺点,redis中还引入了定期删除的策略. 定期删除 每隔一段时间(默认100ms)随机抽取一部分设置了过期时间的key,若过期,则删除. 定期删除弥补了惰性删除的缺点.Redis会通过定期删除策略定期主动淘汰一批已过期的key. redis 的内存淘汰策略当redis达到内存最大限制时,redis可能会自动清除旧的数据. 默认情况下 memcached 使用的就是这种方式. 而redis会根据配置的淘汰策略,来决定具体的行为. 当前版本,redis支持6种内存淘汰策略: 不删除策略(noeviction) 默认的内存淘汰策略.在达到内存最大限制时,如果执行了需要更多内存的操作,则直接返回错误信息. 大多数写命令都会导致占用更多的内存.少数命令(如 DEL )例外. 优先删除最近少使用的key策略(allkeys-lru) 在达到内存最大限制时,通过近似LRU算法删除最近最少使用的key. 优先删除最近少使用的带过期时间key策略(volatile-lru) 和allkeys-lru策略类似,但是只针对设置了过期时间的key进行淘汰. 随机删除策略(allkeys-random) 在达到内存最大限制时,随机删除一部分key. 随机删除带过期时间key策略(allkeys-random) 在达到内存最大限制时,随机删除一部分设置了过期时间key. 过期时间顺序淘汰策略(volatile-ttl) 根据过期时间顺序,优先删除剩余过期时间短的key. 一般来说: 如果redis被用于缓存,那么推荐volatile-lru策略.通过使用频率决定淘汰的顺序 如果redis被用于持久化存储业务数据的中间件(如 分布式锁 队列 基数统计 等功能).建议使用默认策略.由程序员来决定要淘汰哪些key 如果既有缓存,又需要持久化,那么建议使用两个不同的redis集群. 设置redis内存最大限制目的是把redis使用的内存最大值限制在一个范围内.通过redis.conf 进行配置 12# 编辑redis配置文件，加入最大内存使用限制，我根据服务器的情况设置为3G maxmemory 3221225472 redis中的近似LRU算法 LRU 是 Least Recently Used 的缩写，即最近最少使用，是一种常用的淘汰算法，选择最近最久未使用的元素予以淘汰. LRU的实现方式很简单.把数据存储在链表里,每当访问一个元素时将它移动到链表的顶端.当元素数量超出链表的容量限制时,则将链表尾部的数据丢弃掉.Java里的LinkedList就是这么干的. 但是,对于可能存储大量元素的redis来说,使用链表的代价实在是太大了,需要巨量的存储空间. 目前redis使用的是近似LRU算法.毕竟,对于redis来说,并不需要一个完全准确的LRU算法,就算移除了一个最近访问过的Key,影响也不大. redis中的近似LRU的实现方式当内存满了需要淘汰数据时，会调用 dictGetSomeKeys 选取指定的数目的key，然后更新到eviction pool里面. eviction pool是一个数组，保存了之前随机选取的key及它们的idle时间，数组里面的key按idle时间升序排序. 如果选取的key的idle时间比eviction pool里面idle时间最小的key还要小，那么就不会把它插入到eviction pool里面. 然后淘汰掉eviction pool里idle时间最大的key. 通过采用eviction pool，把一个全局排序问题 转化成为了局部的比较问题.这个算法还是值得借鉴的.","link":"/posts/4dcdfl1c.html"},{"title":"Spring Cloud 使用Nacos作为配置中心","text":"什么是nacosNacos 是阿里巴巴开源的一个更易于构建云原生应用的注册中心、配置管理和服务管理平台 为什么需要配置中心首先说说什么是配置中心.顾名思义,就是用来管理项目中所有模块的配置的系统.虽然听起来很简单.但是也不要小瞧了这个模块.如果一个中大型互联网项目,不使用配置中心的模式.一大堆的各类配置项，各种不定时的修改需求，一定会让开发非常头疼且管理十分混乱. 那么使用nacos作为配置中心有哪些好处呢? 在属性上使用@NacosValue注解,实现动态刷新配置,消除了配置变更时重新部署应用和服务的需要,让配置管理变得更加高效和敏捷. 配置支持版本控制.可以查看配置的历史记录(最多30天).方便追溯,还原对配置的修改. 支持灰度发布.Nacos 支持以IP为粒度的灰度配置. 更安全.配置跟随源代码保存在代码库中，容易造成配置泄露.配置保存在nacos中,可以防止配置泄露. 如何在spring-boot中使用nacos作为配置中心启动nacos 通过github下载nacos nacos/releases 下载解压后进入bin文件夹,运行命令nohup sh startup.sh -m standalone &amp;&gt;/dev/null &amp;后台启动nacos 访问http://serverhost:8848/nacos/index.html 即可看到nacos管理页面,默认账号密码nacos/nacos 配置namespace通过namespace实现多环境下部署不同配置. 新建命名空间.进入nacos管理页面 &gt;命名空间&gt; 新建命名空间 填写命名空间名和描述”test” “测试环境配置” 创建完成以后的列表页面 配置dataId通过dataId为每一个服务创建一个配置,或者让多个服务使用同一套配置 点击配置管理 &gt; 配置列表 &gt; +创建新配置 点击发布创建配置成功 通过maven配置引入相关依赖添加依赖(这里本人使用的配置版本是 0.2.1 ): 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;nacos-config-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;${nacos-config-spring-boot.version}&lt;/version&gt;&lt;/dependency&gt; 开启配置使用 @NacosPropertySource 加载 dataId 为 pay-service 的配置源，并开启自动更新 123456789@SpringBootApplication@NacosPropertySource(dataId = \"pay-service\", autoRefreshed = true)public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); }} 在工程中配置nacos地址和namespace编辑application-{环境}.yml 12345nacos: config: server-addr: xx.xx.xxx.xxx:8848 #这里填写的是命名空间id.每个环境一个命名空间,从而实现多环境配置. namespace: a27fad67-3fc0-4560-817a-b5f7db337656 设置属性值通过 Nacos 的 @NacosValue 注解向实体类注入属性值 1234567@Data@Generated@Componentpublic class ErpConfig { @NacosValue(value = \"${pay.erp.contract.path.getContractByIdCard}\", autoRefreshed = true) private String erpPathGetContractByIdCard;} 使用的时候可以 12@Autowiredprivate ErpConfig erpConfig; 通过get方法获取值. 亦可直接在spring-bean内部使用@NacosValue直接设置属性值. nacos实现动态刷新配置的原理Nacos 的客户端维护了一个长轮询的任务，去检查服务端的配置信息是否发生变更，如果发生了变更，那么客户端会拿到变更的 groupKey 再根据 groupKey 去获取配置项的最新值即可.","link":"/posts/8589b7e.html"},{"title":"Spring Cloud 如何在微服务之间透传HttpHeader","text":"什么是透传其实很简单,引用&lt;&lt;维基百科&gt;&gt;中的定义: 透传，即透明传输（pass-through），指的是在通讯中不管传输的业务内容如何，只负责将传输的内容由源地址传输到目的地址，而不对业务数据内容做任何改变。 如何在微服务之间透传HttpHeader两种方式: 通过在请求上,或者在类上添加@Headers注解 通过实现RequestInterceptor接口,对特定的请求设置Header 通过添加注解的方式123@Headers({\"Content-Type: application/json\",\"Outer-Key: {OuterKey}\"})@PostMapping(value = \"/card-blank/batch-create\")Response batchCreateCard(@RequestBody CreateCardBlankDTO condition,@Param(\"OuterKey\") String type); 使用 Header : {参数名称} 可以传递动态header属性 2.使用RequestInterceptor在@FeginClient上引入配置类configuration = FeignConfig.class,使用配置类的好处是可以复用配置. 12@FeignClient(value = \"bas\", configuration = FeignConfig.class)public interface BasRemoteProxy { FeignConfig.class类: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354@Configurationpublic class FeignConfig { @Bean @Primary public RequestInterceptor requestInterceptor() { return new HeaderInterceptor(); } /** * 保证Hystrix熔断时仍可以传递信息 */ @Bean @Qualifier(\"contractfeignHystrixConcurrencyStrategy\") public FeignHystrixConcurrencyStrategy contractfeignHystrixConcurrencyStrategy() { return new FeignHystrixConcurrencyStrategy(); } public static class HeaderInterceptor implements RequestInterceptor { /** * 服务间需要透传的头 */ private final Set&lt;String&gt; HEADER_NAMES = new HashSet&lt;&gt;(Arrays.asList(\"tenant_code\", \"oauth2-authentication\", \"oauth2-authority\")); public HeaderInterceptor(String ...args) { HEADER_NAMES.addAll(Arrays.asList(args)); } @Override public void apply(RequestTemplate requestTemplate) { // 微服务透传header ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder .getRequestAttributes(); if (null == attributes || null == attributes.getRequest()) { return; } HttpServletRequest request = attributes.getRequest(); Enumeration&lt;String&gt; headerNames = request.getHeaderNames(); if (headerNames != null) { while (headerNames.hasMoreElements()) { String headerName = headerNames.nextElement(); if (HEADER_NAMES.contains(headerName)) { String value = request.getHeader(headerName); requestTemplate.header(headerName, value); } } } } }}","link":"/posts/bc105261.html"},{"title":"自动拆装箱介绍以及可能引起的问题","text":"什么是自动拆装箱?在说问题之前,先介绍一下这个比较基础的概念. 自动装箱自动装箱,目的是将原始类型值转自动地转换成对应的对象.由编译器提供支持. Java为每种基本数据类型都提供了对应的包装器类型，至于为什么会为每种基本数据类型提供包装器类型在此不进行阐述，有兴趣的朋友可以查阅相关资料。在Java SE5之前，如果要生成一个数值为10的Integer对象，必须这样进行： 1Integer i = new Integer(10); 而从java1.5开始就提供了自动装箱的特性,只需要这样就能生成一个数值为10的Integer对象: 1Integer i = 10; 这个过程中会自动根据数值创建对应的 Integer对象,通过反编译字节码可以看出,编译器会把Integer i = 10 转换成Integer i = Integer.valueOf(int),这就是自动装箱. 自动拆箱那什么是拆箱呢？顾名思义，跟装箱对应，就是自动将包装器类型转换为基本数据类型.就像这样: 1Integer i = 10; 简单来说就是,编译器会自动把包装类型转换成基本数据类型. 拆箱过程是通过包装器类型的共有父类Number的xxValue方法实现的. 比如intValue(),longValue(),doubleValue(). 拆装箱的时机 赋值操作 ==,+,-,*,运算时 注意,当 == 的两个对象都是包装器的引用时,则是比较指向的是否是同一个对象.如果要比较两个对象的值,需要使用equals() 而如果其中有一个操作数是表达式（即包含算术运算）则比较的是数值（即会触发自动拆箱的过程. 自动拆装箱与缓存Integer是包装类，Integer i = num，自动装箱而num值的范围如果取(-128&lt;=num&lt;=127)，那么就在IntegerCache中直接取已经创建好的对象，不会创建新的Integer对象. 也就是如果对两个-128&lt;=num&lt;=127的Integer类型做 == 比较,和通过equals()比较的结果是一样的. 自动拆装箱可能引起的问题自动拆装箱介绍完了.那么说一下我今天碰到一个坑,就是由于自动拆装箱的特性导致的问题. 通过上文我们已经知道,自动拆装箱的调用时机.那么自动拆装箱在反射中的表现是怎么样的呢? 先上结论 通过反射获取方法时,不支持自动拆装箱 通过反射调用方法时,不支持自动拆装箱 通过反射获取构造器时,支持自动拆装箱 例子原有方法A1234//注意,参数`id`的类型是longpublic String getIdentity(long id) { return Math.toIntExact(channel.getCode(id));} 通过反射获取getIdentity()方法12tClass.getMethod(\"getIdentity\",Long.class);//执行到这一行时抛出java.lang.NoSuchMethodException 可以看出,反射获取方法时不支持泛型的.具体是什么原因呢？跟踪JDK源码,来到Class#searchMethods(): 123456789101112131415161718private static Method searchMethods(Method[] methods, String name, Class&lt;?&gt;[] parameterTypes){ Method res = null; String internedName = name.intern(); for (int i = 0; i &lt; methods.length; i++) { Method m = methods[i]; //先判断方法名是否相等,再判断方法参数是否相等,最后判断方法返回值是否符合规则 if (m.getName() == internedName &amp;&amp; arrayContentsEq(parameterTypes, m.getParameterTypes()) //这一行是重点 &amp;&amp; (res == null || res.getReturnType().isAssignableFrom(m.getReturnType()))) res = m; } return (res == null ? res : getReflectionFactory().copyMethod(res));} 重点在arrayContentsEq(parameterTypes, m.getParameterTypes()),这个方法是用来判断方法的参数列表是否符合我们需要的类型.我们来看代码: 123456789101112131415161718192021private static boolean arrayContentsEq(Object[] a1, Object[] a2) { if (a1 == null) { return a2 == null || a2.length == 0; } if (a2 == null) { return a1.length == 0; } if (a1.length != a2.length) { return false; } //循环比较参数类型 for (int i = 0; i &lt; a1.length; i++) { if (a1[i] != a2[i]) { return false; } } return true;} 注意第14行的循环,把方法的实际参数列表和我们需要的参数列表作比较,如果完全符合则返回true.有任何一个参数不符合则返回false. 我们在arrayContentsEq()方法内部打上断点,可以观察到通过反射获取到的 getIdentity()方法参数只有一个 long.class. 而我们提供的参数是Long.class,由于两个参数类型不相同,jdk找不到对应的方法,所以抛出了异常. 解决问题由于获取方法不支持自动装箱或拆箱，那么我们在使用反射时候很容易错误，如何避免这个问题呢？ 答案：智者借力而行，当然是用现有的轮子了。那就是Apache common beanutils. 使用Beanutils 中的 MethodUtils.getMatchingAccessibleMethod()获取对象,这个方法在内部做了兼容判断,解决了反射中自动拆装箱的问题. 查看源码,其中有一个getPrimitiveWrapper() 方法: 12345678910111213141516171819202122232425262728293031323334353637383940414243public static Class&lt;?&gt; getPrimitiveWrapper(final Class&lt;?&gt; primitiveType) { // does anyone know a better strategy than comparing names? if (boolean.class.equals(primitiveType)) { return Boolean.class; } else if (float.class.equals(primitiveType)) { return Float.class; } else if (long.class.equals(primitiveType)) { return Long.class; } else if (int.class.equals(primitiveType)) { return Integer.class; } else if (short.class.equals(primitiveType)) { return Short.class; } else if (byte.class.equals(primitiveType)) { return Byte.class; } else if (double.class.equals(primitiveType)) { return Double.class; } else if (char.class.equals(primitiveType)) { return Character.class; } else { return null; } } 就是做了一些if-else判断.如果原有类型是基本类型则返回包装类型.十分简单粗暴的实现方式. (不过看注释作者好像也对这个简单粗暴的方法不是很满意呢hhh.有没有更好的实现方式呢?)","link":"/posts/a6fa4b5a.html"},{"title":"用lua脚本实现redis原子事务","text":"redis事务redis事务的实现原理很简单,把事务里的命令放进队列中,当事务提交时redis会按照提交顺序执行每一条命令. redis事务的优点 使用简单,把一组命令放在multi和exec两个命令之间,multi和exec代表事务的开始和结束.通过discard和watch放弃执行剩下的命令. redis事务的缺点 默认情况下事务和普通命令修改同一个key时,首先修改的执行结果会被覆盖.无法保证数据的一致性. 通过watch命令监控某个key,如果在事务执行过程中这个key被修改那么直接返回失败.通过业务层处理保证数据的一致性. 在spring中实现redis事务的方式就是通过template.setEnableTransactionSupport(true)配置redisTemplate事务支持,通过在方法上添加@Transactional 注解实现声明式事务. 看起来好像合理,但是实际操作时发现坑有点多. 首先在事务过程中执行get操作是无效的,返回的永远是null. 为什么呢? 因为get的时候不会立刻执行命令,而是等待方法执行完以后,框架给我们exec的时候才会去真正执行并且返回结果. 也就是说,除非一个事务操作里只有写操作,没有读操作.否则不推荐使用spring的redis事务. lua脚本(redis2.6+)在被redis事务折磨了一段时间以后,我发现了lua脚本这个宝藏. 相比redis事务,通过redis执行lua脚本有哪些优点呢? 在redis中执行lua脚本会有阻塞.保证脚本执行过程中变量不会被其他事务修改,保证了脚本执行的原子性. 脚本复用.客户端可以把lua脚本永久保存在redis中达到脚本的复用. lua是高级语言.所以可以通过脚本封装一些业务逻辑和运算操作,形成一个原子事务. 通过lua实现一个tryLock的操作: 12345678910111213141516--KEYS[1] = 表示锁的集合--ARGV[1] = 过期时间--ARGV[2] = 锁的名称if (redis.call('exists', KEYS[1]) == 0) then redis.call('hset', KEYS[1], ARGV[2], 1); redis.call('pexpire', KEYS[1], ARGV[1]); return nil; end;--if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then redis.call('hincrby', KEYS[1], ARGV[2], 1); redis.call('pexpire', KEYS[1], ARGV[1]); return nil; end;--return redis.call('pttl', KEYS[1]);","link":"/posts/x228e33.html"},{"title":"在linux下配置swap分区","text":"如何在linux下配置swap分区简单配置,做个备忘方便以后自己查阅使用. 1.创建swap文件1234#在/var/swap下创建swap文件#大小是2048000bytes#/var/swap 可以替换成自己喜欢的地址$ dd if=/dev/zero of=/var/swap bs=1024 count=2048000 2.设置swap文件12345678#格式化swap文件$ mkswap -f /var/swap#加载sawp文件$ swapon /var/swap#关闭swap$ swapoff /var/swap#开机启动挂载echo \"/var/swap swap swap defaults 0 0\" &gt;&gt; \"/etc/fstab\"","link":"/posts/714fb999.html"},{"title":"docker的大坑,无视系统防火墙暴露端口","text":"起因一直以来我都对服务器暴露的端口进行了严格的限制,只开放了80,443,22等少数几个端口.像mysql,mongodb这样的服务肯定是不能对外开放的.另外我还关闭了密码登录,只使用ssh公钥登录公网上的服务器. 而各个linux服务器的发行版都提供了方便的工具来配置防火墙,比如Ubuntu的ufw ,redhat &amp; centos的firewall 和iptables. 我呢,也就很放心的使用工具来配置防火墙.直到有一天我发现.redis被挂了定时调度的挖矿脚本.我觉得很奇怪.为什么内网的redis会被挂马. 排查问题的原因排查问题要从源头开始,我尝试用家里的电脑telnet 公网服务器ip 6379 发现居然可以直接访问到! 顿时我的冷汗就下来了.防火墙居然莫名其妙的失效了 ! 通过命令查看防火墙的状态: 123456789101112//查看防火墙状态# $ systemctl status firewalld● firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled) Active: active (running) since 四 2019-9-13 21:25:23 EST; 1 day 7h ago Main PID: 773 (firewalld) Memory: 1000.0K CGroup: /system.slice/firewalld.service └─773 /usr/bin/python -Es /usr/sbin/firewalld --nofork --nopid//查看开放的端口# $ firewall-cmd --list-ports443/tcp 80/tcp 可以看出,防火墙是 active 状态.端口只开放了443和80.emm. 好像看不出有啥问题的样子. 我又尝试着telnet了一下mongodb和mysql的端口,发现好像并不能访问到. 说明,防火墙其实还是有作用的.只是我们可以绕过防火墙访问redis,那么问题出在哪里呢? 1234567//查看iptables# $ iptables -L -n ... Chain DOCKER (1 references)target prot opt source destination ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:6339 ctstate NEW.. 原来是你小子(DOCKER).是谁给你的熊心豹子胆绕过系统防火墙直接操作iptables的! 联想到最近用docker重做了redis服务.这个锅应该应该是docker的没跑了. 问题分析为什么会出现这种情况呢?因为docker是基于宿主机的,而firewalld之类的防火墙是后来在iptables上封装的.所以docker的网络规则会直接无视掉非iptables的防火墙. 另外,把iptables改回去是没用的,下次docker启动就会改回来. 解决问题: 如果不想把服务暴露在公网上.可以在run时加上127.0.0.1 像这样的配置就不会有问题: 1-p 127.0.0.1:6379:6379/tcp 或者在docker-compose.yml里加上: 12ports: - 127.0.0.1:6379:6379/tcp","link":"/posts/822eebbb.html"},{"title":"用scala + netty 写一个rest服务器","text":"作为一个Programmer,要多多尝试新的技术. 老实说,一开始让我用scala写代码的时候,我是拒绝的. 因为之前听过的关于scala的传说里面,最让我印象深刻的就是它的难度.据说scala是”全宇宙唯一一个复杂度超过c++的预言”. 听起来是很厉害的样子.但是,我又实在是忍不住想尝试一下传说中的函数式是一等公民的语言是什么样的.所以忍不住尝试用scala + netty 写了一个rest服务器.. 写完以后,我的感想是…其实也没有也没那么复杂啦.抛开那些繁杂的特性,单纯的把scala当作lambda强化版的java,也是相当不错的. 核心类Dispatcher.scala 只用了七十多行代码就实现了url路由功能 1234567891011121314151617181920212223242526272829303132333435363738394041@Singletonclass Dispatcher { private[this] val log = LoggerFactory.getLogger(this.getClass) private[this] val allocator = UnpooledByteBufAllocator.DEFAULT private[this] var mapping: mutable.Set[MappingCase] = _ @Inject def this(scanner: MappingScanner) { this() mapping = scanner.routerMapping(Constant.mappingPackage) } def dispatch(req: FullHttpRequest): Option[DefaultFullHttpResponse] = { log.debug(\"dispatch uri :{}\", req.uri()) mapping .find(x =&gt; x.pattern.matches(req.uri()) &amp;&amp; x.method == req.method.toString.toUpperCase) .map(_.mappings) .flatMap(routingTo(req, _)) .orElse(strResponse(str = Constant.NOT_FOUND, status = HttpResponseStatus.NOT_FOUND)) } private def routingTo(req: FullHttpRequest, mapping: Mappings): Option[DefaultFullHttpResponse] = { try { mapping.execute(req).flatMap { case str: String =&gt; strResponse(str) case bytes: Array[Byte] =&gt; response(allocator.heapBuffer(bytes.length).writeBytes(bytes), contentType = \"application/octet-stream\") case ref: AnyRef =&gt; Option(ref).map(JSON.toJSONString(_)).flatMap(strResponse(_, contentType = \"application/json\")) case _ =&gt; strResponse(\"null\") } } catch { case e: Exception =&gt; log.error(\"execute mapping error:\\r\\n\", e) strResponse(str = Option(e.getMessage).getOrElse(\"exception with 417\"), status = HttpResponseStatus.EXPECTATION_FAILED) } }... 虽然scala可以直接使用java的库,但是,很多小细节都在提醒我,它是scala. 比如,scala的集合类是另外实现的一套,和java的集合类不兼容. 默认情况下 scala调用java方法 不支持协变.比如 调用add(Collection&lt;T&gt;)时不能用List&lt;T&gt;作为参数. scala的所有算术操作符都是函数.可以任意重载. scala支持元编程.可以自己实现一套基于scala的方言.同样是基于jvm的java却做不到这点. 总体来说,我认为scala的很多特性都很棒.用这些特性写代码会很舒服.能让代码更精简 和 减少很多低级错误. scala比java更强大,更超前. 有很多特性 是scala 先有 然后 java 才跟进的.比如多行字符串.lambda表达式 等等. 说scala比java超前 5 -10 年我觉得一点都不过分. 但是,scala不太适用于java web.强行使用scala写java web会比较痛苦. 比如当你遇到集合类的转换问题 和调用java方法不支持协变的问题的时候.为了处理好这些问题,写出来的代码一点都不优雅,甚至可以说很丑陋. 相对于scala而言, 能够和java无缝对接的 kotlin更适合用来写java web. kotlin是介于scala 和 java之间的语言. 按照编程语言能力来划分 kotlin的特性应该属于scala的子集. 有时间的话再来聊聊kotlin吧. 代码 : https://gitee.com/minagamiyuki/my-scala-dispatcher","link":"/posts/5f4e8de.html"},{"title":"记一次由于网关重试导致的生产事故","text":"前言简单介绍一下背景,我们组是做支付清算的.主要业务是划扣,出入金,结算,报表 这些. 刚起步半年多,用户规模只有不到20w.这是前提. 最早发现问题的是客服MM.周五晚上八点左右.有很多账期日的客户向客服投诉,他们绑定的银行卡被多划扣了一笔钱. 原本该从用户卡里划2000块 或者3000块,统统加倍,变成4000块 或者 6000块.卡里钱不够的就被三光了. 这是个很严重的问题.就说一下我们是如何处理这次问题的吧. 处理线上问题首先.我们关闭了自动划扣功能.已知划扣功能有问题的情况下还继续从用户卡里划钱就太冒险了. 然后是问题的影响范围.通过查询流水记录我们知道,被多次划扣的用户的账期日都是周五当天.略微松了一口气.我们整理了一份名单.通过这份名单可以给被多次划扣的用户退款. 有一些用户卡里正好没有钱,所以幸免了.其中有一部分用户的名字 经常 在催收mm的咆哮中出现.但是今天不太一样,他们的名字看起来特别亲切. 接下来是找到问题的原因. 直接原因是,因为Zuul网关重试导致划扣api被调用了2次.从而导致用户被划了2笔钱. 解释一下网关重试为什么会导致多次划扣.我们服务端是springcloud 微服务架构,自动划扣是由调度服务发起的.调度服务的架构大概是这样: 众所周知微服务互相调用需要走网关.通过网关调用微服务有个机制是,如果一段时间被调用的服务没有响应,那么网关会自动发起重试.默认的周期大概是30秒. 从划扣服务的接口日志来看,是因为接口被多次调用.导致重复划扣.再看调度服务的日志,只发起了一次划扣请求.那么问题就只能是由于网关重试导致的了. 剩下的是验证 &amp; 解决问题. 知道问题原因以后,处理起来就简单啦. 验证 : 写个api,内部 Thread.sleep() 阻塞30秒,打印日志.通过调度中心调用它.看看是否会出现调用2次的情况.验证成功. 解决问题 : 通过Redis为划扣操作加上互斥锁 .保证同一个用户不会被重复划扣. 当然,挨骂肯定是少不了的.关键还是要吸取教训,不能被一块石头绊倒2次.","link":"/posts/9b7c1620.html"},{"title":"那些酷炫实用好玩的idea插件","text":"推荐一些我收集的酷炫实用好玩的idea插件. 1.TabNinetabNine是一款基于深度学习的,支持所有语言的智能代码自动补全工具. 一般来说,我们会使用IDE自带的提示工具.对于TabNine来说,它的优点就是提示更加精确. 老实说我被它的提示的准确度惊讶到了.写java代码的时候可能给人的感觉不是特别惊艳,因为IDEA自带的Java提示已经足够强大了. 但是在你写python,javascript这样的动态语言的时候,tabNine会让你感到惊讶:这是怎么做到的? 它是怎么知道我想输入什么的? 推荐指数: ★★★☆(java) ​ ★★★★☆(其他动态语言) 2.GenerateAllSetter看名字应该不难猜到它的用途. 这是一款国人开发的插件.通过它可以一键生成JavaBean的setter方法.还支持生成默认值.使用方法就像下面这张图里一样. 拥有它,可以节省很多无意义的编码时间,虽然功能简单但是非常实用,值得推荐. 推荐指数: ★★★★★ 3.Lombok以前的Java项目中，充斥着太多不友好的代码：POJO的getter/setter/toString/equalsAndHashcode;logger == getLogger;非空判断,I/O流的关闭操作等等，这些样板代码既没有技术含量，又影响着代码的美观，所以Lombok应运而生. 有了Lombok以后只需要一个注解,就能轻松实现上面那些内容了. 这个插件的作用是提供IDE对lombok的支持.使得lombok在编译阶段起作用. 有人对这类插件表示担忧,他们认为过于依赖这类插件不了解注解底层的原理可能会有意想不到的效果.这点我是同意的. 但是我认为弄清楚插件的原理和享受插件带来的方便并不冲突. 是否要引入像lombok这样只要引入一次就能永久受益的插件还是自己权衡吧. 推荐指数: ★★★★★ (用lombok注解实现的pojo.非常简洁清晰美观) 4. .ignore这个插件功能很简单,可以一键把文件/目录加入.gitignore中,如果没有.gitignore文件还会自己创建. 推荐使用git的用户安装它. 推荐指数: ★★★ 5.RestfulToolkit这个插件是用于接口自测的.可以帮助生成接口mock参数,还可以根据url跳转到对应的方法定义.支持springboot. 安装后，右侧会有RestServices侧边栏，点击后会显示当前项目所有请求地址，可以进行输入查询，然后会直接把请求方式，地址以及参数列出来，默认请求服务器为本机（localhost:8080）,可根据需求更改. 查看接口列表&amp; 一键发送请求(图片来自转载): 它的最大优点是,直接集成于项目，不需要输入地址以及多个参数，便能直接访问. 我推荐它的原因是.用它做测试接口真的很方便,完全开箱即用.工期赶的时候甚至连单元测试都可以不用写了hhhh(小伙伴们不要学哦) 推荐指数: ★★★★ 6.activate-power-mode这是一款让你在写代码的时候,整个IDE都为之颤抖的插件.就像下面这张图里一样: 不解释了.赶紧上车吧. 推荐指数: ★★★★ 7.SonarLintSonarLint是一个代码质量检测插件.可以帮我们检测代码中的BUG,不规范的写法,还能提供最佳实践的提示. 相比IDE自带的代码异常提示,SonarLint的提示范围要更大.也就是IDE自带工具检测不出来但是确实存在的问题,SonarLint可以.通过警告提示告诉你,你的代码有了问题. 而且 对于每个问题,SonarLint都给出了示例，还有相应的解决方案，教我们怎么修改，极大的方便了我们的开发. 另外,SonarLint还能无缝对接SonarCube,同步自定义的代码规则. 如果公司使用SonarLint作为代码质量检测工具,那么SonarLint绝对是代码质量检测插件的首选. 推荐指数: ★★★★ 8.GsonFormat可以一键将json字符串转化成pojo的插件. 支持别名.支持fastjson,jackjson,gson等反序列化工具. 虽然不能百分百转换成想要的类型(比如JSON有null value的情况下),但是允许自己手动进行修正. 当你需要对接一个100+字段的接口的时候.GsonFormat会让你感到欣慰. 效率神器,强烈推荐. 推荐指数: ★★★★","link":"/posts/c32a0416.html"},{"title":"spring-boot配置基于Redisson的分布式对象网格","text":"Redisson官方介绍Redisson是一个在Redis的基础上实现的Java驻内存数据网格（In-Memory Data Grid）。它不仅提供了一系列的分布式的Java常用对象，还提供了许多分布式服务。其中包括(BitSet, Set, Multimap, SortedSet, Map, List, Queue, BlockingQueue, Deque, BlockingDeque, Semaphore, Lock, AtomicLong, CountDownLatch, Publish / Subscribe, Bloom filter, Remote service, Spring cache, Executor service, Live Object service, Scheduler service) Redisson提供了使用Redis的最简单和最便捷的方法。Redisson的宗旨是促进使用者对Redis的关注分离（Separation of Concern），从而让使用者能够将精力更集中地放在处理业务逻辑上。 配置pom.xml: 123456&lt;!-- version:3.12.1 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;${redisson.version}&lt;/version&gt;&lt;dependency&gt; application.yml: 12345678910redis: host: redis-host port: 6379 password: meiyoumima jedis: pool: max-idle: 10 min-idle: 0 max-active: 16 max-wait: -1 RedisConfig.java: 1234567891011121314151617181920/** * @Author: lzh * @Date: 2020/2/2 1:21 下午 */@Configurationpublic class RedisConfig { @Bean public RedissonClient redissonClient(RedisProperties redisProperties) { Config config = new Config(); SingleServerConfig singleServerConfig = config.useSingleServer().setAddress(\"redis://\" + redisProperties.getHost() + \":\" + redisProperties.getPort()); if (StringUtils.isNotBlank(redisProperties.getPassword())) { singleServerConfig.setPassword(redisProperties.getPassword()); } RedissonClient redissonClient = Redisson.create(config); return redissonClient; }} 使用1234567891011121314151617//注入bean@Autowiredprivate RedissonClient redissonClient; /** * 一键获取各种分布式的java常用对象和服务 */ public void test(){ //获取读写锁 ReadWriteLock readWriteLock = redissonClient.getReadWriteLock(\"LOCK_NAME\"); //获取独占锁 Lock lock = redissonClient.getLock(\"LOCK_NAME\"); //获取map Map&lt;Object, Object&gt; map = redissonClient.getMap(\"MAP\"); //获取布隆过滤器 RBloomFilter&lt;Object&gt; bloomFilter =redissonClient.getBloomFilter(\"BLOOM_FILTER\"); }","link":"/posts/a880503f.html"},{"title":"在mysql中如何选择合适的数据类型","text":"先上总结​ • 更小的字段通常更好 .一般情况下,应该尽量使用可以正确存储数据的最小数据类型.​ • 简单的类型比复杂类型的操作消耗的CPU周期少.比如 整型比字符串操作代价更低.因为字符集和校对规则(排序规则)使字符串比整型更复杂.​ • 尽量避免NULL值.如果查询条件中包含可为NULL的列,对Mysql来说更难优化.因为可为NULL的列使得索引和值比较都更加复杂.(但是,把可为NULL的列改为NOT NULL 对性能提升比较小.所以在查询优化时不需要优先考虑)​ • 存储IPV4地址时,使用无符号整数,而不是varchar(15).只需要4个字节,节省存储空间,同时效率高很多.Mysql提供inet_aton()和inet_ntoa()函数在这两种表示方法之间转换​ • 在选择标识列(ID Column),整数类型通常是最好的选择.因为它们很快并且可以使用自增值.尽量避免使用字符串类型,特别是在MyISAM中,mysql默认对字符串使用压缩索引,这会导致查询慢许多.​ • 如果存储UUID值,则应该移除 “ - “ 符号.或者更好的做法是,用hex()函数转换uuid为16字节的数字,并且存储在一个binary(16)中.检索时可以通过hex()函数将id值格式化为16进制格式.通过unhex()获取原值. 1.时间类型 Datetime 和Timestamp都能存储相同类型的数据,并且精确到秒.然而Timestamp只使用Datetime一半的存储空间,并且会根据时区变化,具有自动更新的能力.但是Timestamp允许的时间范围要小很多.(1970.1.1 - 2038.1.18).Datetime类型可以保存大范围的值,从1001年到9999年,精确到秒. 2.整数类型可以用来存储实数和整数.tinyint smallint mediumint int bigint 分别占用 8,16,24,32,64位存储空间.有可选的unsigned属性,表示不允许负值,可以使正数的上限提高一倍. 3.实数类型包含了小数部分.然而,它们不只是用来存储小数,也可以使用decimal来存储比bigint还大的整数.float 和double支持标准的浮点运算.decimal用来存储精确的小数.因为需要额外的空间和计算开销.所以应该尽量只在需要对小数进行精确计算时使用decimal. 4.字符串类型varchar和char是最主要的两种字符串类型.varchar用来存储可变长字符串,是最常见的字符串类型.char的字符串是定长的,适合存储很短的字符串,或者定长的字符串.例如char很适合用来存储MD5摘要,因为这是一个定长的值.对于经常变更的数据,char也比varchar更好.因为定长的char不容易产生碎片.(注意 : 更长的列会消耗更多的内存,因为Mysql会分配固定大小的内存块用来保存内部值,尤其是在使用临时表排序时.所以最好只分配真正需要的空间) 5.大数据类型blob和text都是为了存储大数据而设计的字符串数据类型.分别才用二进制和字符串方式存储.与其他类型不同,mysql会把blob 和 text当做独立的对象来处理.存储引擎在存储时会做特殊处理,当blob和text值太大时,InnoDB会使用专门的外部存储区域进行存储.blob和text唯一的差别是,blob存储的是二进制数据,没有字符集和排序规则.而text有字符集和排序规则.blob和text的排序规则 和其他数据类型不同,它们只对每个列最前max_sort_length做排序,而不是整个字符串.mysql不能使用blob 和 text列全部长度的字符串进行索引,也不能使用这些字符串索引排序. 6.枚举类型有时候可以使用枚举列enum代替常用的字符串类型.枚举可以把一些不重复的字符串存储成一个预定义的集合,mysql在存储枚举时非常紧凑,会根据列表值的数量压缩到1-2字节.mysql会在存储每个值时存储整数.并且在排序时根据内部存储的整数进行排序,而不是定义的字符串.枚举的问题在于,字符串列表是固定的.因此添加或者删除字符串必须使用alter table.因此,对于一系列可能会改变的字符串.使用枚举不是一个好主意.除非你能接受只在列表末尾添加元素,在mysql5.1以后不需要重建整个表来完成修改 7.位数据类型mysql有少数几种存储类型使用紧凑的位存储数据.所有这些位类型,不管底层存储格式和处理方式如何,从技术上来说都是字符串类型.bit类型可以存储一个或多个true/false值.bit(1)定义一个包含1个位的字段,bit(2)存储两个位,以此类推.set类型可以有零或多个值.它的行为跟枚举类似,其值来自表创建时规定的允许的一列值,改变列的定义需要alter table.这对大表来说是非常昂贵的操作.虽然在set上无法使用索引查找,但是mysql提供了像find_in_set()和field()这样的函数,方便在查询中使用一种替代set的方法是,使用一个整数包装一系列的位.例如,把8个位包装到一个tinyInt中,并按照位操作来使用.比起set,这样使用的好处是可以不使用alter table 改变字段的枚举值,缺点是查询语句更难写,而且更难理解.因此是否采用这种技术取决于个人的偏好. 8.二进制类型二进制类型是在数据库中存储二进制数据的数据类型.二进制类型包括binary，varbinary，bit，tinyblob，blob，mediumblob，longblob。binary类型的长度是固定的,不足最大长度的空间由‘\\0’补全.varbinary的长度是可变的,在创建时指定了最大长度.varbinary类型实际占用的空间为实际长度加一。这样，可以有效的节约系统的空间。","link":"/posts/b82125f7.html"},{"title":"关于mysql事务四大特性的个人理解","text":"什么是数据库的事务？事务其实就是单个数据逻辑单元组成的对象操作集合，而数据库的终极目标就是使数据库从一个一致的状态转换到另一个一致的状态，这就是ACID中的一致性（Consistency），而原子性（Atomicity）、隔离性（Isolation）、持久性（Durability）是为了实现这个目标的手段。 原子性 一个事务是一连串的操作组成，增删改查的集合。 而事务的原子性就要求整个事务中的所有操作要么全部成功,要么全部失败回滚.对于一个事务而言,不可能只执行其中的一部分操作.所以原子性就好比一个Java方法,要么所有语句全部执行成功,要么执行失败回滚.不会出现执行一半的情况. 持久性好的，这时候我们假设原子性保证了.一个事务未提交的时候，发生了错误就执行rollback，那么事务就不会提交了. 但是当我们事务执行成功了，执行commit指令之后，遇到了错误会怎么样？ 我们都知道 提交事务以后会将数据刷盘,,进行持久化操作.而在这个刷盘过程中出现停电,宕机等可以中断刷盘的操作,那么这个过程中会不会出现有一半数据刷盘成功,另一半没刷进去? 当然不会,事务的持久性保证了事务一旦完成,它对数据库的改变是永久性的,即使系统出现故障. 隔离性原子性，持久性是事务一致性的充分条件,但是还无法构成必要条件.这两个都解决了,接下来就是并发访问时的一致性问题了. 回到隔离性，很显然事务的隔离性，说的就是，事务并发时,事务的修改结果在什么时候能被其他事务看到. 在java中我们使用锁和volatile关键字来保证其他线程不会读取到修改变量的中间值,只能读取到初始值和修改完成以后的值.但是这些都是你根据业务来操作的，所以数据库为我们封装了‘四把锁’，对应四种隔离级别： 读未提交，其隔离级别最低，允许脏读。换句话说就是，如果一个事务正在处理某一数据，并对其进行了更新，但是同时没有提交事务，允许另一个事务也可以访问 读已提交，和读未提交的区别就是。读未提交可以读取到别人没有提交的数据，但是读已提交只能读取到别人提交后的值，事务进行的中间值不会读取到 可重复读，简单来说就是事务处理过程中多次读取同一个数据的时候，这个值不会发生改变，其值都和第一次查询到的数据是一致的 可串行化，是最严格的隔离级别，他要求所有的事务都被串行执行，既事务只能一个接一个的进行处理，不能并发执行 这样原子性、持久性、隔离性(锁机制以及各种并发安全控制机制)和事务一致性就构成了充分必要条件","link":"/posts/151a190a.html"},{"title":"mysql索引类型和优化","text":"关于Msql的索引类型先解释一下索引的重要性,如果正确合理的设计并使用索引的Msql是一辆兰博基尼的话,那么没有设计和使用索引的Mysql就是一辆破三轮车. 对于没有索引的表,单表查询可能几十万条数据就是瓶颈了.原因是在查询时,两张无关联条件的表会因为全表扫描发生笛卡尔积.假设表a有1000条数据,表b有2000条数据,那么扫描的行数就是1000 * 2000 = 2000000行. 当然,在数据量真的很少的情况下,会出现笛卡尔积比走索引快的情况.这种情况下也就不用考虑所谓的优化问题了. 而一般的大中型网站单日可能就会产生几十万甚至上百万条的数据,没有索引查询就会变得非常缓慢. B-tree索引(默认索引) Btree是一种是为磁盘等外存储设备设计的一种平衡查找树.在create table时,如果没有指定索引类型,会默认使用该类型.大多数mysql引擎都支持这种索引.不同的存储引擎会以不同的方式使用b-tree索引.性能也各有不同,各有优劣.例如 MyISAM使用前缀压缩技术使得索引更小.而InnoDB则按照源数据格式进行存储.再如MyISAM通过数据的物理位置引用被索引的行,而InnoDB则根据主键引用被索引的行.B-tree意味着所有值都是按顺序存储的.并且每一个叶子页到根的距离相同.B-tree索引能够加快数据的访问速度,因为存储引擎不再需要扫描全表,取而代之的是通过索引的根节点进行搜索.B-tree索引支持的查询类型 : 全列匹配.也就是和索引中的所有列进行匹配. 最左前缀匹配.也就是可以使用索引最左开始的n个列进行查询. 匹配范围值. B-tree索引的限制 : 最左前缀原则.也就是最左优先,在检索数据时从联合索引的最左边开始匹配 举个例子 : 123456#建立索引indexs(col1,col2,col3)#这个联合索引实际上相当于建立了(col1),(col1,col2),(col1,col2,col3) 3个索引KEY indexs on test(col1,col2,col3);#现在有如下查询SELECT * FROM table1 WHERE col1=“1” AND clo2=“2” AND clo4=“4”#那么上面这个语句会遵循最左前缀原则,检索时会使用(col1,col2)进行匹配 不能跳过索引中的列.假设有联合索引A,B,C,D,如果查询条件不指定C,则Mysql只能使用索引的A,B列.因为这些限制,在优化性能时,可能需要创建相同的列但顺序不同的索引来满足不同类型的查询需求. 举个例子 : 123456#建立索引indexs(col1,col2,col3)#这个联合索引实际上相当于建立了(col1),(col1,col2),(col1,col2,col3) 3个索引KEY indexs on test(col1,col2,col3);#现在有如下查询SELECT * FROM table1 WHERE col1=“1” AND clo3=“2”#因为缺少col2,所以检索时只能使用(col1)进行匹配 哈希索引哈希索引基于哈希表实现.只有精确匹配索引的所有列的查询才有效.对于每一行数据,哈希索引都会根据所有的索引列计算一个hash code.哈希索引会将所有的hash code存储在索引中,同时在哈希表中保存指向每个数据行的指针. 因为索引本身只存储hash code,所以索引的结构十分紧凑,这也让哈希表的速度非常快.然而.哈希表也有它的限制: 不支持排序. 不支持覆盖索引. 不支持通过部分索引列进行查找. 不支持范围查询,只支持等值比较查询. 哈希碰撞可能导致的性能下降问题. 存储的是指向数据行的指针，所以不支持覆盖索引.因为这些限制,哈希表只适用于某些特定的场合.而一旦适用哈希索引,它带来的性能提升会非常显著.比如,在关联多个表查询时的中间表,哈希索引就非常适合它的需求. 全文索引MySQL从3.23.23版开始支持全文索引和全文检索，FULLTEXT索引仅可用于 MyISAM 表； 他们可以从CHAR、VARCHAR或TEXT列中作为CREATE TABLE语句的一部分被创建，或是随后使用ALTER TABLE 或CREATE INDEX被添加。 对于较大的数据集，将你的资料输入一个没有FULLTEXT索引的表中，然后创建索引，其速度比把资料输入现有FULLTEXT索引的速度更为快。不过切记对于大容量的数据表，生成全文索引是一个非常消耗时间非常消耗硬盘空间的做法。 它更类似搜索引擎,而不是简单的where匹配. 关于索引的优化以上说的都是索引的好处.但是过多使用索引会导致滥用.因此索引也有它的缺点,虽然索引提高了表的查询速度,但是同时会降低表的更新速度. 因为在更新表时,mysql不但要保存数据,还要更新索引文件.一般这个问题不太严重.但是你如果在一张大表上建立了很多索引.索引文件会膨胀的很快. 下面是一些我总结和收藏的优化索引的方法: 使用前缀索引索引开始的部分字符，这样可以大大节省索引空间，从而提高索引效率。 当索引是很长的字符序列时,这个索引将会很占内存,而且会很慢,这时候就会用到前缀索引了.所谓的前缀索引就是以字段前面几个字符作为索引.这样可以大大节省索引空间,从而提高索引效率. 使用覆盖索引 覆盖索引（covering index）指一个查询语句的结果只用从索引中就能够取得，不必从数据表中读取.覆盖索引可以极大的提升性能 尽量避免同时使用多个范围条件 众所周知,mysql无法同时使用多个索引 尽量减少like语句的使用 如果非使用不可，如何使用也是一个问题。like “%aaa%” 不会使用索引而like “aaa%”可以使用索引。","link":"/posts/43dd42bd.html"},{"title":"如何基于gitee搭建一个属于自己的免费图床","text":"最近发现自己博客里的图片变成广告图片了.很苦恼.当初用的是一个国内的免费图床.当时用的还是很顺畅的. 众所周知,图床这东西盈利困难,所以他们为了赚钱,把我之前上传的一些图片都变成广告了. 为了避免这种事情再次发生所以我决定.搭一个属于自己的免费图床.是的没错,免费.不需要vps,不需要域名.只需要动动手就行了. 操作步骤:1.创建gitee账号略过 2.创建仓库 点击右上角 +号 .点击新建仓库 会来到如下页面: 各个选项 : 仓库名称: 随便填 路径: 自动生成的 介绍: 随便填 是否开源: 公开.选私有的话其他人就看不到你贴出去的图片了. 使用Readme初始化仓库: 勾选上,会自动帮你创建master分支. 选择分支模型: 单分支模式 点击创建仓库.如果来到下面的页面就算是创建成功了. 3.如何像使用普通图床一样方便的使用我希望能像使用普通图床一样方便的使用git作为图床.那么就需要用到PicGo 这个软件 官方介绍: PicGo: 一个用于快速上传图片并获取图片URL链接的工具 支持拖拽图片上传 支持快捷键上传剪贴板里第一张图片 Windows和macOS支持右键图片文件通过菜单上传 (v2.1.0+) 上传图片后自动复制链接到剪贴板 支持自定义复制到剪贴板的链接格式 下载地址: https://github.com/Molunerfinn/PicGo/releases 如何配置PicGo 安装 : 略 配置: 安装完点在菜单栏上找到图标,打开详情窗口以后界面是这样 : 2.因为默认图床配置不支持gitee,需要安装第三方插件,点那个插件设置,搜索gitee,安装gitee-uploader: 3.安装完以后重新启动PicGo.在图床设置里看到gitee,点击它来到gitee配置选项卡 各个选项: repo: 是仓库地址去掉https://gitee.com/剩下的内容 branch: maseter. token: 通过gitee右上角设置 =&gt; 安全设置 =&gt; 私人令牌生成. ​ 注意:得到这个令牌就能对你的账号为所欲为,所以不要轻易地泄露它 path:随便填 customPath:根据自己的喜好选择 点击确定 设为默认图床完成配置 使用回到主页面 =&gt; 上传区 链接格式可以自由选择 选择点击上传选择可爱的四毛.等待提示上传完成.就能在相册看到上传的图片了","link":"/posts/15f090bb.html"},{"title":"找到jvm中占用cpu最高的线程","text":"前言之前有一个微服务运行一段时间就开始卡住.服务器cpu占用达到400% +. 因为cpu爆满导致eureka无法发送心跳包.明明两个实例都还存活,但是注册中心显示却这个服务的状态是down. 一开始以为是个意外事件.重新部署一下就好.但是发现重启以后没过几个小时又开始卡住. 我就开始寻找最近提交的代码里是否有非常消耗CPU资源的逻辑.但是找了一段时间以后还是一无所获. 不过马上反应过来就是先把代码还原到上个版本.但是问题依旧.于是就开始排查问题. 排查过程 第一步 找到cpu占用最高的进程 123456#TOP命令 按P 根据CPU占用率排序 找到占用率最高的进程$ top PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 238012 root 20 0 895700 12344 6124 S 425.3 10.2 5:29.52 java 977472 root 20 0 143040 5628 4140 S 0.7 0.6 0:03.38 sshd 977543 root 20 0 148960 2948 1436 S 0.7 0.3 0:01.12 top 第二步 找到cpu占用最高的线程 12345678910111213141516171819#通过top -Hp pid 查看这个进程下的所有线程 #按P 根据CPU占用率排序 找到占用率最高的线程$ top -Hp 238012 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 240189 root 20 0 4076552 502708 10944 S 0.3 10.2 0:12.47 vert.x-eventloo 240027 root 20 0 4076552 502708 10944 S 400.2 10.2 0:22.09 nioEventLoopGro 239653 root 20 0 4076552 502708 10944 S 0.0 10.2 0:00.00 java 239655 root 20 0 4076552 502708 10944 S 0.0 10.2 0:17.50 java 239656 root 20 0 4076552 502708 10944 S 0.0 10.2 0:42.87 VM Thread 239657 root 20 0 4076552 502708 10944 S 0.0 10.2 0:00.02 Reference Handl 239658 root 20 0 4076552 502708 10944 S 0.0 10.2 0:00.00 Finalizer 239659 root 20 0 4076552 502708 10944 S 0.0 10.2 0:00.00 Signal Dispatch 239660 root 20 0 4076552 502708 10944 S 0.0 10.2 2:31.11 C2 CompilerThre 239661 root 20 0 4076552 502708 10944 S 0.0 10.2 0:29.14 C1 CompilerThre......#通过 printf '%x\\n' pid 将线程pid转换成对应的16进制pid$ printf '%x\\n' 2401893aa3d#可以看出占用cpu最高线程id是3aa3d,线程号是238012.接下来会用到 第三步 使用jstack 获取对应的线程信息 1234567891011#获取有问题的线程名称#238012是一开始获取到的进程号,3aa3d是这个进程下最耗费cpu的线程号#可以看出名叫\"nioLoopGroup-4-3\"的线程消耗了大量cpu$ jstack 238012 | grep 3aa3d\"nioLoopGroup-4-3\" prio=5 os_prio=0 tid=0x0000000003aa3d00 nid=0x8c38 #用jstack pid(进程pid)&gt;stack.dump 导出该最耗费cpu进程的堆栈轨迹信息$ jstack &gt;stack.dump#用 less stack.dump 命令打开dump$ less stack.dump#通过 /线程名 找到对应的堆栈轨迹信息/nioLoopGroup-4-3 一般来说,出现问题的代码大部分是我们写的业务代码.所以我们可以通过查看那些我们自己创建的类的相关信息去定位. 像我这次遇到的问题是因为死锁导致的.当然实际情况中还包括一些其它的原因,比如死循环,这些都需要具体去分析.","link":"/posts/6653a588.html"},{"title":"简单vpn服务在家连接公司内网设施","text":"最近疫情严重,公司开启了远程办公模式.为了在远程访问内网资源,自然是少不了VPN. 为什么会想要自己搭一个vpn呢,因为公司提供的vpn实在是太难用了.又慢又卡. 网上提供的vpn访问内网的方式都过于复杂.要组建子网,配置ipv4转发.一套折腾下来整个人都不好了.我只是想要在家里能够单方面的访问内网的所有资源而已,那么有没有简单一点的方法呢? 答案是有的.我稍微研究了一下.发现最简单的方法是在内网搭建vpn服务器.然后再通过frp把500,4500端口暴露出去.这样一来只要电脑连上vpn就相当于在内网啦.可以为所欲为的访问所有的内网资源. 简单记录一下搭建过程,留做备用. 需要的资源 : 公网服务器 * 1 内网服务器 * 1 依赖 : docker(内网服务) frp(公网服务和内网服务都需要) 注意: 因为是udp转发,所以像花生壳这样的http转发是不可以作为公网服务器的. ​##公网服务器配置 安装frp###1.如何下载? 12下载地址是: https://github.com/fatedier/frp/releasesfrp文件名称: frp_版本号_linux_amd64.tar.gz ###2.下载最新版本 12//举栗,下载的版本是当前的最新版本0.31.2wget https://github.com/fatedier/frp/releases/download/v0.31.2/frp_0.31.2_linux_amd64.tar.gz ###3.解压 1tar -zxvf frp_0.31.2_linux_amd64.tar.gz ###4.进入解压目录 123cd frp_0.31.2_linux_amd64//这里会发现有frpc、frpc.ini和frps、frps.ini 四个文件//其中frps frps.ini是服务端的相关文件.我们只需要关注这两个文件就好啦. ###5.编辑配置文件 123456789101112131415vi frps.ini//使用如下配置bind_addr = 0.0.0.0bind_port = 5443kcp_bind_port = 5443#控制台端口dashboard_port = 6443#控制台账号密码dashboard_user = 账号dashboard_pwd = 密码vhost_http_port = 18080vhost_https_port = 18081log_level = infolog_max_days = 3token = 密码 ###6.启动服务 1234// 启动服务 ./frps -c ./frps.ini//需要后台启动就在命令前面加上nohup,最后追加 &amp;nohup ./frps -c ./frp.ini &amp; ##内网服务器配置 安装dokcer 12#redhat&amp;centos:yum install -y docker 12# 启动docker服务 sudo systemctl start docker 安装ipsec-vpn-server 1234567891011docker run \\ --name ipsec-vpn-server\\ --restart=always \\ -e VPN_IPSEC_PSK=PSK密钥\\ -e VPN_USER=用户名 \\ -e VPN_PASSWORD=密码 \\ -p 500:500/udp \\ -p 4500:4500/udp \\ -v /lib/modules:/lib/modules:ro \\ -d --privileged \\ hwdsl2/ipsec-vpn-server 安装完以后docker ps -a 看看是否有名叫ipsec-vpn-server的服务.有的话说明vpn服务安装成功啦.是不是特别简单? 安装frp ###1-3和服务端配置没有差别.略过 ###4.进入解压目录 123cd frp_0.31.2_linux_amd64//这里会发现有frpc、frpc.ini和frps、frps.ini 四个文件//其中frpc frpc.ini是客户端的相关文件.我们只需要关注这两个文件就好啦. ###5.编辑配置文件 123456789101112131415161718vi frpc.ini//使用如下配置[common]server_addr = frp服务IPserver_port = 5443(frp服务端口)token = 密码(如果有的话) [vpn0]type = udplocal_ip =127.0.0.1local_port = 500remote_port = 500[vpn1]type = udplocal_ip = 127.0.0.1local_port = 4500remote_port = 4500 6.启动服务 1234// 启动服务 ./frpc -c ./frpc.ini//需要后台启动就在命令前面加上nohup,最后追加 &amp;nohup ./frpc -c ./frpc.ini &amp;","link":"/posts/f528012a.html"},{"title":"工作流引擎选型调研","text":"最近接到一个任务,要重构原来的工作流引擎. 目前工作流引擎有两大类型,顺序工作流和状态机工作流,我们公司使用的是基于顺序工作流的Flowable. 先介绍一下顺序工作流吧. 顺序工作流顺序工作流的特点是,它的执行过程是一个连续的序列,在完成一个步骤以后会自动执行下一个步骤. 举个例子,用顺序工作流模拟请假审批的工作: 第一步,提交申请 第二步,送交领导审批 第三步,审批结束 尽管顺序工作流也可以使用分支和循环,并且可以接收外部事件,但它的执行过程是可以预料的,并且总是向前执行直到完成为止. 对于业务逻辑相对固定的流程来说,使用顺序工作流没什么毛病.但是对于流程引擎可以随意改变的情况,顺序工作流的适配性就会差一些. 比如我司的合同审批流程,有各种在xx情况下可以跳过某个节点的逻辑,在更复杂的场景下,甚至节点的顺序都有可能根据业务场景发生变化. 这意味着对于每份合同来说,都会有不同的流程.使用顺序工作流需要在节点之间来回跳转.导致业务逻辑变得十分复杂. 既然顺序工作流不适合流程不固定的场景,那么有没有更合适的解决方案呢? 有的,那就是引入基于事件驱动的状态机工作流 状态机工作流状态机工作流中定义了一系列的状态. 正常情况下,状态机总是停留在预设的状态中,直到事件触发以后才会跳转到新的状态,直到完成. 状态机工作流的特点 可以定义状态 可以通过事件来描述工作流如何从一个状态跳转到另一个状态 这么说有点抽象,举个订单流转的例子来说明一下状态机工作流是如何构造订单状态变化的. 流程图: 创建订单 =&gt; 状态 : 初始化 发起支付 = &gt; 状态 : 等待支付 根据订单金额 : 金额 = 0 跳过支付 =&gt; 状态 : 待收货 = &gt; 确认收货 =&gt; 完成 金额 大于0 =&gt; 支付成功 =&gt; 状态:待发货 =&gt; 发货 =&gt; 状态 : 待收货 = &gt; 确认收货 =&gt; 完成 =&gt; 支付失败 =&gt; 订单取消 用伪代码来描述订单工作流是如何控制订单状态流转的 定义订单状态和订单状态改变事件 12345// 订单状态枚举OrderStatus {//初始化,待支付,待发货,待收货,订单取消,交易完成 INIT,WAIT_PAY,WAIT_DELIVER,WAIT_RECEIVED,CANCEL,SUCCESS} 123456789101112131415//订单状态改变事件枚举OrderStatusChangeEvent { //发起支付事件 //支付成功事件 //发货事件 //跳过支付事件 //确认收货事件 //支付失败事件 INIT_PAY, PAID, DELIVERT, SKIP_PAY, RECEIVED, UN_PAID} 定义订单状态改变事件触发监听器 1234567891011121314151617181920212223242526272829303132OrderStatusListener{ @Event(\"INIT =&gt; WAIT_PAY : INIT_PAY\") initEvent(){ // 初始化订单业务逻辑 } @Event(\"WAIT_PAY =&gt; WAIT_DELIVER : PAID\") paidEvent(){ // 支付成功业务逻辑 } @Event(\"WAIT_DELIVER =&gt; WAIT_RECEIVE : DELIVERT\") delivertEvent(){ // 发货业务逻辑 } @Event(\"WAIT_PAY =&gt; WAIT_RECEIVE : SKIP_PAY\") skipPayEvent(){ // 跳过支付业务逻辑 } @Event(\"WAIT_RECEIVE =&gt; SUCCESS : RECEIVED\") receivedEvent(){ //确认收货业务逻辑 } @Event(\"WAIT_PAY =&gt; CANCEL : UN_PAID\") unpaidEvent(){ // 支付失败业务逻辑 }} 调用 12345678910111213141516171819OrderService { val stateMachine; //创建订单 fun createOrder(){ //触发事件 Order order = statemachine.sendEvent(\"INIT_PAY\"); return order.tradeNo; } //支付 fun pay(tradeNo){ //触发事件 Order order = statemachine.sendEvent(tradeNo,\"PAID\"); return order; } ....} 从上面的例子可以看出,状态工作流引擎的优点: 所有业务逻辑都被封装成事件,通过触发事件调用业务逻辑 增加新的业务逻辑 也只需要新增事件,拓展性佳 结构层次清晰,适合流程节点的状态相对不固定情况 缺点: 对于比较通用的,相对固定的流程(比如 请假,离职)来说.没有现成的流程模板.需要自己开发 流程引擎和业务逻辑强耦合. 状态机工作流引擎 Spring Statemachine spring statemachine是使用 Spring框架下的状态机概念创建的一种应用程序开发框架。它使得状态机结构层次化,简化了配置状态机的过程. 官方文档: https://docs.spring.io/spring-statemachine/docs/2.0.0.RELEASE/reference/htmlsingle/ 中文教程: https://github.com/cjqCN/spring-statemachine-learning","link":"/posts/6c75d1d2.html"},{"title":"排查线上redis连接超时问题","text":"表现 在访问redis时,偶尔会出现超时的情况,频率不高.timeout默认10s,连接池用的lettuce 出现超时的时机地点随机,浏览器请求和客户端请求都有可能出现超时 错误信息1io.lettuce.core.RedisCommandTimeoutException: Command timed out after 10 second(s) 初步判断可能是因为redis执行了时间复杂度为O(n)的命令导致阻塞. 排查过程 慢查询导致阻塞 通过redis-cli查询slowlog 1redis-cli: slowlog get 20 返回的执行时间最长的命令是keys,阻塞了0.5秒.可以排除因为redis命令导致阻塞原因. 网络链路问题 ssh到微服务部署节点测试网络状态(内网地址) 看起来好像没啥问题. 发了工单,阿里回复rds网络链路通畅.顺便还建议我们用scan代替keys (捂脸) 客户端问题 后来我发现,有好多人遇到了跟我一样的问题.不管是把超时时间设置的更长,还是设置redis的超时时间,都没用. 最后选择的解决方案是把驱动换成jedis,换了驱动以后问题就不再出现了.","link":"/posts/a8d5a9fb.html"},{"title":"用spring shell 写一个spring 应用监控程序","text":"Spring shell介绍不是所有应用都需要一个漂亮的用户界面,有时候很多场景也需要使用终端进行交互. 我们可以通过引入Spring Shell jar并添加自定义命令(作为spring bean上的方法)轻松构建一个功能齐全的命令行应用程序. Spring shell特性Spring Shell的功能包括： 一个简单的，注释驱动的编程模型，用于提供自定义命令 使用Spring Boot自动配置功能作为命令插件策略的基础 选项卡完成，着色和脚本执行 自定义命令提示符，shell历史文件名，结果和错误的处理 基于域特定标准动态启用命令 与bean验证API集成 已经内置命令，如清晰的屏幕，华丽的帮助，退出 ASCII艺术表，带格式，对齐，花式边框等 快速完成一个Spring boot应用监控程序现在有一个需求是 需要一个命令行应用程序,用来监控服务器上spring boot应用的状态 和 启动路径 通过bash脚本实现这个功能十分繁琐,但是通过Spring shell却可以轻而易举的做到. 完整的工程会附在文章最后 1.引入依赖首先,通过spring initializr初始化spring boot应用,并且引入Spring Shell依赖: 1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.springframework.shell&lt;/groupId&gt; &lt;artifactId&gt;spring-shell-starter&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.jetbrains.kotlin&lt;/groupId&gt; &lt;artifactId&gt;kotlin-reflect&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.jetbrains.kotlin&lt;/groupId&gt; &lt;artifactId&gt;kotlin-stdlib-jdk8&lt;/artifactId&gt;&lt;/dependency&gt; 因为是用kotlin写的,所以还引用了kotlin的依赖.如果不是kotlin工程可以无视kotlin-reflect和kotlin-stdlib-jdk8 2.启动应用启动以后控制台是这样的: 相对于Spring web应用而言 Spring Shell多了命令提示符shell:&gt;.它的作用是用来接收用户输入的指令 Spring Shell 自带一些指令.比如help 显示可用命令,clear清屏等等. 3.编写自定义指令通过@ShellComponent注解可以创建自定义命令实现类 通过@ShellMethod注解可以实现自定义命令,方法参数即为命令参数 现在我们需要两个命令: ps 用于显示当前服务器上运行的spring boot应用 kill 用于强制中止spring boot应用 代码实现如下: 1234567891011121314151617181920212223242526272829303132333435363738394041@ShellComponent(\"local\")class Local { @ShellMethod(key = [\"ps\"], value = \"显示运行中的 spring boot\") fun ps() { val data: Array&lt;Array&lt;String&gt;&gt; = localJmxConn().mapNotNull { it.use { conn -&gt; val runtimeMXBean = conn.rmxb() when { isSpringBootApplication(runtimeMXBean) -&gt; { val memoryMXBean = conn.mmxb() val heapMemoryUsage = memoryMXBean.heapMemoryUsage val heapUsage = \"${heapMemoryUsage.used / (1000 * 1000)}MB/${heapMemoryUsage.max / (1000 * 1000)}MB\" val pid = runtimeMXBean.systemProperties[\"PID\"] ?: \"\" val javaVersion = runtimeMXBean.systemProperties[\"java.version\"] ?: \"\" val running = runtimeMXBean.systemProperties[\"sun.java.command\"]?. substring(runtimeMXBean.systemProperties[\"sun.java.command\"]?.lastIndexOf(\".\")!! + 1) ?: \"\" val start = processUserFriendlyTime(System.currentTimeMillis() - runtimeMXBean.startTime) val ports = getPorts(pid) /** * \"PID\",\"MAIN\",\"RUNNING\",\"HEAP\",\"PORT\",\"PATH\",\"JAVA_VERSION\" */ return@mapNotNull arrayOf(pid, running, start, heapUsage, ports, StringUtils.abbreviate(runtimeMXBean.classPath, 50), javaVersion) } else -&gt; return@mapNotNull null } } }.toTypedArray() printPsTable(data) } @ShellMethod(key = [\"kill\"], value = \"kill spring boot app\") fun kill(pid: String) { \"kill -9 $pid\".command() println(\"kill pid $pid success\") }} 尝试运行: 成功.ｂ（￣▽￣）ｄ 4.源码地址https://gitee.com/minagamiyuki/application-manager","link":"/posts/acd577xy.html"},{"title":"警惕java数组协变特性导致的问题","text":"为什么会想到这个问题呢? 最近在垠神博客上看到的这道java高级面试题: 要知道这个问题的答案,首先需要了解java的协变特性. 关于java的协变特性.我在网上找到了一些资料,再加上自己的理解.写了这篇文章. 什么是协变和逆变?先看例子: 1Object[] objects = new String[2]; 我们都知道String[]是Object的子类,String类型的对象可以赋给Object类型. 也就是,可以把子类型的变量赋给赋给父类型.这就是协变. 而逆变和协变相反,可以把父类型的对象赋给子类型. 用数学函数的定义,可以这么描述: 当A ≦ B时,如果有f(A) ≦ f(B),那么f叫做协变； 当A ≦ B时,如果有f(B) ≦ f(A),那么f叫做逆变； 在实际编程中,很少使用到逆变.这是因为因为使用逆变的类在运行时很难追踪到精确的类型,不直观.从而导致出现比较复杂的类型规则. 里氏替换原则在说下面的内容之前,先说一下这个大家都知道的规则 : 如果A是B的子类型，那么A类型的表达式可用于任何出现B类型表达式的地方 假设有类Fruit和Apple,Apple ≦ Fruit，Fruit类有一个方法fun1()，返回一个Object对象: 1234567public Object fun1() { return null;}Fruit f = new Fruit();//...//某地方用到了f对象Object obj = f.fun1(); 那么现在Apple对象覆盖fun1()，假设可以返回一个String对象： 12345678@Overridepublic String fun1() { return \"\";}Fruit f = new Apple();//...//某地方用到了f对象Object obj = f.fun1(); 那么任何使用Fruit对象的地方都能替换成Apple对象吗？显然是可以的。 举得例子是返回值，如果是方法参数呢？调用父类方法fun2(String)的地方肯定可以被一个能够接受更宽类型的方法替代：fun2(Object)…… 需要注意,里氏替换并不会改变方法的原有类型. 回到一开始的问题王垠举的例子到底哪里错了?我们运行一下例子里的代码. 123456public static void f() { String[] a = new String[2]; Object[] b = a; a[0] = \"hi\"; b[1] = Integer.valueOf(42);} 这段代码编译期不会有问题.而运行阶段会在 1b[1] = Integer.valueOf(42); 这一句话引发运行时异常. 从实现的角度来看,java的数组是协变的.所以可以把子类数组String[]赋给父类Object[].而这种类型赋值并不会改变数组的原有类型. 也就是在上面的例子中,尽管b表面上是Object[],但是运行时它的实际类型是String[].内部元素也只能添加Stiring类型. 于是就引发了这种现象：编译期编译通过（因为表面的类型检查认为没有什么问题），运行时抛出ArrayStoreException（把事实上的Integer对象存进了String数组）. 而原题说的”错了”,是本质上的,我觉得他的意思是”java数组支持协变”这件事情,本身就是错误的 . 而&lt;&lt; Effective Java &gt;&gt;里则直言不讳地指出,java数组支持协变是一种设计缺陷: 从本质上讲Object[] b = a;这段代码就不应该通过编译.你不能把一个Integer类型放到String数组里.相比运行时抛出异常,我宁愿在编译时就找出错误. 为什么会出现这种问题 ? 因为泛型是java1.5以后加入的,早期的java没有泛型.但是有数组.所以只能通过妥协的方式,也就是通过数组协变来实现一部分泛型的功能. 如何才能让错误暴露的更明显我认为.最好的办法就是从根源上解决.在编译期间让问题更明显的暴露出来,不要使代码进入运行时,就可以了. 但是,java底层实现和api里已经有大量的Object[]了,所以出于兼容性考虑,简单粗暴的禁止数组协变是行不通的. 那么只能退而求其次,保证交付的代码里不要使用数组的协变特性. 现代IDE已经帮我们做好编译期检查了.虽然不是强制约束,但是在编译期间还是能够通过警告发现问题的. 通过编译期检查的方式,使得问题能够更直接的暴露出来.这就是我认为的正解.","link":"/posts/b68884c2.html"},{"title":"Coding Page服务跪啦","text":"这两天 Coding Page服务突然跪啦.没有任何征兆,连带着我的博客也一起被干掉了. 不得不说,国内的静态网页托管服务是真的不太行. 还好github-page上面有备份.改一下dns解析就恢复了.","link":"/posts/0d23dk31.html"},{"title":"用一个简单例子解释什么是Oauth2","text":"Oauth2的定义OAuth 2.0 是目前最流行的授权机制，用来授权第三方应用，获取用户数据.第三方无需知道用户的账号及密码，就可获取到用户的授权信息 1.Oauth2解决了什么问题为了更好地理解Oauth2的使用场景,我举一个微信的例子 有一个网站,提供微信登录服务.可以使用微信登录这个网站. 网站必须得到用户的授权,才能登录这个网站.那么,网站如何得到用户的授权呢? 传统的方法是,用户把微信账号密码告诉网站,这样网站就能通过验证账号密码的方式获得授权. 但是,这样做有以下几个严重缺陷: 网站会得知用户的微信账号密码,会保存用户的账号密码.这样做不安全 网站能够得知用户在微信的所有资料,聊天记录.用户无法限制网站获得授权的有效期和范围 用户只有修改微信密码,才能收回对网站的授权,但是这样做,会使得其他所有获得用户授权的第三方应用失效 只要有一个第三方程序被破解,就会导致用户密码泄漏.导致所有获得用户授权的第三方应用被破解 Oauth2就是为了解决上面的问题诞生的 2.名词解释在解释Oauth2的工作流程之前,需要了解几个名词 Third-party application：第三方应用程序，或者”客户端“(client,application) HTTP service：HTTP服务提供商，本文中的”微信“ Authorization server：认证服务器，即服务提供商专门用来处理认证的服务器 Resource server：资源服务器，即服务提供商存放用户生成的资源的服务器。它与认证服务器，可以是同一台服务器，也可以是不同的服务器 Resource Owner：资源所有者，或者用户(user) 知道了这些,就不难理解,Oauth2的作用就是,让客户端能够安全可控的获得授权,和”服务提供商“进行互动 3.Oauth2的工作流程 (来自RFC6479的工作流程图) 以微信授权登录为例子: 用户(user)访问第三方应用(client),第三方应用需要用户登录验证.用户选择微信授权登录 第三方应用(client)向微信(HTTP Service)发起登录授权请求 微信服务器拉起用户授权界面 用户同意授权 微信验证服务器(Authorization server)确认无误以后,向第三方应用发放授权令牌(accessToken) 第三方应用通过 accessToken 向微信用户服务器(Resource server)获取用户信息 获取用户信息成功,对用户信息进行处理 至此,Oauth2的授权登录过程结束 4.基于Spring Cloud实现Oauth2认证授权待填坑..","link":"/posts/4d23k891.html"},{"title":"lombok指北","text":"关于lombok的原理简单来说,lombok的原理是,在编译期间通过通过扫描带有lombok注解的类.然后通过编译生成类对应的代码. 类似的工具还有cglib ,可以通过Enhancer增强现有的类.不过cglib使用的是反射的机制.所以性能相对来说较差.而lombok是在编译期生成代码,性能和原生.java代码没有差别. 通过ide插件支持,lombok生成的代码在源码阶段对使用者也是可用的 通过使用 @Data注解可以为pojo类生成getter() setter()方法.从而消除冗长的代码.虽然通过ide的代码自动生成功能也能做到这点,但是我认为使用Lombok可以使代码更简洁,阅读起来更清晰 lombok的弱点我认为 过于强大 是lombok的弱点(但是算不上缺点). 使用lombok会使得编译以后的字节码和.java的表现不一样.所以在使用lombok之前.我们需要了解它做了什么,避免出现意料之外的结果. lombok功能介绍val使用val定义final型局部变量.而不需要写入实际的类型 1234//String str = \"\";val str = \"\";//HashMap&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;();val map = new HashMap&lt;String,Object&gt;(); var使用var定义可变局部变量.使用方法和val相同 @NonNull@NonNull 注解可以在方法参数/类属性上使用. 如果放在方法参数上，lombok将在方法/构造函数主体的开头插入一个null检查，并在参数为null时抛出一个携带参数名称的NullPointerException 如果放在类属性上，则任何使用lombok生成的代码对该属性赋值时,也将生成对应的null检查 12345public Optional&lt;SettAcct&gt; getSettAccountBy(@NonNull Account accountNumber) { //Assert.notNull(accountNumber,\"accountNumber is required non-null but is null\"); //lombok在编译期间 生成了上面的代码 return handle(accountNumber);} @Cleanup使用@Cleanup 注解局部变量可以确保无论发生什么,最后都会通过调用其close()方法将其关闭 它的实现原理是,在编译时将被注解的变量声明后面的所有语句包装到将作用域结束到try-finally块中，最后在finally块中调用close()方法关闭资源 这样使用: 12345678910public void copyFile(String in, String out) throws IOException { @Cleanup FileInputStream inStream = new FileInputStream(in); @Cleanup FileOutputStream outStream = new FileOutputStream(out); byte[] b = new byte[65536]; while (true) { int r = inStream.read(b); if (r == -1) break; outStream.write(b, 0, r); }} 实际运行的代码是这样的: 123456789101112131415161718public void copyFile(String in, String out) throws IOException { @Cleanup FileInputStream inStream = new FileInputStream(in); try { @Cleanup FileOutputStream outStream = new FileOutputStream(out); try { byte[] b = new byte[65536]; while (true) { int r = inStream.read(b); if (r == -1) break; outStream.write(b, 0, r); } } finally { if (outStream != null) outStream.close(); } } finally { if (inStream != null) inStream.close(); }} @Getter/@Setter通过在类属性上使用@Getter/@Setter方法,可以使lombok为类生成标准的getter()/setter()方法. 比如: 12@Getter private int foo; 会在编译期自动为类生成下面的方法: 123public int getFoo() { return this.foo;} 这两个注解也可以应用到类上，在这种情况下，lombok会为这个类的所有非静态字段生成getter()/setter()方法 @ToString可以在类上使用 @ToString注解，lombok会为其生成toString()方法的实现;默认情况下,它将按顺序打印类名和所有非静态字段，并用逗号分隔. 通过@ToString(callSuper = true),可以将toString的超类实现的输出包含到输出中. 需要注意的是如果被注解的类没有父类,会打印Object.toString()的输出,而这个默认几乎是没有意义的.所以除非被注解的类确实有一个父类,否则不要将callSuper 设置为true. 通过使用@ToString(exclude = &quot;field&quot;)可以在输出时跳过某些字段. 或者可以使用@ToString(onlyExplicitlyIncluded=true)精确指定要使用的字段，然后通过@Include注解标记要包含的所有字段 以上其实都是lombok源码和官方文档里面的东西.我做的只是把它翻译成中文而已.","link":"/posts/0b44qx09.html"},{"title":"lombok指北","text":"","link":"/posts/dd12w31z.html"},{"title":"用antlr写一个编译器","text":"","link":"/posts/dd12w31z.html"},{"title":"aop技术介绍,例子以及原理","text":"AOP的基本概念引用&lt;&lt;维基百科&gt;&gt;上对AOP的定义: 面向切面的程序设计（Aspect-oriented programming，AOP，又译作面向方面的程序设计、剖面导向程序设计）是一种程序设计范型.它的作用是将横切关注点与业务主体进行进一步分离，以提高程序代码的模块化程度。 通过在现有代码基础上增加额外的通知（Advice）机制，能够对被声明为“切点（Pointcut）”的代码块进行统一管理与装饰，如“对所有方法名以‘set*’开头的方法添加后台日志”。该思想使得开发人员能够将与代码核心业务逻辑关系不那么密切的功能（如日志功能）添加至程序中，同时又不降低业务代码的可读性。面向切面的程序设计思想也是面向切面软件开发的基础. 从维基百科对AOP的定义可以看出,使用AOP的目的: 提高可读性和可维护性,减少代码的侵入性. 也就是说,在这种场景下,并非必须使用AOP,其他方式完全可以实现同样的效果.只是使用AOP实现同样的功能会更加简洁. 术语解释 : 通知（Advice） 就是你想要的功能，也就是上面说的 安全，事物，日志等。你给先定义好，然后在想用的地方使用它. 连接点（JoinPoint） 这个更好解释了，就是spring允许你使用通知的地方，那可真就多了，基本每个方法的前，后（两者都有也行），或抛出异常时都可以是连接点，spring只支持方法连接点.其他如aspectJ还可以让你在构造器或属性注入时都行，不过那不是咱关注的，只要记住，和方法有关的前后 ,或者两者，都是连接点. 切入点（Pointcut） 上面说的连接点的基础上，来定义切入点，你的一个类里，有15个方法，那就有几十个连接点，但是你并不想在所有方法附近都使用通知（使用叫织入，以后再说），你只想让其中的几个，在调用这几个方法之前，之后或者抛出异常时干点什么，那么就用切点来定义这几个方法，让切点来筛选连接点，选中那几个你想要的方法。比如可以定义一个get*开头的方法的切点. 切面（Aspect) 切面是通知和切入点的结合.现在发现了吧，没连接点什么事情，连接点就是为了让你好理解切点，搞出来的，明白这个概念就行了。通知说明了干什么和什么时候干（什么时候通过方法名中的before,after，around等就能知道），而切入点说明了在哪干（指定到底是哪些方法），这就是一个完整的切面定义. 引入（introduction） 允许我们向现有的类添加新方法属性。这不就是把切面（也就是新方法属性：通知定义的）用到目标类中吗 目标（target） 引入中所提到的目标类，也就是要被通知的对象，也就是真正的业务逻辑，它可以在毫不知情的情况下，被咱们织入切面。而自己专注于业务本身的逻辑. 织入(weaving) 把切面应用到目标对象来创建新的代理对象的过程. 实现AOP的两种方式一是采用动态代理技术，利用截取消息的方式，对该消息进行装饰，以取代原有对象行为的执行；二是采用静态织入的方式，引入特定的语法创建“方面”，从而使得编译器可以在编译期间织入有关“方面”的代码. Spring中实现AOP的方式Spring AOP使用的是动态代理,有以下实现方式 : JDK动态代理 使用业务类实现的接口创建代理类.如果没有实现接口则无法使用JDK动态代理. CGLIB动态代理 cglib是针对类来实现代理的，原理是对指定的业务类生成一个子类，并重写其中业务方法实现代理。因为采用的是继承，所以不能对final修饰的类进行代理。 Spring AOP 选择代理方式的策略 如果目标对象有接口,则使用JDK的动态代理. 如果目标对象没有接口则默认使用cgLib动态代理. 支持通过配置自己选择代理方式 一个例子通过AOP技术实现接口日志功能. 在Controller类或者方法上添加注解就能给该类下的所有接口 或者单独为某个接口添加接口日志功能. 首先是定义切入点.我们希望能在所有使用@RestApiLog注解的类或者方法上新增一个通知(接口日志). 注解 : 12345@Target({ElementType.METHOD, ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)public @interface RestApiLog {} 切面 : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176/** * @author laizhehao */@Generated@Aspect@Component@Slf4jpublic class LoggerAspect { @Around(\"@within(com.laizeh.util.common.RestApiLog)\")//这里定义了切入点.也就是所有带@RestApiLog 注解的目标 public Object process(ProceedingJoinPoint joinPoint) throws Throwable {//每当代码执行到我们定义的切入点的时候,就会调用这个方法.同时把关注点方法作为参数传入,方便我们进行操作 Object[] args = joinPoint.getArgs();//获取方法参数 RestApiLog logEntity = new RestApiLog(); Object result = null; //起始时间 long startMs = System.currentTimeMillis(); try { //请求 Optional.ofNullable(RequestContextHolder.getRequestAttributes()) .map(k -&gt; (ServletRequestAttributes) k) .ifPresent(k -&gt; { HttpServletRequest request = k.getRequest(); logEntity.setRequestMethod(request.getMethod()); logEntity.setContentType((StringUtils.isEmpty(request.getContentType())) ? \"empty\" : request.getContentType()); logEntity.setRequestURI(StringUtils.abbreviate(request.getRequestURI(), 255)); logEntity.setRequestURL(StringUtils.abbreviate(request.getRequestURL().toString(), 255)); logEntity.setUserAgent(Optional.ofNullable(request.getHeader(\"user-agent\")).orElse(\"\")); logEntity.setIp(request.getRemoteAddr()); logEntity.setHeaders(getHeadersInfo(request)); logEntity.setRealIp(getIpAddress(request)); }); //类名 String className = joinPoint.getTarget().getClass().getName(); logEntity.setClassName(className); //请求方法 String method = joinPoint.getSignature().getName() + \"()\"; logEntity.setMethod(method); //参数 String methodParam = wrapArgs(joinPoint); logEntity.setParams(methodParam); //调用原本的方法并且获取调用结果 result = joinPoint.proceed(args); logEntity.setWithThrows(false); } catch (Throwable throwable) { logEntity.setWithThrows(true); logEntity.setThrowable(throwable); throw throwable; } finally { generateExecuteMs(logEntity, startMs); log.info( \"\\r\\n\" + \"\\r\\n\" + \" Request URL : \" + logEntity.getRequestURL() + \"\\r\\n\" + \" Http Method : \" + logEntity.getRequestMethod() + \"\\r\\n\" + \" Request URI : \" + logEntity.getRequestURI() + \"\\r\\n\" + \" Request Params : \" + logEntity.getParams() + \"\\r\\n\" + \" Http Headers : \" + logEntity.getHeaders() + \"\\r\\n\" + \" Content-Type : \" + logEntity.getContentType() + \"\\r\\n\" + \" Class name : \" + logEntity.getClassName() + \"\\r\\n\" + \" Method Name : \" + logEntity.getMethod() + \"\\r\\n\" + \" Request IP : \" + logEntity.getIp() + \"\\r\\n\" + \" Real IP : \" + logEntity.getRealIp() + \"\\r\\n\" + \" User Agent : \" + logEntity.getUserAgent() + \"\\r\\n\" + \" Execution Time : \" + logEntity.getExecuteMs() + \"ms\" + \"\\r\\n\" + \" WithThrows : \" + wrapThrowMessage(logEntity) + \"\\r\\n\" + \" Result : \" + wrapResult(result) + \"\\r\\n\" + \"\\r\\n\" ); } return result; } /** * 生成入参string * * @param joinPoint * @return */ private String wrapArgs(ProceedingJoinPoint joinPoint) { String[] parameterNames = ((MethodSignature) joinPoint.getSignature()).getParameterNames(); Object[] args = joinPoint.getArgs(); String message = IntStream.range(0, args.length).filter(i -&gt; args[i] != null).mapToObj(i -&gt; parameterNames[i] + \"=\" + StringUtils.abbreviate(args[i].toString(), 800)).collect(Collectors.joining(\",\")); return \"{\" + message + \"}\"; } private String wrapResult(Object result) { return result == null ? \"&lt;&gt;\" : StringUtils.abbreviate(result.toString(), 1500); } /** * 获取用户真实ip * * @param request * @return */ private static String getIpAddress(HttpServletRequest request) { String ip = request.getHeader(\"x-forwarded-for\"); if (ip == null || ip.length() == 0 || \"unknown\".equalsIgnoreCase(ip)) { ip = request.getHeader(\"Proxy-Client-IP\"); } if (ip == null || ip.length() == 0 || \"unknown\".equalsIgnoreCase(ip)) { ip = request.getHeader(\"WL-Proxy-Client-IP\"); } if (ip == null || ip.length() == 0 || \"unknown\".equalsIgnoreCase(ip)) { ip = request.getHeader(\"HTTP_CLIENT_IP\"); } if (ip == null || ip.length() == 0 || \"unknown\".equalsIgnoreCase(ip)) { ip = request.getHeader(\"HTTP_X_FORWARDED_FOR\"); } if (ip == null || ip.length() == 0 || \"unknown\".equalsIgnoreCase(ip)) { ip = request.getRemoteAddr(); if (\"127.0.0.1\".equals(ip) || \"0:0:0:0:0:0:0:1\".equals(ip)) { //根据网卡取本机配置的IP InetAddress inet = null; try { inet = InetAddress.getLocalHost(); } catch (UnknownHostException e) { log.error(\"\"); } ip = inet == null ? null : inet.getHostAddress(); } } return ip; } /** * 获取异常信息 * * @param logEntity * @return */ private String wrapThrowMessage(RestApiLog logEntity) { return (logEntity.isWithThrows() &amp;&amp; logEntity.getThrowable() != null) ? (logEntity.getThrowable().getClass().getName() + \"[\" + logEntity.getThrowable().getMessage() + \"]\") : \"false\"; } /** * 获取执行时间 * * @param logEntity * @param startMs */ private void generateExecuteMs(RestApiLog logEntity, long startMs) { //结束时间 long endMs = System.currentTimeMillis(); //执行时间 long executeMs = endMs - startMs; logEntity.setExecuteMs(executeMs); } /** * 获取头部信息 &amp;&amp; 脱敏 * * @param request * @return */ @SuppressWarnings(\"rawtypes\") private Map&lt;String, String&gt; getHeadersInfo(HttpServletRequest request) { Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); Enumeration headerNames = request.getHeaderNames(); while (headerNames.hasMoreElements()) { String key = (String) headerNames.nextElement(); String value = request.getHeader(key); map.put(key, isSecret(key) ? \"*******************\" : value); } return map; } private boolean isSecret(String key) { return key == null || key.contains(\"auth\") || key.contains(\"refresh\") || key.contains(\"token\"); }}","link":"/posts/e8a7a218.html"},{"title":"使用Jenkins Pipeline 进行持续交付","text":"持续交付持续交付是一种DevOps 的实践. 持续交付系统可以自动把代码的更新发布到测试/开发环境. 在持续交付时,开发人员可以自动执行预设的单元测试,负载测试,api可靠测试等.这有助于开发人员更全面地验证更新并抢先发现其中的问题. 具体来说,持续交付就是把部署,单元测试的过程自动化.通过触发器或者定时器运行我们编写的脚本自动构建代码. Jenkins Pipeline核心概念Pipeline(流水线语法)就是一套运行于Jenkins上的工作流框架，将原本独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排与可视化 Pipeline是Jenkins2.X的最核心的特性,可以用编写脚本的形式来描述构建过程 规范 Pipeline使用Groovy语法构建 Pipeline顶层必须是一个 block, 比如: pipeline { } Pipeline里里没有;作为语句分隔符,每条语句都必须在自己的行上 Pipeline的块 {} 只能由 节段, 指令, 步骤, 或赋值语句组成.属性引用语句被视为无参方法调用. 例如, input被视为 input() 下面通过举例的方式介绍Jenkins Pipeline的使用方法.例子中隐去了一部分关键信息. 一个简单的例子 stage 阶段，一个Pipeline可以划分成若干个Stage，每个Stage代表一组操作，例如：“构建”，“测试”，“部署” steps 步骤，Step是最基本的操作单元，小到创建一个目录，大到构建一个Docker镜像，可以由各类Jenklins 插件提供功能，例如：sh ‘make’ agent 指定用来执行pipeline的jenkins服务器.流水线中的每个阶段都必须在某个地方执行,agent 部分即指定具体在哪里执行.必须定义在pipeline块的顶部 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647pipeline { agent any stages { stage(\"检出\") { steps { checkout([ $class: 'GitSCM', branches: [[name: env.GIT_BUILD_REF]], userRemoteConfigs: [[ url: env.GIT_REPO_URL, credentialsId: env.CREDENTIALS_ID ]] ]) } } stage(\"构建\") { steps { echo \"构建中...\" sh 'docker version' // 请在这里放置您项目代码的单元测试调用过程，例如: // sh 'mvn package' // mvn 示例 // sh 'make' // make 示例 } } stage(\"测试\") { steps { echo \"单元测试中...\" // 请在这里放置您项目代码的单元测试调用过程，例如: // sh 'mvn test' // mvn 示例 // sh 'make test' // make 示例 echo \"单元测试完成.\" } } stage(\"部署\") { steps { echo \"部署中...\" // 请在这里放置收集单元测试报告的调用过程，例如: // sh 'mvn tomcat7:deploy' // Maven tomcat7 插件示例： // sh './deploy.sh' // 自研部署脚本 echo \"部署完成\" } } }} 这个例子里定义了4个stage. jenkins会按照”检出”,”构建”,”测试”,”部署”的顺序依次执行.如果任意一个stage执行失败.流水线会中断 构建静态博客网站使用命令行和插件执行构建过程 首先检出代码.然后通过命令行执行构建,最后通过ssh-agent插件部署到github-page仓库 123456789101112131415161718192021222324252627282930313233343536pipeline { agent any stages { stage('检出') { steps { checkout([$class: 'GitSCM', branches: [[name: env.GIT_BUILD_REF]], userRemoteConfigs: [[url: env.GIT_REPO_URL, credentialsId: env.CREDENTIALS_ID]]]) } } stage('构建') { steps { echo '构建中...' sh 'npm config set registry https://registry.npm.taobao.org --global &amp;&amp; npm config set disturl https://npm.taobao.org/dist --global' sh ' npm install -g hexo' sh 'npm install -g gulp' sh 'npm install' echo '构建完成.' } } stage('部署') { steps { echo '清除缓存...' sh 'hexo clean' echo '生成网站...' sh 'hexo g' echo '压缩网站加快访问速度...' sh 'gulp' sshagent([env.SSH_AGENT_ID]) { echo '部署中...' sh 'hexo d' echo '部署完成' } } } }} 使用SCP部署Java Web项目 tools 通过tools可自动安装工具，并放置环境变量到PATH 注意:工具名称必须在Jenkins 管理Jenkins → 全局工具配置中预配置 占位符 除了单引号和三引号字符串以外,在所有的字符串字面量中都可以插入任何Groovy表达式.插值是用其值来替换字符串中占位符的行为.占位符表达式是由${}包围的或者以$为前缀的点表达式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263pipeline { agent { label params.runEnv } tools { maven 'mvn-3.3.9' jdk 'jdk-1.8' } stages { stage('Checkout') { steps { checkout([$class: 'GitSCM', branches: [[name: env.GIT_BUILD_REF]], userRemoteConfigs: [[url: env.GIT_REPO_URL, credentialsId: env.CREDENTIALS_ID]]]) } } stage('SonarAnalysis') { steps{ script { if (params.needAnaly) { sh 'mvn clean org.jacoco:jacoco-maven-plugin:prepare-agent install -Dmaven.test.failure.ignore=true -P${environment} -Dsonar.branch=${environment}' sh 'mvn sonar:sonar' } } } } stage('Build') { steps { script { if(params.packageWay == '重新打包'){ if (params.mvnForceUpdate) { sh \"mvn clean package -P${environment} -Dmaven.test.skip=true -U\" } else { sh \"mvn clean package -P${environment} -Dmaven.test.skip=true\" } } if(params.packageWay == '上传nexus'){ if (params.mvnForceUpdate) { sh \"mvn clean deploy -P${environment} -Dmaven.test.skip=true -U\" } else{ sh \"mvn clean deploy -P${environment} -Dmaven.test.skip=true\" } } } } } stage('Run') { steps { script { def ips = ['${serverAddress}'] if (params.serverAddress != '无') { for (def ip : ips) { sh \"ssh root@${ip} '/root/projects/payReboot.sh stop ${params.projectName}'\" if (params.packageWay != '直接重启') { sh \"ssh root@${ip} 'mkdir -p /root/projects/\" + params.projectName + \"; cd /root/projects/\" + params.projectName + \"; rm -rf ./*'\" def cmdCopy = \"scp ./service/target/\" + params.projectName + \".jar root@${ip}:/root/projects/\" + params.projectName sh \"${cmdCopy}\" } sh \"ssh root@${ip} 'cd /root/projects/\" + params.projectName + \";/root/projects/payReboot.sh start ${params.projectName}'\" } } } } } }} 使用SCP发布文件 node 作用和agent类似.都是用于指定执行任务的Jenkins服务器. 但是agents用在表述性pipeline中，可以不仅仅是nodes,还可以是docker container等. node则是被用于脚本化pipeline中. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def remoteConfig = [:]remoteConfig.name = \"my-remote-server\"remoteConfig.host = \"${env.REMOTE_HOST}\"remoteConfig.allowAnyHosts = true// 使用 SCP 作为文件传输remoteConfig.fileTransfer = \"SCP\"node { withCredentials([sshUserPrivateKey( credentialsId: \"${env.REMOTE_CRED}\", keyFileVariable: \"privateKeyFilePath\" )]) { // SSH 登陆用户名 remoteConfig.user = \"${env.REMOTE_USER_NAME}\" // SSH 私钥文件地址 remoteConfig.identityFile = privateKeyFilePath stage(\"通过 SSH 执行命令\") { // 本地创建一个 test.sh 脚本，用来发送到远端执行 writeFile ( file: 'test.sh', text: 'ls' ) // 将本地的文件复制到远端 sshPut( remote: remoteConfig, // 本地文件或文件夹 from: 'test.sh', // 远端文件或文件夹 into: '.' ) // 将远端的文件复制到本地 sshGet ( remote: remoteConfig, // 远端文件或文件夹 from: 'test.sh', // 本地文件或文件夹 into: 'test_new.sh', // 是否允许覆盖本地文件 override: true ) } }}","link":"/posts/a5086bf.html"},{"title":"Spring Cloud Open Feign详解","text":"Fegin介绍Feign是一个声明式的http客户端,只需要创建一个接口并加上注解,就可以通过feigin调用http接口. 另外Feign默认集成了Ribbon和Hystrix，并和Eureka结合，默认实现了负载均衡和熔断器的效果。 简单来说: Feign使用基于接口的注解进行定义 Feign整合了Ribbon和Hystrix 使用Feign可以像调用原生java方法一样调用其他微服务暴露的接口 如何通过Feign调用其它微服务 需要在maven工程引入依赖12345678910111213141516171819&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;relativePath/&gt;&lt;/parent&gt;&lt;dependencies&gt;&lt;!-- feign --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;/dependencies&gt; 在启动类上加上@EnableFeignClients注解以启用Feign客户端123456789@SpringBootApplication@EnableDiscoveryClient@EnableFeignClientspublic class ServiceFeignApplication { public static void main(String[] args) { SpringApplication.run(ServiceFeignApplication.class, args); }} 定义一个Fegin接口,通过@FeignClient(&quot;服务名&quot;)选择调用的服务,通过定义方法指定需要定义的接口123456789101112131415161718192021@FeignClient(value = \"bas\")public interface BasRemoteProxy { @PostMapping(\"/api/business-flow\") BusinessDTO pushBasWater(@RequestBody BusinessDTO businessDTO); //使用query参数 //需要在参数前面加上注解 @RequestParam(\"参数名称\") @PostMapping(\"/sms/sendMessage\") void sendMessage(@RequestParam(\"smsSign\") String smsSign, @RequestParam(\"phone\") String phone, @RequestParam(\"smsContent\") String smsContent); //使用url占位符 //需要在参数前面加上注解 @PathVariable(\"参数名称\") @GetMapping(\"/lending-organizations/{organizationCode}\") FundProductDTO getLendingOrganization(@PathVariable(value = \"organizationCode\") Long organizationCode); //使用对象作为get参数 //需要使用 @SpringQueryMap 注解 @GetMapping(path = \"/data-access/all\") Page&lt;DataAccess&gt; dataAccessQuery(@SpringQueryMap PageableParams params);} 在Feign中使用熔断器(Hystrix)进行服务降级处理Hystrix熔断器的作用用一句话概括,就是在调用服务失败/出现异常时,提供一个服务降级的处理逻辑. 对于查询操作, 我们可以实现一个fallback方法, 当请求后端服务出现异常的时候, 可以使用fallback方法返回的值. fallback方法的返回值一般是设置的默认值或者来自缓存. 触发Hystrix熔断器的条件 接口调用超时. 接口调用抛出了非HystrixBadRequestException的异常 并发数量超过了Hystrix线程池的线程数目.那么多余的请求会被抛弃. 有大量请求到达,而且有多于一定百分比的请求失败. 如何使用 ? 通过application.yml 配置启用Hystrix123feign: hystrix: enabled: true 接下来我们要给Feign 客户端BasRemoteProxy配置fallback.配置fallback有两种方式: 实现Feign接口 实现FallbackFactory 方式1:Fallback实现 - 实现Feign接口通过实现BasRemoteProxy接口实现,需要通过@Component注册为spring组件 BasRemoteProxyFallBack类: 123456789101112131415161718192021222324//@Component(\"basRemoteProxyFallBack\")@Slf4jpublic class BasRemoteProxyFallBack implements BasRemoteProxy { @Override public BusinessDTO pushBasWater(BusinessDTO businessDTO) { log.error(\"调用pushBasWater接口失败\"); //模拟内部调用失败 throw new RuntimeException(\"内部异常\"); } @Override public void sendMessage(String smsSign, String phone, String smsContent) { log.error(\"调用sendMessage接口失败\"); } @Override public FundProductDTO getLendingOrganization(Long organizationCode) { log.error(\"调用getLendingOrganization接口失败\"); return null; }} BasRemoteProxy配置: 1234@FeignClient(value = \"bas\", fallback = BasRemoteProxyFallBack.class)public interface BasRemoteProxy {...... 方式2 : Fallback实现 - 实现FallbackFactory上面的方法实现方式简单，但是获取不到HTTP请求错误状态码和信息 ，这时就可以使用工厂模式来实现Fallback,同样需要通过@Component注册为spring组件 BasRemoteProxyFallBackFactory类: 123456789101112131415161718192021222324252627282930313233@Component@Slf4jpublic class BasRemoteProxyFallBackFactory implements FallbackFactory&lt;BasRemoteProxy&gt; { @Override public BasRemoteProxy create(Throwable throwable) { //打印异常信息 log.error(\"BasRemoteProxy调用异常\",throwable); //编写fallback,为了直观, return new BasRemoteProxy() { @Override public BusinessDTO pushBasWater(BusinessDTO businessDTO) { log.error(\"调用pushBasWater接口失败\"); //模拟内部调用失败 throw new RuntimeException(\"内部异常\"); } @Override public void sendMessage(String smsSign, String phone, String smsContent) { log.error(\"调用sendMessage接口失败\"); } @Override public FundProductDTO getLendingOrganization(Long organizationCode) { log.error(\"调用getLendingOrganization接口失败\"); return null; } }; }} BasRemoteProxy配置: 12@FeignClient(value = \"bas\", fallbackFactory = BasRemoteProxyFallBackFactory.class)public interface BasRemoteProxy { 设置超时时间12345678feign: client: config: default: #连接超时时间 connectTimeout: 100000 # 处理超时时间 readTimeout: 1000000 为Feign自定义 Error Decoder在使用Feign调用微服务,触发熔断机制时,会出现一些问题: 调用失败时,异常信息不直观.类似：BasRemoteProxy#pushBasWater(ParamVO) failed and no fallback available. 无法获取到服务提供者抛出的原始异常信息 通过自定义 ErrorDecoder可以解决上面的问题. 如何编写当调用服务时，如果服务返回的http status &gt;= 300且不是404，就会进入到Feign的ErrorDecoder中，因此如果我们要解析异常信息，就要实现ErrorDecoder接口. ErrorDecoder接口: 123public interface ErrorDecoder { public Exception decode(String methodKey, Response response);} 参数: methodKey : 出现异常的FeginClient类#方法名(参数) 例: BasRemoteProxy#pushBasWater(ParamVO) response: 封装了从客户端接收到的响应值. 编写FeignErrorDecoder类: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Configuration@ConditionalOnClass(ErrorDecoder.class)public class FeignErrorDecoder extends ErrorDecoder.Default implements ErrorDecoder { private static final Logger log = LoggerFactory.getLogger(FeignErrorDecoder.class); @Override public Exception decode(String methodKey, Response response) { String message = StringUtils.EMPTY; if (response.body() != null) { try { message = Util.toString(response.body().asReader()); } catch (IOException e) { log.error(\"feign 异常处理失败：\", e); } } // 这里直接拿到我们抛出的异常信息 try { JsonNode jsonNode = JsonUtils.readTree(message); //获取errorCode int errorCode = jsonNode.get(\"errorCode\").asInt(NumberUtils.INTEGER_MINUS_ONE); //获取异常信息message String errorMessage = jsonNode.get(\"message\").asText(StringUtils.EMPTY); //根据code和message包装不同类型的异常 switch (errorCode) { case 417: return new BizException(errorCode, errorMessage); case 500: if(errorMessage.contains(\"NullPointerException\")){ return new NullPointerException(errorMessage); } if(errorMessage.contains(\"IllegalArgumentException\")){ return new IllegalArgumentException(errorMessage); } if(errorMessage.contains(\"IllegalStateException\")){ return new IllegalStateException(errorMessage); } default: return new UnhandledFeignException(errorCode, message); } } catch (Exception e) { log.error(\"feign 异常处理失败：\", e); String msg = format(\"status %s reading %s\", response.status(), methodKey); if (response.body() != null) { msg += \"; content:\\n\" + message; } return new DecodeException(msg); } }}","link":"/posts/fc26f183.html"},{"title":"基于redis的分布式读写锁实现","text":"分布式锁在分布式系统中,如果不同的系统或者同一个系统的不同服务共享了同一个资源.那么在访问这些资源的时候,就需要一些互斥的方式防止出现数据一致性问题.在这种情况下就需要使用分布式锁了. 读写锁读写锁是在同一时间允许多个读操作访问,但是在写操作时会阻塞其他所有的读写操作.java 提供的默认实现是ReentrantReadWriteLock 设计思路 节点 简单介绍一下.首先是如何定义锁节点.通过redis-key来代表一个锁. 比如 WLOCK:ARTICLE .这个节点代表了资源ARTICLE的读锁.RLOCK:ARTICLE 则代表了资源ARTICLE的写锁. 获取锁 如果是读锁: 判断是否存在写锁. 如果否,读锁计数器 + 1并返回成功. 如果是,重复以上步骤直到获取到锁或者超时为止. 如果是写锁: 判断是否存在写锁或者读锁. 如果否, 设置写锁 并且返回成功. 如果是,重复以上步骤直到获取到锁或者超时为止. 释放锁 如果是读锁: 读锁计数器 - 1. 如果计数器 == 0,删除对应的锁. 如果是写锁: 直接删除对应的锁 如何保证操作的原子性 通过在redis中执行lua脚本实现原子操作. 官方解释 : Atomicity of scripts Redis uses the same Lua interpreter to run all the commands. Also Redis guarantees that a script is executed in an atomic way: no other script or Redis command will be executed while a script is being executed. This semantic is similar to the one of MULTI / EXEC. From the point of view of all the other clients the effects of a script are either still not visible or already completed. 大概意思是说,在redis中执行lua脚本,是具有原子性的.作用就类似加上了multi/exec.可以放心使用. 如何探测死锁 对每一个锁的对象提供一个计数器 每当获取锁失败时计数器 + 1.计数达到阈值时发起告警. 为什么不用Redisson的实现 因为根据需求,解锁操作和上锁操作可能在不同的实例上.而Redisson提供的读写锁是和线程绑定的. 风险 实例A获取锁失败抛出异常,然后在finally块中执行了unlock()操作.因为锁计数器是所有实例共享的,所以可能把其他实例上的锁解开了,这时候某个实例可能还在对数据进行操作,从而导致数据一致性问题. 如何处理死锁 = 计数器探测 + 达到阈值时发起告警. … 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235/** * 分布式读写锁 * &lt;p&gt; * * @Author: lzh * @Date: 2020/2/7 10:54 上午 */@Slf4jpublic class DistributedReadWriteLock implements ReadWriteLock { private static final String PREFIX = \"LOCK\"; /** * 死锁探测阈值 * */ private static final Long DEAD_LOCK_NOTIFY_LIM = 10000L; /** * 单线程executor */ private final ExecutorService pool = Executors.newSingleThreadExecutor(); /** * 资源唯一标识 */ private final String key; /** * 死锁计数器 */ private final AtomicInteger deadLockCount; public DistributedReadWriteLock(@NonNull String key) { this.key = key; this.deadLockCount = new AtomicInteger(0); } /** * 获取锁的key * @param key * @param mode * @return */ String getLockName(@NonNull String key, @NonNull Mode mode) { return PREFIX + \":\" + \"LOCK:\" + key + \":\" + (mode == READ ? \"R_LOCK\" : \"W_LOCK\"); } @Override public Lock readLock() { return new DistributedLock(READ); } @Override public Lock writeLock() { return new DistributedLock(WRITE); } class DistributedLock implements Lock { /** * 获取锁成功时,lua脚本返回的标识 */ private static final String SUCCESS = \"SUCCESS\"; private final Mode mode; public DistributedLock(@NonNull Mode mode) { this.mode = mode; } @Override public void lock() { String acquireLockResult = tryAcquireLock(key, mode, -1, null); if (!SUCCESS.equals(acquireLockResult)) { throw new IllegalStateException(\"get lock error:\" + acquireLockResult); } } @Override public void lockInterruptibly() throws InterruptedException { throw new UnsupportedOperationException(\"unsupported\"); } @Override public boolean tryLock() { String result = tryAcquireLock(key, mode, 0, TimeUnit.MILLISECONDS); if (SUCCESS.equals(result)) { return true; } return false; } @Override public boolean tryLock(long time, @NonNull TimeUnit unit) throws InterruptedException { String result = tryAcquireLock(key, mode, time, unit); if (SUCCESS.equals(result)) { return true; } return false; } @Override public void unlock() { releaseLock(key, mode); } @Override public Condition newCondition() { throw new UnsupportedOperationException(\"不支持的操作\"); } private String releaseLock(@NonNull String key, @NonNull Mode mode) { //release lock script RedisScript&lt;String&gt; script = RedisScript.of( \"local mode = ARGV[1]\\n\" + \"if(mode == 'READ') then \\n\" + \" local r_key = redis.call('decr',KEYS[2]);\\n\" + \" if(r_key &lt;= 0)then\\n\" + \" redis.call('del',KEYS[2]);\\n\" + \" end;\\n\" + \" return 'SUCCESS';\\n\" + \"else if(mode == 'WRITE') then\\n\" + \" redis.call('del',KEYS[1]);\\n\" + \" return 'SUCCESS';\\n\" + \"end;\\n\" + \"end;\", String.class); return Helper.getRedisTemplate().execute(script, Arrays.asList(getLockName(key, WRITE), getLockName(key, READ)), mode.name()); } /** * 尝试获取锁 * * @param key key * @param mode 模式 读或者写 参考读写锁的定义 * @param time 超时时间 -1 = 不超时 0 = 只尝试一次 * @param unit 时间单位 * @return */ private String tryAcquireLock(@NonNull String key, @NonNull Mode mode, long time, TimeUnit unit) { Assert.isTrue(time &gt;= -1, \"invalid time\"); Future&lt;?&gt; future = pool.submit(() -&gt; { long mills = time == -1 ? -1 : unit.toMillis(time); int sleepMills = 100; String result = null; while (!SUCCESS.equals(result)) { //acquire lock script RedisScript&lt;String&gt; of = RedisScript.of( \"local mode = ARGV[1]\\n\" + \"if(mode == 'READ') then\\n\" + \" local w_key = redis.call('exists',KEYS[1]);\\n\" + \" if(w_key == 1) then\\n\" + \" return 'WRITE_LOCKED';\\n\" + \" else\\n\" + \" local r_key = redis.call('incr',KEYS[2]);\\n\" + \" return 'SUCCESS';\\n\" + \" end;\\n\" + \"elseif(mode == 'WRITE') then\\n\" + \" local w_key = redis.call('exists',KEYS[1]);\\n\" + \" local r_key = redis.call('exists',KEYS[2]);\\n\" + \" if(r_key == 1) then\\n\" + \" return 'READ_LOCKED';\\n\" + \" elseif(w_key == 1) then\\n\" + \" return 'WRITE_LOCKED';\\n\" + \" else \\n\" + \" local r_key = redis.call('setnx',KEYS[1],'LOCKED');\\n\" + \" return 'SUCCESS';\\n\" + \" end;\\n\" + \"end;\", String.class); result = Helper.getRedisTemplate().execute(of, Arrays.asList(getLockName(key, WRITE), getLockName(key, READ)), mode.name()); try { if (!SUCCESS.equals(result)) { //如果达到尝试次数限制,直接返回 if (mills &gt;= 0 &amp;&amp; (mills -= sleepMills) &lt;= 0) { break; } Thread.sleep(sleepMills); } if (deadLockCount.getAndIncrement() &gt;= DEAD_LOCK_NOTIFY_LIM) { log.error(\"dead lock found when trying to get lock,key:[{}]\", key); } } catch (InterruptedException ignored) { } } return result; }); return wrapReturn(future); } private String wrapReturn(Future&lt;?&gt; future) { try { return (String) future.get(); } catch (InterruptedException e) { return \"INTERRUPTED\"; } catch (ExecutionException e) { log.error(\"EXECUTION_ERROR\", e); return \"EXECUTION_ERROR\"; } } } /** * 锁的模式.READ = 读锁,WRITE = 写锁 */ enum Mode { READ, WRITE } /** * 静态内部类注入redisTemplate */ @Component static class Helper { private static RedisTemplate&lt;String, String&gt; redisTemplate; public static RedisTemplate&lt;String, String&gt; getRedisTemplate() { return Helper.redisTemplate; } Helper(@Qualifier(\"stringRedisTemplate\") RedisTemplate&lt;String, String&gt; redisTemplate) { Helper.redisTemplate = redisTemplate; } }}","link":"/posts/5d5a403f.html"},{"title":"使用docker搭建mongo集群+密码验证","text":"之前尝试过用docker搭建单机版的mongodb.但是.单机版的mongo有个问题,不支持事务. 目前 mongodb 4.0已经支持副本集级别的事务了. 为了让测试环境的Mongo和生产环境的功能保持一致.我决定尝试使用docker搭建一个一主两从的高可用的mongo复制集. 在这里把搭建过程记录下来. 搭建过程使用keyfile的访问控制方式部署复制集keyfile是mongo复制集 集群内部的一种认证机制.为了连接到复制集,成员需要对keyfile进行认证. 不使用验证也可以搭建集群~ 但是连带登录密码也不能使用,出于安全性考虑不推荐这么干. Keyfiles是安全的最小格式，非常适合测试和开发环境。对于生产环境，官方推荐使用x.509 certificates. 生成keyfile 生成keyfile并修改权限 12openssl rand -base64 756 &gt; ~/mongo-cluster/mongodbKeyfile.keychmod 400 ~/mongo-cluster/mongodbKeyfile.key 创建共有配置文件mongod.conf 生成mongod.conf配置并修改权限 容器内部端口27017,keyfile路径为容器内/data/configdb/mongodbKeyfile.key,复制集名称为replset0 12345678910111213141516cat&gt;~/mongo-cluster/mongod.conf&lt;&lt;EOFnet: port: 27017 bindIp: 0.0.0.0 bindIpAll: true systemLog: logAppend: true security: keyFile: \"/data/configdb/mongodbKeyfile.key\" authorization: \"enabled\"setParameter: enableLocalhostAuthBypass: true replication: replSetName: \"replset0\"EOFchmod 400 ~/mongo-cluster/mongod.conf 创建自定义网络 创建名为mongo-network的自定义网络,用于部署docker集群 123456docker network create \\ --driver=bridge \\ --subnet=172.28.0.0/16 \\ --ip-range=172.28.5.0/24 \\ --gateway=172.28.5.254 \\ mongo-network 启动副本集容器启动主服务器命令解释 : 创建了名为m0的docker容器,并且把容器端口27107映射到宿主机47107 加入自定义网络mongo-network,并且绑定ip172.28.5.100 挂载数据卷,将宿主的~/mongo-cluster/data0,mongod.conf和mongodbKeyfile.key挂载到容器内部. 创建用户名=root,密码=123的管理员.然后为mongo启用权限控制. 1234567docker run --name m0 -p 47017:27017 \\--network mongo-network \\--ip 172.28.5.100 \\-v ~/mongo-cluster/data0:/data/db \\-v ~/mongo-cluster/mongod.conf:/data/configdb/mongod.conf \\-v ~/mongo-cluster/mongodbKeyfile.key:/data/configdb/mongodbKeyfile.key \\-d mongo sh -c \"mongod --fork --syslog &amp;&amp;mongo admin --eval \\\"db.createUser({user: 'root', pwd: '123', roles: [{ role: 'root', db: 'admin' }]})\\\" &amp;&amp; mongod --shutdown &amp;&amp; mongod --auth -f /data/configdb/mongod.conf \" \\ 启动复制集11234567docker run --name m1 -p 47018:27017 \\--network mongo-network \\--ip 172.28.5.101 \\-v ~/mongo-cluster/data1:/data/db \\-v ~/mongo-cluster/mongod.conf:/data/configdb/mongod.conf \\-v ~/mongo-cluster/mongodbKeyfile.key:/data/configdb/mongodbKeyfile.key \\-d mongo sh -c \"mongod --fork --syslog &amp;&amp;mongo admin --eval \\\"db.createUser({user: 'root', pwd: '123', roles: [{ role: 'root', db: 'admin' }]})\\\" &amp;&amp; mongod --shutdown &amp;&amp; mongod --auth -f /data/configdb/mongod.conf \" \\ 启动复制集21234567docker run --name m2 -p 47019:27017 \\--network mongo-network \\--ip 172.28.5.102 \\-v ~/mongo-cluster/data2:/data/db \\-v ~/mongo-cluster/mongod.conf:/data/configdb/mongod.conf \\-v ~/mongo-cluster/mongodbKeyfile.key:/data/configdb/mongodbKeyfile.key \\-d mongo sh -c \"mongod --fork --syslog &amp;&amp;mongo admin --eval \\\"db.createUser({user: 'root', pwd: '123', roles: [{ role: 'root', db: 'admin' }]})\\\" &amp;&amp; mongod --shutdown &amp;&amp; mongod --auth -f /data/configdb/mongod.conf \" \\ 这里有两个需要注意的点: 副本集服务器的管理员密码一定要和主服务器保持一致.否则会出现认证失败. 端口映射命令-p 47019:27017会绕过操作系统防火墙直接通过iptables开放端口.这是一个很大的坑. 如果只在本机访问可以用命令-p 127.0.0.1:47019:27017代替. 初始化集群主服务器 通过Mongo shell登录admin库 12345docker exec -it m0 mongo admin#使用启动容器时设定的账号密码进行认证db.auth(\"root\",\"123\")#认证成功1 初始化集群配置 1234config = { _id:\"replset0\", members:[{_id:0,host:\"172.28.5.100:27017\"},{_id:1,host:\"172.28.5.101:27017\"},{_id:2,host:\"172.28.5.102:27017\"}]}rs.initiate(config)#初始化成功{ \"ok\" : 1 } 复制集服务器 退出主服务器,登录复制集服务器,进行初始化操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161 #初始化复制集1 docker exec -it m1 mongo admin #使用启动容器时设定的账号密码进行认证 db.auth(\"root\",\"123\") #认证成功 1 #默认情况下复制集是不允许访问的.开启访问功能 rs.slaveOk(); #退出复制集1 #初始化复制集2 docker exec -it m2 mongo admin #使用启动容器时设定的账号密码进行认证 db.auth(\"root\",\"123\") #认证成功1 #默认情况下复制集是不允许访问的.开启访问功能 rs.slaveOk(); #查看集群状态 rs.status(); { \"set\" : \"replset0\", \"date\" : ISODate(\"2020-03-11T09:44:29.436Z\"), \"myState\" : 2, \"term\" : NumberLong(1), \"syncingTo\" : \"172.28.5.100:27017\", \"syncSourceHost\" : \"172.28.5.100:27017\", \"syncSourceId\" : 0, \"heartbeatIntervalMillis\" : NumberLong(2000), \"majorityVoteCount\" : 2, \"writeMajorityCount\" : 2, \"optimes\" : { \"lastCommittedOpTime\" : { \"ts\" : Timestamp(1583919866, 1), \"t\" : NumberLong(1) }, \"lastCommittedWallTime\" : ISODate(\"2020-03-11T09:44:26.390Z\"), \"readConcernMajorityOpTime\" : { \"ts\" : Timestamp(1583919866, 1), \"t\" : NumberLong(1) }, \"readConcernMajorityWallTime\" : ISODate(\"2020-03-11T09:44:26.390Z\"), \"appliedOpTime\" : { \"ts\" : Timestamp(1583919866, 1), \"t\" : NumberLong(1) }, \"durableOpTime\" : { \"ts\" : Timestamp(1583919866, 1), \"t\" : NumberLong(1) }, \"lastAppliedWallTime\" : ISODate(\"2020-03-11T09:44:26.390Z\"), \"lastDurableWallTime\" : ISODate(\"2020-03-11T09:44:26.390Z\") }, \"lastStableRecoveryTimestamp\" : Timestamp(1583919836, 4), \"lastStableCheckpointTimestamp\" : Timestamp(1583919836, 4), \"electionParticipantMetrics\" : { \"votedForCandidate\" : true, \"electionTerm\" : NumberLong(1), \"lastVoteDate\" : ISODate(\"2020-03-11T09:43:55.290Z\"), \"electionCandidateMemberId\" : 0, \"voteReason\" : \"\", \"lastAppliedOpTimeAtElection\" : { \"ts\" : Timestamp(1583919825, 1), \"t\" : NumberLong(-1) }, \"maxAppliedOpTimeInSet\" : { \"ts\" : Timestamp(1583919825, 1), \"t\" : NumberLong(-1) }, \"priorityAtElection\" : 1, \"newTermStartDate\" : ISODate(\"2020-03-11T09:43:56.388Z\"), \"newTermAppliedDate\" : ISODate(\"2020-03-11T09:43:57.251Z\") }, \"members\" : [ { \"_id\" : 0, \"name\" : \"172.28.5.100:27017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 43, \"optime\" : { \"ts\" : Timestamp(1583919866, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1583919866, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2020-03-11T09:44:26Z\"), \"optimeDurableDate\" : ISODate(\"2020-03-11T09:44:26Z\"), \"lastHeartbeat\" : ISODate(\"2020-03-11T09:44:29.071Z\"), \"lastHeartbeatRecv\" : ISODate(\"2020-03-11T09:44:29.373Z\"), \"pingMs\" : NumberLong(0), \"lastHeartbeatMessage\" : \"\", \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"infoMessage\" : \"\", \"electionTime\" : Timestamp(1583919835, 1), \"electionDate\" : ISODate(\"2020-03-11T09:43:55Z\"), \"configVersion\" : 1 }, { \"_id\" : 1, \"name\" : \"172.28.5.101:27017\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 43, \"optime\" : { \"ts\" : Timestamp(1583919866, 1), \"t\" : NumberLong(1) }, \"optimeDurable\" : { \"ts\" : Timestamp(1583919866, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2020-03-11T09:44:26Z\"), \"optimeDurableDate\" : ISODate(\"2020-03-11T09:44:26Z\"), \"lastHeartbeat\" : ISODate(\"2020-03-11T09:44:29.071Z\"), \"lastHeartbeatRecv\" : ISODate(\"2020-03-11T09:44:27.877Z\"), \"pingMs\" : NumberLong(0), \"lastHeartbeatMessage\" : \"\", \"syncingTo\" : \"172.28.5.102:27017\", \"syncSourceHost\" : \"172.28.5.102:27017\", \"syncSourceId\" : 2, \"infoMessage\" : \"\", \"configVersion\" : 1 }, { \"_id\" : 2, \"name\" : \"172.28.5.102:27017\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 102, \"optime\" : { \"ts\" : Timestamp(1583919866, 1), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2020-03-11T09:44:26Z\"), \"syncingTo\" : \"172.28.5.100:27017\", \"syncSourceHost\" : \"172.28.5.100:27017\", \"syncSourceId\" : 0, \"infoMessage\" : \"\", \"configVersion\" : 1, \"self\" : true, \"lastHeartbeatMessage\" : \"\" } ], \"ok\" : 1, \"$clusterTime\" : { \"clusterTime\" : Timestamp(1583919866, 1), \"signature\" : { \"hash\" : BinData(0,\"2c8e97L8m8sn43vbxoTdttVR6PM=\"), \"keyId\" : NumberLong(\"6802883895105683458\") } }, \"operationTime\" : Timestamp(1583919866, 1) } 到这里通过docker搭建mongo集群的过程就全部完成了. 需要注意: 初始化集群配置时,如果需要从非本地节点(localhost)连接集群.那么config中配置的ip地址必须是外网能够访问到的地址. 如何连接到集群通过spring data mongodb 连接到集群的配置 在application-{环境}.yml中配置连接用户名root,密码123,数据库名为pay-platform的复制集,配置如下:123mongodb: uri: mongodb://root:123@172.28.5.100:47017,172.28.5.101:47018,172.28.5.102:47019/?replicaSet=replset0 database: pay-platform 通过可视化工具连接到集群建议只连接到主服务器即可,无需连接副本集,不影响常规读写操作.","link":"/posts/4dcdf059.html"},{"title":"java agent技术详解","text":"JDK1.5开始，Java新增了Instrumentation(Java Agent API)和JVMTI(JVM Tool Interface)功能，允许JVM在加载某个class文件之前对其字节码进行修改，同时也支持对已加载的class(类字节码)进行重新加载(Retransform)。 利用Java Agent这一特性衍生出了APM(Application Performance Management，应用性能管理)、RASP(Runtime application self-protection，运行时应用自我保护)、IAST(Interactive Application Security Testing，交互式应用程序安全测试)等相关产品，它们都无一例外的使用了Instrumentation/JVMTI的API来实现动态修改Java类字节码并插入监控或检测代码。 Java Agent有两种运行模式： 启动Java程序时添加-javaagent(Instrumentation API实现方式)或-agentpath/-agentlib(JVMTI的实现方式)参数，如java -javaagent:/data/XXX.jar LingXeTest。 JDK1.6新增了attach(附加方式)方式，可以对运行中的Java进程附加Agent。 这两种运行方式的最大区别在于第一种方式只能在程序启动时指定Agent文件，而attach方式可以在Java程序运行后根据进程ID动态注入Agent到JVM。 Java Agent Hello World让我们来运行一个Java的HelloWorld程序。 HelloWorld示例代码： 12345678910111213package com.anbai.sec.agent;/** * Creator: yz * Date: 2020/1/2 */public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello World...\"); }} 程序运行结果： 1Hello World... 假设我们现在有一个需求：必须在不重新编译某个类的情况下(甚至有可能是不重启应用服务的情况下)动态的改变类方法的执行逻辑是非常困难的，但如果使用Agent的Instrumentation API就可以非常容易的实现了，例如将下列程序(HelloWorld.java)的输出变成Hello Agent...。 首先我们需要修改：javaweb-sec/javaweb-sec-source/javasec-agent/src/main/resources/MANIFEST.MF文件中的Premain-Class配置为com.anbai.sec.agent.JavaSecHelloWorldAgent，然后再执行如下命令使用Maven构建Agent Jar包： 12cd javaweb-sec/javaweb-sec-source/javasec-agentmvn clean install Maven构建完成后在javaweb-sec/javaweb-sec-source/javasec-agent/target目录会自动生成一个javasec-agent.jar文件，这个文件也就是我们写好的用于处理HelloWorld程序输出结果的Java Agent程序。 JavaSecHelloWorldAgent动态替换HelloWorld字符串示例代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package com.anbai.sec.agent;import java.lang.instrument.ClassFileTransformer;import java.lang.instrument.Instrumentation;import java.security.ProtectionDomain;import java.util.Arrays;/** * Creator: zh * Date: 2020/3/22 */public class JavaSecHelloWorldAgent { /** * 替换HelloWorld的输出字符串为\"Hello Agent...\"，将二进制转换成字符串数组，替换字符串数组并生成新的二进制 * * @param className 类名 * @param classBuffer 类字节码 * @return 替换后的类字节码 */ private static byte[] replaceBytes(String className, byte[] classBuffer) { // 将类字节码转换成byte字符串 String bufferStr = Arrays.toString(classBuffer); System.out.println(className + \"类替换前的字节码:\" + bufferStr); bufferStr = bufferStr.replace(\"[\", \"\").replace(\"]\", \"\"); // 查找需要替换的Java二进制内容 byte[] findBytes = \"Hello World...\".getBytes(); // 把搜索的字符串byte转换成byte字符串 String findStr = Arrays.toString(findBytes).replace(\"[\", \"\").replace(\"]\", \"\"); // 二进制替换后的byte值，注意这个值需要和替换的字符串长度一致，不然会破坏常量池 byte[] replaceBytes = \"Hello Agent...\".getBytes(); // 把替换的字符串byte转换成byte字符串 String replaceStr = Arrays.toString(replaceBytes).replace(\"[\", \"\").replace(\"]\", \"\"); bufferStr = bufferStr.replace(findStr, replaceStr); // 切割替换后的byte字符串 String[] byteArray = bufferStr.split(\"\\\\s*,\\\\s*\"); // 创建新的byte数组，存储替换后的二进制 byte[] bytes = new byte[byteArray.length]; // 将byte字符串转换成byte for (int i = 0; i &lt; byteArray.length; i++) { bytes[i] = Byte.parseByte(byteArray[i]); } System.out.println(className + \"类替换后的字节码:\" + Arrays.toString(bytes)); // 返回修改后的二进制 return bytes; } /** * Java Agent模式入口 * * @param args 命令参数 * @param inst Agent Instrumentation 实例 */ public static void premain(String args, final Instrumentation inst) { // 添加自定义的Transformer inst.addTransformer(new ClassFileTransformer() { /** * 类文件转换方法，重写transform方法可获取到待加载的类相关信息 * * @param loader 定义要转换的类加载器；如果是引导加载器，则为 null * @param className 类名,如:java/lang/Runtime * @param classBeingRedefined 如果是被重定义或重转换触发，则为重定义或重转换的类；如果是类加载，则为 null * @param protectionDomain 要定义或重定义的类的保护域 * @param classfileBuffer 类文件格式的输入字节缓冲区（不得修改） * @return 返回一个通过ASM修改后添加了防御代码的字节码byte数组。 */ @Override public byte[] transform(ClassLoader loader, String className, Class&lt;?&gt; classBeingRedefined, ProtectionDomain protectionDomain, byte[] classfileBuffer) { // 将目录路径替换成Java类名 className = className.replace(\"/\", \".\"); // 只处理com.anbai.sec.agent.HelloWorld类的字节码 if (className.equals(\"com.anbai.sec.agent.HelloWorld\")) { // 替换HelloWorld的输出字符串 return replaceBytes(className, classfileBuffer); } return classfileBuffer; } }, true);// 第二个参数true表示是否允许Agent Retransform，需配合MANIFEST.MF中的Can-Retransform-Classes: true配置 }} 我们需要在运行HelloWorld的时候添加-javaagent:jar路径参数，例如： 1java -jar -javaagent:/Users/yz/IdeaProjects/javaweb-sec/javaweb-sec-source/javasec-agent/target/javasec-agent.jar com.anbai.sec.agent.HelloWorld 程序执行结果： 123com.anbai.sec.agent.HelloWorld类替换前的字节码:[....省去两则一致的数据, 87, 111, 114, 108, 100, ...]com.anbai.sec.agent.HelloWorld类替换后的字节码:[....省去两则一致的数据, 65, 103, 101, 110, 116, ...]Hello Agent... 由上可以看到程序的最终执行结果已经被我们动态的修改为了：Hello Agent...，这种方式是最为简单暴力的修改二进制中的字符串值的方式在实际的业务场景下很显然是不可行的，因为只要修改后的字符串长度不一致就会破坏常量池导致程序无法执行，为了能够精准有效的修改类字节码我们通常会使用ASM库。 Instrumentationjava.lang.instrument.Instrumentation是Java提供的监测运行在JVM程序的API。利用Instrumentation我们可以实现如下功能： 动态添加自定义的Transformer(addTransformer)。 动态修改classpath(appendToBootstrapClassLoaderSearch、appendToSystemClassLoaderSearch)。 动态获取所有JVM已加载的类(getAllLoadedClasses)。 动态获取某个类加载器已实例化的所有类(getInitiatedClasses)。 直接修改已加载的类的字节码(redefineClasses)。 动态设置JNI前缀(setNativeMethodPrefix)。 重加载指定类字节码(retransformClasses)。 Instrumentation类方法如下： ClassFileTransformerjava.lang.instrument.ClassFileTransformer是一个转换类文件的代理接口，我们可以在获取到Instrumentation对象后通过addTransformer方法添加自定义类文件转换器。 示例中我们使用了addTransformer注册了一个我们自定义的Transformer到Java Agent，当有新的类被JVM加载时JVM会自动回调用我们自定义的Transformer类的transform方法，传入该类的transform信息(类名、类加载器、类字节码等)，我们可以根据传入的类信息决定是否需要修改类字节码，修改完字节码后我们将新的类字节码返回给JVM，JVM会验证类和相应的修改是否合法，如果符合类加载要求JVM会加载我们修改后的类字节码。 ClassFileTransformer类代码： 123456789101112131415161718package java.lang.instrument;public interface ClassFileTransformer { /** * 类文件转换方法，重写transform方法可获取到待加载的类相关信息 * * @param loader 定义要转换的类加载器；如果是引导加载器，则为 null * @param className 类名,如:java/lang/Runtime * @param classBeingRedefined 如果是被重定义或重转换触发，则为重定义或重转换的类；如果是类加载，则为 null * @param protectionDomain 要定义或重定义的类的保护域 * @param classfileBuffer 类文件格式的输入字节缓冲区（不得修改） * @return 返回一个通过ASM修改后添加了防御代码的字节码byte数组。 */ byte[] transform(ClassLoader loader, String className, Class&lt;?&gt; classBeingRedefined, ProtectionDomain protectionDomain, byte[] classfileBuffer);} 重写transform方法需要注意以下事项： ClassLoader如果是被Bootstrap ClassLoader(引导类加载器)所加载那么loader参数的值是空。 修改类字节码时需要特别注意插入的代码在对应的ClassLoader中可以正确的获取到，否则会报ClassNotFoundException，比如修改java.io.FileInputStream(该类由Bootstrap ClassLoader加载)时插入了我们检测代码，那么我们将必须保证FileInputStream能够获取到我们的检测代码类。 JVM类名的书写方式路径方式：java/lang/String而不是我们常用的类名方式：java.lang.String。 类字节必须符合JVM校验要求，如果无法验证类字节码会导致JVM崩溃或者VerifyError(类验证错误)。 如果修改的是retransform类(修改已被JVM加载的类)，修改后的类字节码不得新增方法、修改方法参数、类成员变量。 addTransformer时如果没有传入retransform参数(默认是false)就算MANIFEST.MF中配置了Can-Redefine-Classes: true而且手动调用了retransformClasses方法也一样无法retransform。 卸载transform时需要使用创建时的Instrumentation实例。","link":"/posts/6ff3a7d0.html"},{"title":"java.util.concurrent 并发包介绍","text":"转载,英文原文:https://www.ibm.com/developerworks/java/tutorials/j-concur/j-concur.html java.util.concurrent 包含许多线程安全、测试良好、高性能的并发构建块。不客气地说，创建 java.util.concurrent 的目的就是要实现 Collection 框架对数据结构所执行的并发操作。通过提供一组可靠的、高性能并发构建块，开发人员可以提高并发类的线程安全、可伸缩性、性能、可读性和可靠性。 如果一些类名看起来相似，可能是因为 java.util.concurrent 中的许多概念源自 Doug Lea 的 util.concurrent 库（请参阅 参考资料）。 JDK 5.0 中的并发改进可以分为三组： • JVM 级别更改。大多数现代处理器对并发对某一硬件级别提供支持，通常以 compare-and-swap （CAS）指令形式。CAS 是一种低级别的、细粒度的技术，它允许多个线程更新一个内存位置，同时能够检测其他线程的冲突并进行恢复。它是许多高性能并发算法的基础。在 JDK 5.0 之前，Java 语言中用于协调线程之间的访问的惟一原语是同步，同步是更重量级和粗粒度的。公开 CAS 可以开发高度可伸缩的并发 Java 类。这些更改主要由 JDK 库类使用，而不是由开发人员使用。 • 低级实用程序类 – 锁定和原子类。使用 CAS 作为并发原语，ReentrantLock 类提供与 synchronized 原语相同的锁定和内存语义，然而这样可以更好地控制锁定（如计时的锁定等待、锁定轮询和可中断的锁定等待）和提供更好的可伸缩性（竞争时的高性能）。大多数开发人员将不再直接使用 ReentrantLock 类，而是使用在 ReentrantLock 类上构建的高级类。 • 高级实用程序类。这些类实现并发构建块，每个计算机科学文本中都会讲述这些类 – 信号、互斥、闩锁、屏障、交换程序、线程池和线程安全集合类等。大部分开发人员都可以在应用程序中用这些类，来替换许多（如果不是全部）同步、wait() 和 notify() 的使用，从而提高性能、可读性和正确性。 本教程将重点介绍 java.util.concurrent 包提供的高级实用程序类 – 线程安全集合、线程池和同步实用程序。这些是初学者和专家都可以使用的”现成”类。 在第一小节中，我们将回顾并发的基本知识，尽管它不应取代对线程和线程安全的了解。那些一点都不熟悉线程的读者应该先参考一些关于线程的介绍，如”Introduction to Java Threads”教程（请参阅参考资料）。 接下来的几个小节将研究 java.util.concurrent 中的高级实用程序类 – 线程安全集合、线程池、信号和同步工具。 最后一小节将介绍 java.util.concurrent 中的低级并发构建块，并提供一些性能测评来显示新 java.util.concurrent 类的可伸缩性的改进。 什么是线程？ 所有重要的操作系统都支持进程的概念 – 独立运行的程序，在某种程度上相互隔离。 线程有时称为 轻量级进程。与进程一样，它们拥有通过程序运行的独立的并发路径，并且每个线程都有自己的程序计数器，称为堆栈和本地变量。然而，线程存在于进程中，它们与同一进程内的其他线程共享内存、文件句柄以及每进程状态。 今天，几乎每个操作系统都支持线程，允许执行多个可独立调度的线程，以便共存于一个进程中。因为一个进程中的线程是在同一个地址空间中执行的，所以多个线程可以同时访问相同对象，并且它们从同一堆栈中分配对象。虽然这使线程更易于与其他线程共享信息，但也意味着您必须确保线程之间不相互干涉。 正确使用线程时，线程能带来诸多好处，其中包括更好的资源利用、简化开发、高吞吐量、更易响应的用户界面以及能执行异步处理。 Java 语言包括用于协调线程行为的原语，从而可以在不违反设计原型或者不破坏数据结构的前提下安全地访问和修改共享变量。 线程有哪些功能？ 在 Java 程序中存在很多理由使用线程，并且不管开发人员知道线程与否，几乎每个 Java 应用程序都使用线程。许多 J2SE 和 J2EE 工具可以创建线程，如 RMI、Servlet、Enterprise JavaBeans 组件和 Swing GUI 工具包。 使用线程的理由包括： • 更易响应的用户界面。 事件驱动的 GUI 工具包（如 AWT 或 Swing）使用单独的事件线程来处理 GUI 事件。从事件线程中调用通过 GUI 对象注册的事件监听器。然而，如果事件监听器将执行冗长的任务（如文档拼写检查），那么 UI 将出现冻结，因为事件线程直到冗长任务完毕之后才能处理其他事件。通过在单独线程中执行冗长操作，当执行冗长后台任务时，UI 能继续响应。 • 使用多处理器。 多处理器（MP）系统变得越来越便宜，并且分布越来越广泛。因为调度的基本单位通常是线程，所以不管有多少处理器可用，一个线程的应用程序一次只能在一个处理器上运行。在设计良好的程序中，通过更好地利用可用的计算机资源，多线程能够提高吞吐量和性能。 • 简化建模。 有效使用线程能够使程序编写变得更简单，并易于维护。通过合理使用线程，个别类可以避免一些调度的详细、交叉存取操作、异步 IO 和资源等待以及其他复杂问题。相反，它们能专注于域的要求，简化开发并改进可靠性。 • 异步或后台处理。 服务器应用程序可以同时服务于许多远程客户机。如果应用程序从 socket 中读取数据，并且没有数据可以读取，那么对 read() 的调用将被阻塞，直到有数据可读。在单线程应用程序中，这意味着当某一个线程被阻塞时，不仅处理相应请求要延迟，而且处理所有请求也将延迟。然而，如果每个 socket 都有自己的 IO 线程，那么当一个线程被阻塞时，对其他并发请求行为没有影响。 线程安全 如果将这些类用于多线程环境中，虽然确保这些类的线程安全比较困难，但线程安全却是必需的。java.util.concurrent 规范进程的一个目标就是提供一组线程安全的、高性能的并发构建块，从而使开发人员能够减轻一些编写线程安全类的负担。 线程安全类非常难以明确定义，大多数定义似乎都是完全循环的。快速 Google 搜索会显示下列线程安全代码定义的例子，但这些定义（或者更确切地说是描述）通常没什么帮助： • . . . can be called from multiple programming threads without unwanted interaction between the threads. • . . . may be called by more than on thread at a time without requiring any other action on the caller’s part. 通过类似这样的定义，不奇怪我们为什么对线程安全如此迷惑。这些定义几乎就是在说”如果可以从多个线程安全调用类，那么该类就是线程安全的”。这当然是线程安全的解释，但对我们区别线程安全类和不安全类没有什么帮助。我们使用”安全”是为了说明什么？ 要成为线程安全的类，首先它必须在单线程环境中正确运行。如果正确实现了类，那么说明它符合规范，对该类的对象的任何顺序的操作（公共字段的读写、公共方法的调用）都不应该使对象处于无效状态；观察将处于无效状态的对象；或违反类的任何变量、前置条件或后置条件。 而且，要成为线程安全的类，在从多个线程访问时，它必须继续正确运行，而不管运行时环境执行那些线程的调度和交叉，且无需对部分调用代码执行任何其他同步。结果是对线程安全对象的操作将用于按固定的整体一致顺序出现所有线程。 如果没有线程之间的某种明确协调，比如锁定，运行时可以随意在需要时在多线程中交叉操作执行。 在 JDK 5.0 之前，确保线程安全的主要机制是 synchronized 原语。访问共享变量（那些可以由多个线程访问的变量）的线程必须使用同步来协调对共享变量的读写访问。java.util.concurrent 包提供了一些备用并发原语，以及一组不需要任何其他同步的线程安全实用程序类。 令人厌烦的并发 即使您的程序从没有明确创建线程，也可能会有许多工具或框架代表您创建了线程，这时要求从这些线程调用的类是线程安全的。这样会对开发人员带来较大的设计和实现负担，因为开发线程安全类比开发非线程安全类有更多要注意的事项，且需要更多的分析。 AWT 和 Swing 这些 GUI 工具包创建了称为时间线程的后台线程，将从该线程调用通过 GUI 组件注册的监听器。因此，实现这些监听器的类必须是线程安全的。 TimerTask JDK 1.3 中引入的 TimerTask 工具允许稍后执行任务或计划定期执行任务。在 Timer 线程中执行 TimerTask 事件，这意味着作为 TimerTask 执行的任务必须是线程安全的。 Servlet 和 JavaServer Page 技术 Servlet 容器可以创建多个线程，在多个线程中同时调用给定 servlet，从而进行多个请求。因此 servlet 类必须是线程安全的。 RMI 远程方法调用（remote method invocation，RMI）工具允许调用其他 JVM 中运行的操作。实现远程对象最普遍的方法是扩展 UnicastRemoteObject。例示 UnicastRemoteObject 时，它是通过 RMI 调度器注册的，该调度器可能创建一个或多个线程，将在这些线程中执行远程方法。因此，远程类必须是线程安全的。 正如所看到的，即使应用程序没有明确创建线程，也会发生许多可能会从其他线程调用类的情况。幸运的是，java.util.concurrent 中的类可以大大简化编写线程安全类的任务。 例子 – 非线程安全 servlet 下列 servlet 看起来像无害的留言板 servlet，它保存每个来访者的姓名。然而，该 servlet 不是线程安全的，而这个 servlet 应该是线程安全的。问题在于它使用 HashSet 存储来访者的姓名，HashSet 不是线程安全的类。 当我们说这个 servlet 不是线程安全的时，是说它所造成的破坏不仅仅是丢失留言板输入。在最坏的情况下，留言板数据结构都可能被破坏并且无法恢复。 public class UnsafeGuestbookServlet extends HttpServlet { private Set visitorSet = new HashSet(); protected void doGet(HttpServletRequest httpServletRequest, ​ HttpServletResponse httpServletResponse) throws ServletException, IOException { ​ String visitorName = httpServletRequest.getParameter(“NAME”); ​ if (visitorName != null) ​ visitorSet.add(visitorName); } } 通过将 visitorSet 的定义更改为下列代码，可以使该类变为线程安全的： private Set visitorSet = Collections.synchronizedSet(new HashSet()); 如上所示的例子显示线程的内置支持是一把双刃剑 – 虽然它使构建多线程应用程序变得很容易，但它同时要求开发人员更加注意并发问题，甚至在使用留言板 servlet 这样普通的东西时也是如此。 线程安全集合 JDK 1.2 中引入的 Collection 框架是一种表示对象集合的高度灵活的框架，它使用基本接口 List、Set 和 Map。通过 JDK 提供每个集合的多次实现（HashMap、Hashtable、TreeMap、WeakHashMap、HashSet、TreeSet、Vector、ArrayList、LinkedList 等等）。其中一些集合已经是线程安全的（Hashtable 和 Vector），通过同步的封装工厂（Collections.synchronizedMap()、synchronizedList() 和 synchronizedSet()），其余的集合均可表现为线程安全的。 java.util.concurrent 包添加了多个新的线程安全集合类（ConcurrentHashMap、CopyOnWriteArrayList 和 CopyOnWriteArraySet）。这些类的目的是提供高性能、高度可伸缩性、线程安全的基本集合类型版本。 java.util 中的线程集合仍有一些缺点。例如，在迭代锁定时，通常需要将该锁定保留在集合中，否则，会有抛出 ConcurrentModificationException 的危险。（这个特性有时称为条件线程安全；有关的更多说明，请参阅参考资料。）此外，如果从多个线程频繁地访问集合，则常常不能很好地执行这些类。java.util.concurrent 中的新集合类允许通过在语义中的少量更改来获得更高的并发。 JDK 5.0 还提供了两个新集合接口 – Queue 和 BlockingQueue。Queue 接口与 List 类似，但它只允许从后面插入，从前面删除。通过消除 List 的随机访问要求，可以创建比现有 ArrayList 和 LinkedList 实现性能更好的 Queue 实现。因为 List 的许多应用程序实际上不需要随机访问，所以Queue 通常可以替代 List，来获得更好的性能。 弱一致的迭代器 java.util 包中的集合类都返回 fail-fast 迭代器，这意味着它们假设线程在集合内容中进行迭代时，集合不会更改它的内容。如果 fail-fast 迭代器检测到在迭代过程中进行了更改操作，那么它会抛出 ConcurrentModificationException，这是不可控异常。 在迭代过程中不更改集合的要求通常会对许多并发应用程序造成不便。相反，比较好的是它允许并发修改并确保迭代器只要进行合理操作，就可以提供集合的一致视图，如 java.util.concurrent 集合类中的迭代器所做的那样。 java.util.concurrent 集合返回的迭代器称为弱一致的（weakly consistent）迭代器。对于这些类，如果元素自从迭代开始已经删除，且尚未由 next() 方法返回，那么它将不返回到调用者。如果元素自迭代开始已经添加，那么它可能返回调用者，也可能不返回。在一次迭代中，无论如何更改底层集合，元素不会被返回两次。 CopyOnWriteArrayList 和 CopyOnWriteArraySet 可以用两种方法创建线程安全支持数据的 List – Vector 或封装 ArrayList 和 Collections.synchronizedList()。java.util.concurrent 包添加了名称繁琐的 CopyOnWriteArrayList。为什么我们想要新的线程安全的List类？为什么Vector还不够？ 最简单的答案是与迭代和并发修改之间的交互有关。使用 Vector 或使用同步的 List 封装器，返回的迭代器是 fail-fast 的，这意味着如果在迭代过程中任何其他线程修改 List，迭代可能失败。 Vector 的非常普遍的应用程序是存储通过组件注册的监听器的列表。当发生适合的事件时，该组件将在监听器的列表中迭代，调用每个监听器。为了防止 ConcurrentModificationException，迭代线程必须复制列表或锁定列表，以便进行整体迭代，而这两种情况都需要大量的性能成本。 CopyOnWriteArrayList 类通过每次添加或删除元素时创建支持数组的新副本，避免了这个问题，但是进行中的迭代保持对创建迭代器时的当前副本进行操作。虽然复制也会有一些成本，但是在许多情况下，迭代要比修改多得多，在这些情况下，写入时复制要比其他备用方法具有更好的性能和并发性。 如果应用程序需要 Set 语义，而不是 List，那么还有一个 Set 版本 – CopyOnWriteArraySet。 ConcurrentHashMap 正如已经存在线程安全的 List 的实现，您可以用多种方法创建线程安全的、基于 hash 的 Map – Hashtable，并使用 Collections.synchronizedMap() 封装 HashMap。JDK 5.0 添加了 ConcurrentHashMap 实现，该实现提供了相同的基本线程安全的 Map 功能，但它大大提高了并发性。 Hashtable 和 synchronizedMap 所采取的获得同步的简单方法（同步 Hashtable 中或者同步的 Map 封装器对象中的每个方法）有两个主要的不足。首先，这种方法对于可伸缩性是一种障碍，因为一次只能有一个线程可以访问 hash 表。同时，这样仍不足以提供真正的线程安全性，许多公用的混合操作仍然需要额外的同步。虽然诸如 get() 和 put() 之类的简单操作可以在不需要额外同步的情况下安全地完成，但还是有一些公用的操作序列，例如迭代或者 put-if-absent（空则放入），需要外部的同步，以避免数据争用。 Hashtable 和 Collections.synchronizedMap 通过同步每个方法获得线程安全。这意味着当一个线程执行一个 Map 方法时，无论其他线程要对 Map 进行什么样操作，都不能执行，直到第一个线程结束才可以。 对比来说，ConcurrentHashMap 允许多个读取几乎总是并发执行，读和写操作通常并发执行，多个同时写入经常并发执行。结果是当多个线程需要访问同一 Map 时，可以获得更高的并发性。 在大多数情况下，ConcurrentHashMap 是 Hashtable或 Collections.synchronizedMap(new HashMap()) 的简单替换。然而，其中有一个显著不同，即 ConcurrentHashMap 实例中的同步不锁定映射进行独占使用。实际上，没有办法锁定 ConcurrentHashMap 进行独占使用，它被设计用于进行并发访问。为了使集合不被锁定进行独占使用，还提供了公用的混合操作的其他（原子）方法，如 put-if-absent。ConcurrentHashMap 返回的迭代器是弱一致的，意味着它们将不抛出ConcurrentModificationException ，将进行”合理操作”来反映迭代过程中其他线程对 Map 的修改。 队列 原始集合框架包含三个接口：List、Map 和 Set。List 描述了元素的有序集合，支持完全随即访问 – 可以在任何位置添加、提取或删除元素。 LinkedList 类经常用于存储工作元素（等待执行的任务）的列表或队列。然而，List 提供的灵活性比该公用应用程序所需要的多得多，这个应用程序通常在后面插入元素，从前面删除元素。但是要支持完整 List 接口则意味着 LinkedList 对于这项任务不像原来那样有效。Queue 接口比 List 简单得多，仅包含 put() 和 take() 方法，并允许比 LinkedList 更有效的实现。 Queue 接口还允许实现来确定存储元素的顺序。ConcurrentLinkedQueue 类实现先进先出（first-in-first-out，FIFO）队列，而 PriorityQueue 类实现优先级队列（也称为堆），它对于构建调度器非常有用，调度器必须按优先级或预期的执行时间执行任务。 interface Queue extends Collection { boolean offer(E x); E poll(); E remove() throws NoSuchElementException; E peek(); E element() throws NoSuchElementException; } 实现 Queue 的类是： • LinkedList 已经进行了改进来实现 Queue。 • PriorityQueue 非线程安全的优先级对列（堆）实现，根据自然顺序或比较器返回元素。 • ConcurrentLinkedQueue 快速、线程安全的、无阻塞 FIFO 队列。 任务管理之线程创建 线程最普遍的一个应用程序是创建一个或多个线程，以执行特定类型的任务。Timer 类创建线程来执行 TimerTask 对象，Swing 创建线程来处理 UI 事件。在这两种情况中，在单独线程中执行的任务都假定是短期的，这些线程是为了处理大量短期任务而存在的。 在其中每种情况中，这些线程一般都有非常简单的结构： while (true) { if (no tasks) wait for a task; execute the task; } 通过例示从 Thread 获得的对象并调用 Thread.start() 方法来创建线程。可以用两种方法创建线程：通过扩展 Thread 和覆盖 run() 方法，或者通过实现 Runnable 接口和使用 Thread(Runnable) 构造函数： class WorkerThread extends Thread { public void run() { /* do work */ } } Thread t = new WorkerThread(); t.start(); 或者： Thread t = new Thread(new Runnable() { public void run() { /* do work */ } } t.start(); 重新使用线程 因为多个原因，类似 Swing GUI 的框架为事件任务创建单一线程，而不是为每项任务创建新的线程。首先是因为创建线程会有间接成本，所以创建线程来执行简单任务将是一种资源浪费。通过重新使用事件线程来处理多个事件，启动和拆卸成本（随平台而变）会分摊在多个事件上。 Swing 为事件使用单一后台线程的另一个原因是确保事件不会互相干涉，因为直到前一事件结束，下一事件才开始处理。该方法简化了事件处理程序的编写。 使用多个线程，将要做更多的工作来确保一次仅一个线程地执行线程相关的代码。 如何不对任务进行管理 大多数服务器应用程序（如 Web 服务器、POP 服务器、数据库服务器或文件服务器）代表远程客户机处理请求，这些客户机通常使用 socket 连接到服务器。对于每个请求，通常要进行少量处理（获得该文件的代码块，并将其发送回 socket），但是可能会有大量（且不受限制）的客户机请求服务。 用于构建服务器应用程序的简单化模型会为每个请求创建新的线程。下列代码段实现简单的 Web 服务器，它接受端口 80 的 socket 连接，并创建新的线程来处理请求。不幸的是，该代码不是实现 Web 服务器的好方法，因为在重负载条件下它将失败，停止整台服务器。 class UnreliableWebServer { public static void main(String[] args) { ServerSocket socket = new ServerSocket(80); while (true) { final Socket connection = socket.accept(); Runnable r = new Runnable() { ​ public void run() { ​ handleRequest(connection); ​ } }; // Don’t do this! new Thread(r).start(); } } } 当服务器被请求吞没时，UnreliableWebServer 类不能很好地处理这种情况。每次有请求时，就会创建新的类。根据操作系统和可用内存，可以创建的线程数是有限的。 不幸的是，您通常不知道限制是多少 – 只有当应用程序因为 OutOfMemoryError 而崩溃时才发现。 如果足够快地在这台服务器上抛出请求的话，最终其中一个线程创建将失败，生成的 Error 会关闭整个应用程序。当一次仅能有效支持很少线程时，没有必要创建上千个 线程，无论如何，这样使用资源可能会损害性能。创建线程会使用相当一部分内存，其中包括有两个堆栈（Java 和 C），以及每线程数据结构。如果创建过多线程，其中 每个线程都将占用一些 CPU 时间，结果将使用许多内存来支持大量线程，每个线程都运行得很慢。这样就无法很好地使用计算资源。 使用线程池解决问题 为任务创建新的线程并不一定不好，但是如果创建任务的频率高，而平均任务持续时间低，我们可以看到每项任务创建一个新的线程将产生性能（如果负载不可预知，还有稳定性）问题。 如果不是每项任务创建一个新的线程，则服务器应用程序必须采取一些方法来限制一次可以处理的请求数。这意味着每次需要启动新的任务时，它不能仅调用下列代码。 new Thread(runnable).start() 管理一大组小任务的标准机制是组合工作队列和线程池。工作队列就是要处理的任务的队列，前面描述的 Queue 类完全适合。线程池是线程的集合，每个线程都提取公用工作队列。当一个工作线程完成任务处理后，它会返回队列，查看是否有其他任务需要处理。如果有，它会转移到下一个任务，并开始处理。 线程池为线程生命周期间接成本问题和资源崩溃问题提供了解决方案。通过对多个任务重新使用线程，创建线程的间接成本将分布到多个任务中。作为一种额外好处，因为请求到达时，线程已经存在，从而可以消除由创建线程引起的延迟。因此，可以立即处理请求，使应用程序更易响应。而且，通过正确调整线程池中的线程数，可以强制超出特定限制的任何请求等待，直到有线程可以处理它，它们等待时所消耗的资源要少于使用额外线程所消耗的资源，这样可以防止资源崩溃。 Executor 框架 java.util.concurrent 包中包含灵活的线程池实现，但是更重要的是，它包含用于管理实现 Runnable 的任务的执行的整个框架。该框架称为 Executor 框架。 Executor 接口相当简单。它描述将运行 Runnable 的对象： public interface Executor { void execute(Runnable command); } 任务运行于哪个线程不是由该接口指定的，这取决于使用的 Executor 的实现。它可以运行于后台线程，如 Swing 事件线程，或者运行于线程池，或者调用线程，或者新的线程，它甚至可以运行于其他 JVM！通过同步的 Executor 接口提交任务，从任务执行策略中删除任务提交。Executor 接口独自关注任务提交 – 这是Executor 实现的选择，确定执行策略。这使在部署时调整执行策略（队列限制、池大小、优先级排列等等）更加容易，更改的代码最少。 java.util.concurrent 中的大多数 Executor 实现还实现 ExecutorService 接口，这是对 Executor 的扩展，它还管理执行服务的生命周期。这使它们更易于管理，并向生命可能比单独 Executor 的生命更长的应用程序提供服务。 public interface ExecutorService extends Executor { void shutdown(); List shutdownNow(); boolean isShutdown(); boolean isTerminated(); boolean awaitTermination(long timeout, ​ TimeUnit unit); // other convenience methods for submitting tasks } Executor java.util.concurrent 包包含多个 Executor 实现，每个实现都实现不同的执行策略。什么是执行策略？执行策略定义何时在哪个线程中运行任务，执行任务可能消耗的资源级别（线程、内存等等），以及如果执行程序超载该怎么办。 执行程序通常通过工厂方法例示，而不是通过构造函数。Executors 类包含用于构造许多不同类型的 Executor 实现的静态工厂方法： • Executors.newCachedThreadPool() 创建不限制大小的线程池，但是当以前创建的线程可以使用时将重新使用那些线程。如果没有现有线程可用， • 将创建新的线程并将其添加到池中。使用不到 60 秒的线程将终止并从缓存中删除。 • Executors.newFixedThreadPool(int n) 创建线程池，其重新使用在不受限制的队列之外运行的固定线程组。在关闭前，所有线程都会因为执行 • 过程中的失败而终止，如果需要执行后续任务，将会有新的线程来代替这些线程。 • Executors.newSingleThreadExecutor() 创建 Executor，其使用在不受限制的队列之外运行的单一工作线程，与 Swing 事件线程非常相似。 • 保证顺序执行任务，在任何给定时间，不会有多个任务处于活动状态。 更可靠的 Web 服务器 – 使用 Executor 前面 如何不对任务进行管理 中的代码显示了如何不用编写可靠服务器应用程序。幸运的是，修复这个示例非常简单，只需将 Thread.start() 调用替换为向 Executor 提交任务即可： class ReliableWebServer { Executor pool = Executors.newFixedThreadPool(7); public static void main(String[] args) { ServerSocket socket = new ServerSocket(80); while (true) { final Socket connection = socket.accept(); Runnable r = new Runnable() { ​ public void run() { ​ handleRequest(connection); ​ } }; pool.execute(r); } } } 注意，本例与前例之间的区别仅在于 Executor 的创建以及如何提交执行的任务。 定制 ThreadPoolExecutor Executors 中的 newFixedThreadPool 和 newCachedThreadPool 工厂方法返回的 Executor 是类 ThreadPoolExecutor 的实例，是高度可定制的。 通过使用包含 ThreadFactory 变量的工厂方法或构造函数的版本，可以定义池线程的创建。ThreadFactory 是工厂对象，其构造执行程序要使用的新线程。 使用定制的线程工厂，创建的线程可以包含有用的线程名称，并且这些线程是守护线程，属于特定线程组或具有特定优先级。 下面是线程工厂的例子，它创建守护线程，而不是创建用户线程： public class DaemonThreadFactory implements ThreadFactory { public Thread newThread(Runnable r) { ​ Thread thread = new Thread(r); ​ thread.setDaemon(true); ​ return thread; } } 有时，Executor 不能执行任务，因为它已经关闭或者因为 Executor 使用受限制队列存储等待任务，而该队列已满。在这种情况下，需要咨询执行程序的 RejectedExecutionHandler 来确定如何处理任务 – 抛出异常（默认情况），放弃任务，在调用者的线程中执行任务，或放弃队列中最早的任务以为新任务腾出空间。ThreadPoolExecutor.setRejectedExecutionHandler 可以设置拒绝的执行处理程序。 还可以扩展 ThreadPoolExecutor，并覆盖方法 beforeExecute 和 afterExecute，以添加装置，添加记录，添加计时，重新初始化线程本地变量，或进行其他执行定制。 需要特别考虑的问题 使用 Executor 框架会从执行策略中删除任务提交，一般情况下，人们希望这样，那是因为它允许我们灵活地调整执行策略，不必更改许多位置的代码。然而，当提交代码暗含假设特定执行策略时，存在多种情况，在这些情况下，重要的是选择的 Executor 实现一致的执行策略。 这类情况中的其中的一种就是一些任务同时等待其他任务完成。在这种情况下，当线程池没有足够的线程时，如果所有当前执行的任务都在等待另一项任务，而该任务因为线程池已满不能执行，那么线程池可能会死锁。 另一种相似的情况是一组线程必须作为共同操作组一起工作。在这种情况下，需要确保线程池能够容纳所有线程。 如果应用程序对特定执行程序进行了特定假设，那么应该在 Executor 定义和初始化的附近对这些进行说明，从而使善意的更改不会破坏应用程序的正确功能。 调整线程池 创建 Executor 时，人们普遍会问的一个问题是”线程池应该有多大？”。当然，答案取决于硬件和将执行的任务类型（它们是受计算限制或是受 IO 的限制？）。 如果线程池太小，资源可能不能被充分利用，在一些任务还在工作队列中等待执行时，可能会有处理器处于闲置状态。 另一方面，如果线程池太大，则将有许多有效线程，因为大量线程或有效任务使用内存，或者因为每项任务要比使用少量线程有更多上下文切换，性能可能会受损。 所以假设为了使处理器得到充分使用，线程池应该有多大？如果知道系统有多少处理器和任务的计算时间和等待时间的近似比率，Amdahl 法则提供很好的近似公式。 用 WT 表示每项任务的平均等待时间，ST 表示每项任务的平均服务时间（计算时间）。则 WT/ST 是每项任务等待所用时间的百分比。对于 N 处理器系统，池中可以近似有 N*(1+WT/ST) 个线程。 好的消息是您不必精确估计 WT/ST。”合适的”池大小的范围相当大；只需要避免”过大”和”过小”的极端情况即可。 Future 接口 Future 接口允许表示已经完成的任务、正在执行过程中的任务或者尚未开始执行的任务。通过 Future 接口，可以尝试取消尚未完成的任务，查询任务已经完成还是取消了，以及提取（或等待）任务的结果值。 FutureTask 类实现了 Future，并包含一些构造函数，允许将 Runnable 或 Callable（会产生结果的 Runnable）和 Future 接口封装。因为 FutureTask 也实现 Runnable，所以可以只将 FutureTask 提供给 Executor。一些提交方法（如 ExecutorService.submit()）除了提交任务之外，还将返回 Future 接口。 Future.get() 方法检索任务计算的结果（或如果任务完成，但有异常，则抛出 ExecutionException）。如果任务尚未完成，那么 Future.get() 将被阻塞，直到任务完成；如果任务已经完成，那么它将立即返回结果。 使用 Future 构建缓存 该示例代码与 java.util.concurrent 中的多个类关联，突出显示了 Future 的功能。它实现缓存，使用 Future 描述缓存值，该值可能已经计算，或者可能在其他线程中”正在构造”。 它利用 ConcurrentHashMap 中的原子 putIfAbsent() 方法，确保仅有一个线程试图计算给定关键字的值。如果其他线程随后请求同一关键字的值，它仅能等待（通过 Future.get() 的帮助）第一个线程完成。因此两个线程不会计算相同的值。 public class Cache { ConcurrentMap&gt; map = new ConcurrentHashMap(); Executor executor = Executors.newFixedThreadPool(8); public V get(final K key) { ​ FutureTask f = map.get(key); ​ if (f == null) { ​ Callable c = new Callable() { ​ public V call() { ​ // return value associated with key ​ } ​ }; ​ f = new FutureTask(c); ​ FutureTask old = map.putIfAbsent(key, f); ​ if (old == null) ​ executor.execute(f); ​ else ​ f = old; ​ } ​ return f.get(); } } CompletionService CompletionService 将执行服务与类似 Queue 的接口组合，从任务执行中删除任务结果的处理。CompletionService 接口包含用来提交将要执行的任务的 submit() 方法和用来询问下一完成任务的 take()/poll() 方法。 CompletionService 允许应用程序结构化，使用 Producer/Consumer 模式，其中生产者创建任务并提交，消费者请求完成任务的结果并处理这些结果。CompletionService 接口由 ExecutorCompletionService 类实现，该类使用 Executor 处理任务并从 CompletionService 导出 submit/poll/take 方法。 下列代码使用 Executor 和 CompletionService 来启动许多”solver”任务，并使用第一个生成非空结果的任务的结果，然后取消其余任务： void solve(Executor e, Collection&gt; solvers) throws InterruptedException { ​ CompletionService ecs = new ExecutorCompletionService(e); ​ int n = solvers.size(); ​ List&gt; futures = new ArrayList&gt;(n); ​ Result result = null; ​ try { ​ for (Callable s : solvers) ​ futures.add(ecs.submit(s)); ​ for (int i = 0; i &lt; n; ++i) { ​ try { ​ Result r = ecs.take().get(); ​ if (r != null) { ​ result = r; ​ break; ​ } ​ } catch(ExecutionException ignore) {} ​ } ​ } ​ finally { ​ for (Future f : futures) ​ f.cancel(true); ​ } ​ if (result != null) ​ use(result); } java.util.concurrent 中其他类别的有用的类也是同步工具。这组类相互协作，控制一个或多个线程的执行流。 Semaphore、CyclicBarrier、CountdownLatch 和 Exchanger 类都是同步工具的例子。每个类都有线程可以调用的方法，方法是否被阻塞取决于正在使用的特定同步工具的状态和规则。 Semaphore Semaphore 类实现标准 Dijkstra 计数信号。计数信号可以认为具有一定数量的许可权，该许可权可以获得或释放。如果有剩余的许可权，acquire() 方法将成功，否则该方法将被阻塞，直到有可用的许可权（通过其他线程释放许可权）。线程一次可以获得多个许可权。 计数信号可以用于限制有权对资源进行并发访问的线程数。该方法对于实现资源池或限制 Web 爬虫（Web crawler）中的输出 socket 连接非常有用。 注意信号不跟踪哪个线程拥有多少许可权；这由应用程序来决定，以确保何时线程释放许可权，该信号表示其他线程拥有许可权或者正在释放许可权，以及其他线程知道它的许可权已释放。 互斥 计数信号的一种特殊情况是互斥，或者互斥信号。互斥就是具有单一许可权的计数信号，意味着在给定时间仅一个线程可以具有许可权（也称为二进制信号）。互斥可以用于管理对共享资源的独占访问。 虽然互斥许多地方与锁定一样，但互斥还有一个锁定通常没有的其他功能，就是互斥可以由具有许可权的线程之外的其他线程来释放。这在死锁恢复时会非常有用。 CyclicBarrier 类可以帮助同步，它允许一组线程等待整个线程组到达公共屏障点。CyclicBarrier 是使用整型变量构造的，其确定组中的线程数。当一个线程到达屏障时（通过调用 CyclicBarrier.await()），它会被阻塞，直到所有线程都到达屏障，然后在该点允许所有线程继续执行。该操作与许多家庭逛商业街相似 – 每个家庭成员都自己走，并商定 1:00 在电影院集合。当您到电影院但不是所有人都到了时，您会坐下来等其他人到达。然后所有人一起离开。 认为屏障是循环的是因为它可以重新使用；一旦所有线程都已经在屏障处集合并释放，则可以将该屏障重新初始化到它的初始状态。 还可以指定在屏障处等待时的超时；如果在该时间内其余线程还没有到达屏障，则认为屏障被打破，所有正在等待的线程会收到 BrokenBarrierException。 下列代码将创建 CyclicBarrier 并启动一组线程，每个线程将计算问题的一部分，等待所有其他线程结束之后，再检查解决方案是否达成一致。如果不一致，那么每个工作线程将开始另一个迭代。该例将使用 CyclicBarrier 变量，它允许注册 Runnable，在所有线程到达屏障但还没有释放任何线程时执行 Runnable。 class Solver { // Code sketch void solve(final Problem p, int nThreads) { final CyclicBarrier barrier = new CyclicBarrier(nThreads, new Runnable() { ​ public void run() { p.checkConvergence(); }} ); for (int i = 0; i &lt; nThreads; ++i) { final int id = i; Runnable worker = new Runnable() { ​ final Segment segment = p.createSegment(id); ​ public void run() { ​ try { ​ while (!p.converged()) { ​ segment.update(); ​ barrier.await(); ​ } ​ } ​ catch(Exception e) { return; } ​ } }; new Thread(worker).start(); } } CountdownLatch CountdownLatch 类与 CyclicBarrier 相似，因为它的角色是对已经在它们中间分摊了问题的一组线程进行协调。它也是使用整型变量构造的，指明计数的初始值，但是与 CyclicBarrier 不同的是，CountdownLatch 不能重新使用。 其中，CyclicBarrier 是到达屏障的所有线程的大门，只有当所有线程都已经到达屏障或屏障被打破时，才允许这些线程通过，CountdownLatch 将到达和等待功能分离。任何线程都可以通过调用 countDown() 减少当前计数，这种不会阻塞线程，而只是减少计数。await() 方法的行为与 CyclicBarrier.await() 稍微有所不同，调用 await() 任何线程都会被阻塞，直到闩锁计数减少为零，在该点等待的所有线程才被释放，对 await() 的后续调用将立即返回。 当问题已经分解为许多部分，每个线程都被分配一部分计算时，CountdownLatch 非常有用。在工作线程结束时，它们将减少计数，协调线程可以在闩锁处等待当前这一批计算结束，然后继续移至下一批计算。 相反地，具有计数 1 的 CountdownLatch 类可以用作”启动大门”，来立即启动一组线程；工作线程可以在闩锁处等待，协调线程减少计数，从而立即释放所有工作线程。下例使用两个 CountdownLatche。一个作为启动大门，一个在所有工作线程结束时释放线程： class Driver { // … void main() throws InterruptedException { CountDownLatch startSignal = new CountDownLatch(1); CountDownLatch doneSignal = new CountDownLatch(N); for (int i = 0; i &lt; N; ++i) // create and start threads ​ new Thread(new Worker(startSignal, doneSignal)).start(); doSomethingElse(); // don’t let them run yet startSignal.countDown(); // let all threads proceed doSomethingElse(); doneSignal.await(); // wait for all to finish } } class Worker implements Runnable { private final CountDownLatch startSignal; private final CountDownLatch doneSignal; Worker(CountDownLatch startSignal, CountDownLatch doneSignal) { this.startSignal = startSignal; this.doneSignal = doneSignal; } public void run() { try { ​ startSignal.await(); ​ doWork(); ​ doneSignal.countDown(); } catch (InterruptedException ex) {} // return; } } Exchanger 类方便了两个共同操作线程之间的双向交换；这样，就像具有计数为 2 的 CyclicBarrier，并且两个线程在都到达屏障时可以”交换”一些状态。（Exchanger 模式有时也称为聚集。） Exchanger 通常用于一个线程填充缓冲（通过读取 socket），而另一个线程清空缓冲（通过处理从 socket 收到的命令）的情况。当两个线程在屏障处集合时，它们交换缓冲。下列代码说明了这项技术： class FillAndEmpty { Exchanger exchanger = new Exchanger(); DataBuffer initialEmptyBuffer = new DataBuffer(); DataBuffer initialFullBuffer = new DataBuffer(); class FillingLoop implements Runnable { public void run() { ​ DataBuffer currentBuffer = initialEmptyBuffer; ​ try { ​ while (currentBuffer != null) { ​ addToBuffer(currentBuffer); ​ if (currentBuffer.full()) ​ currentBuffer = exchanger.exchange(currentBuffer); ​ } ​ } catch (InterruptedException ex) { … handle … } } } class EmptyingLoop implements Runnable { public void run() { ​ DataBuffer currentBuffer = initialFullBuffer; ​ try { ​ while (currentBuffer != null) { ​ takeFromBuffer(currentBuffer); ​ if (currentBuffer.empty()) ​ currentBuffer = exchanger.exchange(currentBuffer); ​ } ​ } catch (InterruptedException ex) { … handle …} } } void start() { new Thread(new FillingLoop()).start(); new Thread(new EmptyingLoop()).start(); } } 锁定和原子之Lock Java 语言内置了锁定工具 – synchronized 关键字。当线程获得监视器时（内置锁定），其他线程如果试图获得相同锁定，那么它们将被阻塞，直到第一个线程释放该锁定。同步还确保随后获得相同锁定的线程可以看到之前的线程在具有该锁定时所修改的变量的值，从而确保如果类正确地同步了共享状态的访问权，那么线程将不会看到变量的”失效”值，这是缓存或编译器优化的结果。 虽然同步没有什么问题，但它有一些限制，在一些高级应用程序中会造成不便。Lock 接口将内置监视器锁定的锁定行为普遍化，允许多个锁定实现，同时提供一些内置锁定缺少的功能，如计时的等待、可中断的等待、锁定轮询、每个锁定有多个条件等待集合以及无阻塞结构的锁定。 interface Lock { void lock(); void lockInterruptibly() throws IE; boolean tryLock(); boolean tryLock(long time, ​ TimeUnit unit) throws IE; void unlock(); Condition newCondition() throws ​ UnsupportedOperationException; } ReentrantLock ReentrantLock 是具有与隐式监视器锁定（使用 synchronized 方法和语句访问）相同的基本行为和语义的 Lock 的实现，但它具有扩展的能力。 作为额外收获，在竞争条件下，ReentrantLock 的实现要比现在的 synchronized 实现更具有可伸缩性。（有可能在 JVM 的将来版本中改进 synchronized 的竞争性能。） 这意味着当许多线程都竞争相同锁定时，使用 ReentrantLock 的吞吐量通常要比 synchronized 好。换句话说，当许多线程试图访问 ReentrantLock 保护的共享资源时，JVM 将花费较少的时间来调度线程，而用更多个时间执行线程。 虽然 ReentrantLock 类有许多优点，但是与同步相比，它有一个主要缺点 – 它可能忘记释放锁定。建议当获得和释放 ReentrantLock 时使用下列结构： Lock lock = new ReentrantLock(); … lock.lock(); try { // perform operations protected by lock } catch(Exception ex) { // restore invariants } finally { lock.unlock(); } 因为锁定失误（忘记释放锁定）的风险，所以对于基本锁定，强烈建议您继续使用 synchronized，除非真的需要 ReentrantLock 额外的灵活性和可伸缩性。 ReentrantLock 是用于高级应用程序的高级工具 – 有时需要，但有时用原来的方法就很好。 Condition 就像 Lock 接口是同步的具体化，Condition 接口是 Object 中 wait() 和 notify() 方法的具体化。Lock 中的一个方法是 newCondition()，它要求锁定向该锁定返回新的 Condition 对象限制。await()、signal() 和 signalAll() 方法类似于 wait()、notify() 和 notifyAll()，但增加了灵活性，每个 Lock 都可以创建多个条件变量。这简化了一些并发算法的实现。 ReadWriteLock ReentrantLock 实现的锁定规则非常简单 – 每当一个线程具有锁定时，其他线程必须等待，直到该锁定可用。有时，当对数据结构的读取通常多于修改时，可以使用更复杂的称为读写锁定的锁定结构，它允许有多个并发读者，同时还允许一个写入者独占锁定。该方法在一般情况下（只读）提供了更大的并发性，同时在必要时仍提供独占访问的安全性。ReadWriteLock 接口和 ReentrantReadWriteLock 类提供这种功能 – 多读者、单写入者锁定规则，可以用这种功能来保护共享的易变资源。 原子变量 即使大多数用户将很少直接使用它们，原子变量类（AtomicInteger、AtomicLong、AtomicReference 等等）也有充分理由是最显著的新并发类。这些类公开对 JVM 的低级别改进，允许进行具有高度可伸缩性的原子读-修改-写操作。大多数现代 CPU 都有原子读-修改-写的原语，比如比较并交换（CAS）或加载链接/条件存储（LL/SC）。原子变量类使用硬件提供的最快的并发结构来实现。 许多并发算法都是根据对计数器或数据结构的比较并交换操作来定义的。通过暴露高性能的、高度可伸缩的 CAS 操作（以原子变量的形式），用 Java 语言实现高性能、无等待、无锁定的并发算法已经变得可行。 几乎 java.util.concurrent 中的所有类都是在 ReentrantLock 之上构建的，ReentrantLock 则是在原子变量类的基础上构建的。所以，虽然仅少数并发专家使用原子变量类，但 java.util.concurrent 类的很多可伸缩性改进都是由它们提供的。 原子变量主要用于为原子地更新”热”字段提供有效的、细粒度的方式，”热”字段是指由多个线程频繁访问和更新的字段。另外，原子变量还是计数器或生成序号的自然机制。 性能与可伸缩性 虽然 java.util.concurrent 努力的首要目标是使编写正确、线程安全的类更加容易，但它还有一个次要目标，就是提供可伸缩性。可伸缩性与性能完全不同，实际上，可伸缩性有时要以性能为代价来获得。 性能是”可以快速执行此任务的程度”的评测。可伸缩性描述应用程序的吞吐量如何表现为它的工作量和可用计算资源增加。可伸缩的程序可以按比例使用更多的处理器、内存或 I/O 带宽来处理更多个工作量。当我们在并发环境中谈论可伸缩性时，我们是在问当许多线程同时访问给定类时，这个类的执行情况。 java.util.concurrent 中的低级别类 ReentrantLock 和原子变量类的可伸缩性要比内置监视器（同步）锁定高得多。因此，使用 ReentrantLock 或原子变量类来协调共享访问的类也可能更具有可伸缩性。 Hashtable 与 ConcurrentHashMap 作为可伸缩性的例子，ConcurrentHashMap 实现设计的可伸缩性要比其线程安全的上一代 Hashtable 的可伸缩性强得多。Hashtable 一次只允许一个线程访问 Map；ConcurrentHashMap 允许多个读者并发执行，读者与写入者并发执行，以及一些写入者并发执行。因此，如果许多线程频繁访问共享映射，使用 ConcurrentHashMap 的总的吞吐量要比使用 Hashtable 的好。 下表大致说明了 Hashtable 和 ConcurrentHashMap 之间的可伸缩性差别。在每次运行时，N 个线程并发执行紧密循环，它们从 Hashtable 或 ConcurrentHashMap 中检索随即关键字，60% 的失败检索将执行 put() 操作，2% 的成功检索执行 remove() 操作。测试在运行 Linux 的双处理器 Xeon 系统中执行。数据显示 10,000,000 个迭代的运行时间，对于 ConcurrentHashMap，标准化为一个线程的情况。可以看到直到许多线程，ConcurrentHashMap 的性能仍保持可伸缩性，而 Hashtable 的性能在出现锁定竞争时几乎立即下降。 与通常的服务器应用程序相比，这个测试中的线程数看起来很少。然而，因为每个线程未进行其他操作，仅是重复地选择使用该表，所以这样可以模拟在执行一些实际工作的情况下使用该表的大量线程的竞争。 线程 ConcurrentHashMap Hashtable 线程 ConcurrentHashMap Hashtable 1 1.0 1.51 2 1.44 17.09 4 1.83 29.9 8 4.06 54.06 16 7.5 119.44 32 15.32 237.2 Lock 与 synchronized 与原子 下列基准说明了使用 java.util.concurrent 可能改进可伸缩性的例子。该基准将模拟旋转骰子，使用线性同余随机数生成器。有三个可用的随机数生成器的实现：一个使用同步来管理生成器的状态（单一变量），一个使用 ReentrantLock，另一个则使用 AtomicLong。下图显示了在 8-way Ultrasparc3 系统上，逐渐增加线程数量时这三个版本的相对吞吐量。（该图对原子变量方法的可伸缩性描述比较保守。） 图 1. 使用同步、Lock 和 AtomicLong 的相对吞吐量 公平与不公平 java.util.concurrent 中许多类中的另外一个定制元素是”公平”的问题。公平锁定或公平信号是指在其中根据先进先出（FIFO）的原则给与线程锁定或信号。ReentrantLock、Semaphore 和 ReentrantReadWriteLock 的构造函数都可以使用变量确定锁定是否公平，或者是否允许闯入（线程获得锁定，即使它们等待的时间不是最长）。 虽然闯入锁定的想法可能有些可笑，但实际上不公平、闯入的锁定非常普遍，且通常很受欢迎。使用同步访问的内置锁定不是公平锁定（且没有办法使它们公平）。相反，它们提供较弱的生病保证，要求所有线程最终都将获得锁定。 大多数应用程序选择（且应该选择）闯入锁定而不是公平锁定的原因是性能。在大多数情况下，完全的公平不是程序正确性的要求，真正公平的成本相当高。下表向前面的面板中的表中添加了第四个数据集，并由一个公平锁定管理对 PRNG 状态的访问。注意闯入锁定与公平锁定之间吞吐量的巨大差别。 图 2. 使用同步、Lock、公平锁定和 AtomicLong 的相对吞吐量 结束语 java.util.concurrent 包中包含大量有用的构建快，可以用它们来改进并发类的性能、可伸缩性、线程安全和可维护性。通过这些构建快，应该可以不再需要在您的代码中大量使用同步、wait/notify 和 Thread.start()，而用更高级别、标准化的、高性能并发实用程序来替换它们。","link":"/posts/88b6ffa4.html"}],"tags":[{"name":"jvm","slug":"jvm","link":"/tags/jvm/"},{"name":"并发","slug":"并发","link":"/tags/%E5%B9%B6%E5%8F%91/"},{"name":"读书笔记","slug":"读书笔记","link":"/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"ioc","slug":"ioc","link":"/tags/ioc/"},{"name":"spring-cloud","slug":"spring-cloud","link":"/tags/spring-cloud/"},{"name":"feign","slug":"feign","link":"/tags/feign/"},{"name":"微服务","slug":"微服务","link":"/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"教程","slug":"教程","link":"/tags/%E6%95%99%E7%A8%8B/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"命令","slug":"命令","link":"/tags/%E5%91%BD%E4%BB%A4/"},{"name":"速查","slug":"速查","link":"/tags/%E9%80%9F%E6%9F%A5/"},{"name":"配置","slug":"配置","link":"/tags/%E9%85%8D%E7%BD%AE/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"持久化","slug":"持久化","link":"/tags/%E6%8C%81%E4%B9%85%E5%8C%96/"},{"name":"bean","slug":"bean","link":"/tags/bean/"},{"name":"心得","slug":"心得","link":"/tags/%E5%BF%83%E5%BE%97/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"事务","slug":"事务","link":"/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"gc","slug":"gc","link":"/tags/gc/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"防火墙","slug":"防火墙","link":"/tags/%E9%98%B2%E7%81%AB%E5%A2%99/"},{"name":"firewalld","slug":"firewalld","link":"/tags/firewalld/"},{"name":"优化","slug":"优化","link":"/tags/%E4%BC%98%E5%8C%96/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"mongodb","slug":"mongodb","link":"/tags/mongodb/"},{"name":"elasticsearch","slug":"elasticsearch","link":"/tags/elasticsearch/"},{"name":"kibana","slug":"kibana","link":"/tags/kibana/"},{"name":"logstash","slug":"logstash","link":"/tags/logstash/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"反射","slug":"反射","link":"/tags/%E5%8F%8D%E5%B0%84/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"lua","slug":"lua","link":"/tags/lua/"},{"name":"bug","slug":"bug","link":"/tags/bug/"},{"name":"swap","slug":"swap","link":"/tags/swap/"},{"name":"安全","slug":"安全","link":"/tags/%E5%AE%89%E5%85%A8/"},{"name":"scala","slug":"scala","link":"/tags/scala/"},{"name":"netty","slug":"netty","link":"/tags/netty/"},{"name":"rest","slug":"rest","link":"/tags/rest/"},{"name":"zuul网关","slug":"zuul网关","link":"/tags/zuul%E7%BD%91%E5%85%B3/"},{"name":"idea","slug":"idea","link":"/tags/idea/"},{"name":"插件","slug":"插件","link":"/tags/%E6%8F%92%E4%BB%B6/"},{"name":"推荐","slug":"推荐","link":"/tags/%E6%8E%A8%E8%8D%90/"},{"name":"Redisson","slug":"Redisson","link":"/tags/Redisson/"},{"name":"spring-boot","slug":"spring-boot","link":"/tags/spring-boot/"},{"name":"数据类型","slug":"数据类型","link":"/tags/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"},{"name":"图床","slug":"图床","link":"/tags/%E5%9B%BE%E5%BA%8A/"},{"name":"免费","slug":"免费","link":"/tags/%E5%85%8D%E8%B4%B9/"},{"name":"gitee","slug":"gitee","link":"/tags/gitee/"},{"name":"踩坑","slug":"踩坑","link":"/tags/%E8%B8%A9%E5%9D%91/"},{"name":"vpn","slug":"vpn","link":"/tags/vpn/"},{"name":"远程办公","slug":"远程办公","link":"/tags/%E8%BF%9C%E7%A8%8B%E5%8A%9E%E5%85%AC/"},{"name":"内网穿透","slug":"内网穿透","link":"/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"},{"name":"工作流","slug":"工作流","link":"/tags/%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"name":"spring boot","slug":"spring-boot","link":"/tags/spring-boot/"},{"name":"spring shell","slug":"spring-shell","link":"/tags/spring-shell/"},{"name":"泛型","slug":"泛型","link":"/tags/%E6%B3%9B%E5%9E%8B/"},{"name":"Oauth2","slug":"Oauth2","link":"/tags/Oauth2/"},{"name":"aop","slug":"aop","link":"/tags/aop/"},{"name":"持续集成","slug":"持续集成","link":"/tags/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/"},{"name":"持续交付","slug":"持续交付","link":"/tags/%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98/"},{"name":"jenkins","slug":"jenkins","link":"/tags/jenkins/"},{"name":"分布式锁","slug":"分布式锁","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"},{"name":"读写锁","slug":"读写锁","link":"/tags/%E8%AF%BB%E5%86%99%E9%94%81/"},{"name":"agent","slug":"agent","link":"/tags/agent/"}],"categories":[{"name":"心得","slug":"心得","link":"/categories/%E5%BF%83%E5%BE%97/"},{"name":"并发","slug":"心得/并发","link":"/categories/%E5%BF%83%E5%BE%97/%E5%B9%B6%E5%8F%91/"},{"name":"spring","slug":"心得/spring","link":"/categories/%E5%BF%83%E5%BE%97/spring/"},{"name":"spring-cloud","slug":"心得/spring-cloud","link":"/categories/%E5%BF%83%E5%BE%97/spring-cloud/"},{"name":"git","slug":"心得/git","link":"/categories/%E5%BF%83%E5%BE%97/git/"},{"name":"jvm","slug":"心得/jvm","link":"/categories/%E5%BF%83%E5%BE%97/jvm/"},{"name":"redis","slug":"心得/redis","link":"/categories/%E5%BF%83%E5%BE%97/redis/"},{"name":"linux","slug":"心得/linux","link":"/categories/%E5%BF%83%E5%BE%97/linux/"},{"name":"mysql","slug":"心得/mysql","link":"/categories/%E5%BF%83%E5%BE%97/mysql/"},{"name":"docker","slug":"心得/docker","link":"/categories/%E5%BF%83%E5%BE%97/docker/"},{"name":"others","slug":"心得/others","link":"/categories/%E5%BF%83%E5%BE%97/others/"},{"name":"吐槽","slug":"吐槽","link":"/categories/%E5%90%90%E6%A7%BD/"},{"name":"java","slug":"心得/java","link":"/categories/%E5%BF%83%E5%BE%97/java/"},{"name":"others","slug":"others","link":"/categories/others/"}]}